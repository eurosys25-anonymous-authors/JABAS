diff --git a/aten/src/ATen/native/cudnn/RNN.cpp b/aten/src/ATen/native/cudnn/RNN.cpp
index 7c96a5f6eb4..9b8b6a200eb 100644
--- a/aten/src/ATen/native/cudnn/RNN.cpp
+++ b/aten/src/ATen/native/cudnn/RNN.cpp
@@ -913,6 +913,7 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> _cudnn_rnn(
   auto weight_buf = weight_buf_r;
   if (!weight_buf.defined()) {
     TORCH_WARN(WEIGHT_FORMAT_WARN);
+    TORCH_INTERNAL_ASSERT(weight_buf.defined());
   }
   if (fn_dropout_state.defined()) {
       auto input_arg = TensorArg(input, "input", 1);
diff --git a/torch/_C/_distributed_c10d.pyi b/torch/_C/_distributed_c10d.pyi
index 83089072883..4bfea432c6f 100644
--- a/torch/_C/_distributed_c10d.pyi
+++ b/torch/_C/_distributed_c10d.pyi
@@ -37,6 +37,9 @@ class Reducer:
         bucket_bytes_cap: int,
         find_unused_parameters: bool,
         gradient_as_bucket_view: bool,
+        model_index: int,
+        num_local_models: int,
+        total_num_modles: int
     ): ...
     def initialize_buckets(self, bucket_indices: List[List[int]]): ...
     ...
diff --git a/torch/__init__.py b/torch/__init__.py
index 74c86ca1177..86d6e57f7e5 100644
--- a/torch/__init__.py
+++ b/torch/__init__.py
@@ -601,6 +601,7 @@ def _assert(condition, message):
 # the public API. The "regular" import lines are there solely for the runtime
 # side effect of adding to the imported module's members for other users.
 
+from torch import iidp as iidp
 from torch import cuda as cuda
 from torch import autograd as autograd
 from torch.autograd import (
diff --git a/torch/autograd/__init__.py b/torch/autograd/__init__.py
index a013c9eb732..cbac94e34a0 100644
--- a/torch/autograd/__init__.py
+++ b/torch/autograd/__init__.py
@@ -72,6 +72,7 @@ def backward(
     create_graph: bool = False,
     grad_variables: Optional[_TensorOrTensors] = None,
     inputs: Optional[Sequence[torch.Tensor]] = None,
+    model_index: int = None
 ) -> None:
     r"""Computes the sum of gradients of given tensors w.r.t. graph leaves.
 
@@ -123,6 +124,7 @@ def backward(
             used to compute the attr::tensors. All the provided inputs must be leaf
             Tensors.
     """
+
     if grad_variables is not None:
         warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
         if grad_tensors is None:
@@ -133,6 +135,8 @@ def backward(
                                "use 'grad_tensors'.")
     if inputs is not None and len(inputs) == 0:
         raise RuntimeError("'inputs' argument to backward() cannot be empty.")
+    if model_index is None:
+        raise RuntimeError("[IIDP] 'model_index' argument to backward() cannot be empty.")
 
     tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
     inputs = tuple(inputs) if inputs is not None else tuple()
@@ -144,7 +148,7 @@ def backward(
 
     Variable._execution_engine.run_backward(
         tensors, grad_tensors_, retain_graph, create_graph, inputs,
-        allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
+        allow_unreachable=True, accumulate_grad=True, model_index=model_index)  # allow_unreachable flag
 
 
 def grad(
@@ -154,7 +158,8 @@ def grad(
     retain_graph: Optional[bool] = None,
     create_graph: bool = False,
     only_inputs: bool = True,
-    allow_unused: bool = False
+    allow_unused: bool = False,
+    model_index: int = 0
 ) -> Tuple[torch.Tensor, ...]:
     r"""Computes and returns the sum of gradients of outputs w.r.t. the inputs.
 
@@ -222,7 +227,7 @@ def grad(
 
     return Variable._execution_engine.run_backward(
         outputs, grad_outputs_, retain_graph, create_graph,
-        inputs, allow_unused, accumulate_grad=False)
+        inputs, allow_unused, accumulate_grad=False, model_index=model_index)
 
 
 # This function applies in case of gradient checkpointing for memory
diff --git a/torch/csrc/autograd/autograd.cpp b/torch/csrc/autograd/autograd.cpp
index e1e70586a07..556b2167d84 100644
--- a/torch/csrc/autograd/autograd.cpp
+++ b/torch/csrc/autograd/autograd.cpp
@@ -69,7 +69,8 @@ variable_list run_backward(
     bool create_graph,
     const variable_list& inputs,
     bool allow_unused,
-    bool accumulate_grad) {
+    bool accumulate_grad,
+    int model_index) {
   size_t num_tensors = outputs.size();
   edge_list roots;
   roots.reserve(num_tensors);
@@ -132,12 +133,15 @@ void backward(
     const variable_list& grad_tensors,
     c10::optional<bool> retain_graph,
     bool create_graph,
-    const variable_list& inputs) {
+    const variable_list& inputs,
+    int model_index) {
   variable_list gradients = _make_grads(tensors, grad_tensors);
   if (!retain_graph) {
     retain_graph = create_graph;
   }
-  run_backward(tensors, gradients, retain_graph.value(), create_graph, inputs, /*allow_unused=*/true, /*accumulate_grad=*/true);
+  run_backward(tensors, gradients, retain_graph.value(), create_graph, inputs,
+              /*allow_unused=*/true, /*accumulate_grad=*/true,
+              /*model_index=*/model_index);
 }
 
 variable_list grad(
@@ -146,16 +150,16 @@ variable_list grad(
     const variable_list& grad_outputs,
     c10::optional<bool> retain_graph,
     bool create_graph,
-    bool allow_unused) {
+    bool allow_unused,
+    int model_index) {
   variable_list gradients = _make_grads(outputs, grad_outputs);
   if (!retain_graph) {
     retain_graph = create_graph;
   }
   return run_backward(
-    outputs, gradients, retain_graph.value(), create_graph, inputs, allow_unused, /*accumulate_grad=*/false);
+    outputs, gradients, retain_graph.value(), create_graph, inputs, allow_unused, /*accumulate_grad=*/false, model_index);
 }
 
-
 namespace forward_ad {
 
 uint64_t enter_dual_level() {
diff --git a/torch/csrc/autograd/autograd.h b/torch/csrc/autograd/autograd.h
index 7f905b21c3b..3ce2465991d 100644
--- a/torch/csrc/autograd/autograd.h
+++ b/torch/csrc/autograd/autograd.h
@@ -41,7 +41,8 @@ TORCH_API void backward(
     const variable_list& grad_tensors = {},
     c10::optional<bool> retain_graph = c10::nullopt,
     bool create_graph = false,
-    const variable_list& inputs = {});
+    const variable_list& inputs = {},
+    int model_index = 0);
 
 /// Computes and returns the sum of gradients of outputs with respect to the inputs.
 ///
@@ -73,7 +74,8 @@ TORCH_API variable_list grad(
     const variable_list& grad_outputs = {},
     c10::optional<bool> retain_graph = c10::nullopt,
     bool create_graph = false,
-    bool allow_unused = false);
+    bool allow_unused = false,
+    int model_index = 0);
 
 namespace forward_ad {
 
diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index af295feba51..04692cbe3bd 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -33,6 +33,7 @@
 #include <sstream>
 #include <queue>
 #include <TH/TH.h>
+#include <vector>
 
 namespace torch { namespace autograd {
 
@@ -211,7 +212,9 @@ bool ReadyQueue::empty() const {
   return heap_.empty();
 }
 
-Engine::Engine() : max_recursion_depth_(MAX_DEPTH), non_reentrant_device_thread_count_(0) {}
+Engine::Engine() : max_recursion_depth_(MAX_DEPTH), non_reentrant_device_thread_count_(0) {
+  //std::cout << "[DEBUG][torch/csrc/autograd/engine.cpp] Engine initialized" << std::endl;
+}
 
 // Send shutdown tasks to all device_ready_queues_ if no backward tasks are running
 // Even though readyQueue should be empty, shutdown tasks have the highest priority
@@ -989,14 +992,26 @@ Engine& Engine::get_base_engine() {
   return engine;
 }
 
+Engine& Engine::get_base_engine_(int model_index) {
+  static Engine engine;
+  return engine;
+}
+
+
+std::vector<EngineStub> engine_stub_list;
 std::atomic<EngineStub> engine_stub(Engine::get_base_engine);
+std::atomic<EngineStub_> engine_stub_(Engine::get_base_engine_);
 
 void set_default_engine_stub(EngineStub stub) {
   engine_stub.store(stub);
 }
 
+void set_default_engine_stub(EngineStub_ stub) {
+  engine_stub_.store(stub);
+}
 
 Engine& Engine::get_default_engine() {
+  TORCH_INTERNAL_ASSERT(false, "get_default_engine() must not be called")
   return engine_stub.load()();
 }
 
@@ -1101,6 +1116,7 @@ void Engine::add_thread_pool_task(const std::weak_ptr<GraphTask>& graph_task) {
   thread_pool_shared_->graphtasks_queue_.push(graph_task);
   // Don't need to be holding the lock while actually creating the thread
   lck.unlock();
+
   if (create_thread) {
     std::thread t(&Engine::reentrant_thread_init, this);
     t.detach();
@@ -1180,3 +1196,4 @@ void GraphTask::init_to_execute(Node& graph_root, const edge_list& outputs, bool
 }
 
 }} // namespace torch::autograd
+
diff --git a/torch/csrc/autograd/engine.h b/torch/csrc/autograd/engine.h
index 7892f47521c..ab012fd9d9a 100644
--- a/torch/csrc/autograd/engine.h
+++ b/torch/csrc/autograd/engine.h
@@ -11,6 +11,7 @@
 #include <torch/csrc/autograd/function.h>
 #include <torch/csrc/autograd/functions/basic_ops.h>
 #include <torch/csrc/autograd/input_buffer.h>
+#include <torch/csrc/python_headers.h>
 
 #include <deque>
 #include <exception>
@@ -258,8 +259,8 @@ struct ReadyQueue {
 struct TORCH_API Engine {
   /// Returns a reference to a static `Engine` instance.
   static Engine& get_default_engine();
-
   static Engine& get_base_engine();
+  static Engine& get_base_engine_(int model_index = 0);
 
   Engine(const Engine&) = delete;
   Engine(Engine&&) = delete;
@@ -388,6 +389,8 @@ private:
 
 // allow python_engine to override the default engine when it loads
 using EngineStub = Engine& (*)();
+using EngineStub_ = Engine& (*)(int);
 TORCH_API void set_default_engine_stub(EngineStub stub);
+TORCH_API void set_default_engine_stub(EngineStub_ stub);
 
 }} // namespace torch::autograd
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index f3b23c66555..03a8eea831e 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -1,5 +1,4 @@
 #include <torch/csrc/python_headers.h>
-
 #include <c10/core/DeviceType.h>
 #include <torch/csrc/Exceptions.h>
 #include <torch/csrc/utils/pybind.h>
@@ -12,6 +11,7 @@
 #include <torch/csrc/autograd/utils/wrap_outputs.h>
 #include <torch/csrc/autograd/utils/python_arg_parsing.h>
 #include <torch/csrc/utils/pycfunction_helpers.h>
+#include <torch/csrc/autograd/python_engine.h>
 
 PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
   using namespace torch::autograd::profiler;
diff --git a/torch/csrc/autograd/python_engine.cpp b/torch/csrc/autograd/python_engine.cpp
index 2b19536c6ba..008bcd0fe4a 100644
--- a/torch/csrc/autograd/python_engine.cpp
+++ b/torch/csrc/autograd/python_engine.cpp
@@ -18,6 +18,11 @@
 
 #include <unordered_set>
 #include <memory> // for unique_ptr
+#include <thread>
+#include <vector>
+#include <iostream>
+
+#define MAX_ENGINE_POOL 10
 
 using namespace torch::autograd;
 
@@ -31,21 +36,31 @@ namespace torch { namespace autograd { namespace python {
 
 PythonEngine::PythonEngine() = default;
 
-Engine& PythonEngine::get_python_engine() {
-  static PythonEngine engine;
+Engine& PythonEngine::get_python_engine(int model_index) {
+  //std::thread::id this_id = std::this_thread::get_id();
+  //std::cout << "[DEBUG][torch/csrc/autograd/python_engine.cpp] "
+  //          << "atuograd engine pool size: "
+  //          << MAX_ENGINE_POOL << std::endl;
+
+  static PythonEngine engine[MAX_ENGINE_POOL];
+
   // This is "probably" thread-safe because the flag is set in a fork handler
   // before any threads are created, and this function is only called with the
   // GIL held. However, using fork + threads is playing with fire so this is
   // more of a "best effort" thing. For example, if the fork occurs while the
   // backwards threads hold a lock, we'll probably deadlock in the engine
   // destructor.
+
   if (_reinitialize_engine) {
-    engine.release_workers();
-    engine.~PythonEngine();
-    new (&engine) torch::autograd::python::PythonEngine();
+    engine[model_index].release_workers();
+    engine[model_index].~PythonEngine();
+    new (&engine[model_index]) torch::autograd::python::PythonEngine();
     _reinitialize_engine = false;
   }
-  return engine;
+
+  //std::cout << "[DEBUG][torch/csrc/autograd/python_engine.cpp] Thread "
+  //          << this_id << " get engine " << &engine[model_index] << std::endl;
+  return engine[model_index];
 }
 
 #if PY_MAJOR_VERSION == 3 && PY_MINOR_VERSION >= 9
@@ -147,13 +162,17 @@ PyObject *THPEngine_run_backward(PyObject *self, PyObject *args, PyObject *kwarg
   PyObject *inputs = nullptr;
   unsigned char allow_unreachable = 0;
   unsigned char accumulate_grad = 0; // Indicate whether to accumulate grad into leaf Tensors or capture
+  int model_index;
+
+
   const char *accepted_kwargs[] = { // NOLINT
       "tensors", "grad_tensors", "keep_graph", "create_graph", "inputs",
-      "allow_unreachable", "accumulate_grad", nullptr
+      "allow_unreachable", "accumulate_grad", "model_index", nullptr
   };
-  if (!PyArg_ParseTupleAndKeywords(args, kwargs, "OObb|Obb", (char**)accepted_kwargs,
-        &tensors, &grad_tensors, &keep_graph, &create_graph, &inputs, &allow_unreachable, &accumulate_grad))
+  if (!PyArg_ParseTupleAndKeywords(args, kwargs, "OObb|Obbi", (char**)accepted_kwargs,
+        &tensors, &grad_tensors, &keep_graph, &create_graph, &inputs, &allow_unreachable, &accumulate_grad, &model_index))
     return nullptr;
+
   THPUtils_assert(PyTuple_Check(tensors), "tensors argument is expected to "
       "be a tuple, but got %s", THPUtils_typename(tensors));
   THPUtils_assert(PyTuple_Check(grad_tensors), "grad_tensors argument is "
@@ -247,7 +266,7 @@ PyObject *THPEngine_run_backward(PyObject *self, PyObject *args, PyObject *kwarg
   variable_list outputs;
   {
     pybind11::gil_scoped_release no_gil;
-    auto& engine = python::PythonEngine::get_python_engine();
+    auto& engine = python::PythonEngine::get_python_engine(model_index);
     outputs = engine.execute(roots, grads, keep_graph, create_graph, accumulate_grad, output_edges);
   }
 
@@ -270,8 +289,20 @@ PyObject *THPEngine_run_backward(PyObject *self, PyObject *args, PyObject *kwarg
 }
 
 PyObject* THPEngine_queue_callback(PyObject *self, PyObject *_callback) {
+  // TODO(@IIDP): Update get_python_engine() to receive the model index as argument.
+  // However, queue_callback() funtion is currently not used in Python code
+  //TORCH_INTERNAL_ASSERT(false,
+  //    "queue_callback() funtion is currently supported "
+  //    "as it doesn't receive the model index as argument")
+  TORCH_WARN_ONCE(
+    "queue_callback() API is only supported for model index of 0. "
+    "TODO(@IIDP): Update get_python_engine() to receive the model index as argument");
+  // NOTE: [IIDP] For computation of GNS, pollux code uses queue_callback() API
+  // queue_callback() funtion is only supported for model index of 0
+  int model_index = 0;
   HANDLE_TH_ERRORS
-  auto& engine = python::PythonEngine::get_python_engine();
+  //auto& engine = python::PythonEngine::get_python_engine();
+  auto& engine = python::PythonEngine::get_python_engine(model_index);
   std::shared_ptr<PyObject> callback(_callback, [](PyObject *obj) { pybind11::gil_scoped_acquire gil; Py_DECREF(obj); });
   Py_INCREF(_callback);
   engine.queue_callback([callback]() {
diff --git a/torch/csrc/autograd/python_engine.h b/torch/csrc/autograd/python_engine.h
index 3a54484d4d3..a5522f9296a 100644
--- a/torch/csrc/autograd/python_engine.h
+++ b/torch/csrc/autograd/python_engine.h
@@ -5,12 +5,14 @@
 #include <torch/csrc/autograd/function.h>
 #include <torch/csrc/autograd/engine.h>
 
+#include <vector>
+
 bool THPEngine_initModule(PyObject *module);
 
 namespace torch { namespace autograd { namespace python {
 
 struct PythonEngine : public Engine {
-  static Engine& get_python_engine();
+  static Engine& get_python_engine(int model_index = 0);
   void thread_init(int device,
       const std::shared_ptr<ReadyQueue>& ready_queue,
       bool should_increment) override;
diff --git a/torch/csrc/distributed/c10d/init.cpp b/torch/csrc/distributed/c10d/init.cpp
index d733654250f..c5ac3353ae9 100644
--- a/torch/csrc/distributed/c10d/init.cpp
+++ b/torch/csrc/distributed/c10d/init.cpp
@@ -243,6 +243,10 @@ PyObject* c10d_init(PyObject* _unused, PyObject* noargs) {
       .def(
           "get_sizes_list",
           &::c10d::GradBucket::getSizesVec,
+          py::call_guard<py::gil_scoped_release>())
+      .def(
+          "get_gradients",
+          &::c10d::GradBucket::getGradients,
           py::call_guard<py::gil_scoped_release>());
 
   py::enum_<::c10d::BuiltinCommHookType>(module, "BuiltinCommHookType", R"(
@@ -259,7 +263,10 @@ An enum-like class for built-in communication hooks: ``ALLREDUCE`` and ``FP16_CO
               std::vector<std::vector<bool>>,
               int64_t,
               bool,
-              bool>(),
+              bool,
+              int,
+              int,
+              int>(),
           py::arg("replicas"),
           py::arg("bucket_indices"),
           py::arg("process_group"),
@@ -267,6 +274,9 @@ An enum-like class for built-in communication hooks: ``ALLREDUCE`` and ``FP16_CO
           py::arg("bucket_bytes_cap") = ::c10d::kDefaultBucketBytesCap,
           py::arg("find_unused_parameters") = false,
           py::arg("gradient_as_bucket_view") = false,
+          py::arg("model_index"),
+          py::arg("num_local_models"),
+          py::arg("total_num_models"),
           py::call_guard<py::gil_scoped_release>())
       .def(
           "initialize_buckets",
@@ -298,9 +308,14 @@ An enum-like class for built-in communication hooks: ``ALLREDUCE`` and ``FP16_CO
           "_set_forward_pass_work_handle",
           &::c10d::Reducer::set_forward_pass_work_handle,
           py::call_guard<py::gil_scoped_release>())
+      .def(
+          "reconfigure",
+          &::c10d::Reducer::reconfigure,
+          py::call_guard<py::gil_scoped_release>())
       .def(
           "_get_local_used_maps",
-          &::c10d::Reducer::get_local_used_maps_on_device);
+          &::c10d::Reducer::get_local_used_maps_on_device)
+      .def("get_rebuilt_bucket_indices", &::c10d::Reducer::get_rebuilt_bucket_indices);
 
   py::enum_<::c10d::ReduceOp>(module, "ReduceOp", R"(
 An enum-like class for available reduction operations: ``SUM``, ``PRODUCT``,
diff --git a/torch/csrc/jit/python/init.cpp b/torch/csrc/jit/python/init.cpp
index 23aa92f2229..57089813eb9 100644
--- a/torch/csrc/jit/python/init.cpp
+++ b/torch/csrc/jit/python/init.cpp
@@ -1331,4 +1331,4 @@ void initJITBindings(PyObject* module) {
   });
 }
 } // namespace jit
-} // namespace torch
+} // namespace torch
\ No newline at end of file
diff --git a/torch/csrc/jit/runtime/graph_executor.cpp b/torch/csrc/jit/runtime/graph_executor.cpp
index cf9bb1bc693..8e2a0c123a7 100644
--- a/torch/csrc/jit/runtime/graph_executor.cpp
+++ b/torch/csrc/jit/runtime/graph_executor.cpp
@@ -668,7 +668,6 @@ struct GraphExecutorImpl : public GraphExecutorImplBase {
     // Phase 3. Run differentiable optimizations (i.e. simple graph rewrites
     //          that we can still execute using autograd).
     runOptimization(opt_graph);
-
     // Phase 4. If this graph will be differentiated, we need to slice out the
     //          symbolically differentiable subgraphs for further optimizations.
     // Phase 5. Apply non-differentiable optimizations to the graphs we've found
@@ -682,6 +681,7 @@ struct GraphExecutorImpl : public GraphExecutorImplBase {
       for (Node* dnode : diff_nodes) {
         GRAPH_DEBUG("Optimizing diff node ", idx);
         auto diff_graph = std::move(dnode->g(attr::Subgraph));
+
         Gradient gradient = differentiate(diff_graph);
         GRAPH_DEBUG("Forward graph:\n", *(gradient.f));
         GRAPH_DEBUG("Backward graph:\n", *(gradient.df));
@@ -964,4 +964,4 @@ Node* replaceBlockWithFallbackGraph(Block* b, ArrayRef<Value*> inputs) {
 }
 
 } // namespace jit
-} // namespace torch
+} // namespace torch
\ No newline at end of file
diff --git a/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py b/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py
index 88c69ed9d4b..1be51751222 100644
--- a/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py
+++ b/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py
@@ -2,6 +2,25 @@ import torch
 import torch.distributed as dist
 
 
+def _allreduce_sum_fut(
+    process_group: dist.ProcessGroup, tensor: torch.Tensor
+) -> torch.futures.Future:
+    group_to_use = process_group if process_group is not None else dist.group.WORLD
+    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()
+
+    def get_value(fut):
+        # print(f'[DEBUG] _allreduce_sum_fut: {fut.value()[0]}')
+        return [fut.value()[0]]
+
+    return fut.then(get_value)
+
+
+def allreduce_sum_hook(
+    process_group: dist.ProcessGroup, bucket: dist._GradBucket
+) -> torch.futures.Future:
+    return _allreduce_sum_fut(process_group, bucket.get_tensors()[0])
+
+
 def _allreduce_fut(
     process_group: dist.ProcessGroup, tensor: torch.Tensor
 ) -> torch.futures.Future:
diff --git a/torch/distributed/optim/__init__.py b/torch/distributed/optim/__init__.py
index 0745dd14458..4f668ff5392 100644
--- a/torch/distributed/optim/__init__.py
+++ b/torch/distributed/optim/__init__.py
@@ -7,3 +7,6 @@ apply the gradients on each worker.
 """
 from .optimizer import DistributedOptimizer
 from .zero_redundancy_optimizer import ZeroRedundancyOptimizer
+
+from .shard_optimizer import ShardSGD
+from .shard_optimizer import ShardAdam
\ No newline at end of file
diff --git a/torch/distributed/optim/shard_optimizer.py b/torch/distributed/optim/shard_optimizer.py
new file mode 100644
index 00000000000..50293da4b63
--- /dev/null
+++ b/torch/distributed/optim/shard_optimizer.py
@@ -0,0 +1,169 @@
+import torch
+from torch.optim.optimizer import Optimizer, required
+import torch.optim._functional as F
+import torch.distributed as dist
+
+import enum
+from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Deque, Set
+
+
+class ShardAdam(Optimizer):
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
+                 weight_decay=0, amsgrad=False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        if not 0.0 <= weight_decay:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay, amsgrad=amsgrad)
+        super(ShardAdam, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(ShardAdam, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('amsgrad', False)
+
+    @torch.no_grad()
+    def step(self, gradients):
+        """Performs a single optimization step.
+
+        Args:
+            closure (callable, optional): A closure that reevaluates the model
+                and returns the loss.
+        """
+        loss = None
+        if gradients is None or len(gradients) == 0:
+            raise RuntimeError("Arguments gradients must not be empty or None")
+        for group in self.param_groups:
+            params_with_grad = []
+            grads = []
+            exp_avgs = []
+            exp_avg_sqs = []
+            state_sums = []
+            max_exp_avg_sqs = []
+            state_steps = []
+            params = group['params']
+
+            for p in params:
+                grad = None
+                for g in gradients:
+                    if p.index == g.index:
+                        grad = g
+                if grad is not None:
+                    params_with_grad.append(p)
+                    if grad.is_sparse:
+                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
+                    grads.append(grad)
+
+                    state = self.state[p]
+                    # Lazy state initialization
+                    if len(state) == 0:
+                        state['step'] = 0
+                        # Exponential moving average of gradient values
+                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        # Exponential moving average of squared gradient values
+                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        if group['amsgrad']:
+                            # Maintains max of all exp. moving avg. of sq. grad. values
+                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+
+                    exp_avgs.append(state['exp_avg'])
+                    exp_avg_sqs.append(state['exp_avg_sq'])
+
+                    if group['amsgrad']:
+                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])
+
+                    # update the steps for each param group update
+                    state['step'] += 1
+                    # record the step after step update
+                    state_steps.append(state['step'])
+
+            beta1, beta2 = group['betas']
+            F.adam(params_with_grad,
+                   grads,
+                   exp_avgs,
+                   exp_avg_sqs,
+                   max_exp_avg_sqs,
+                   state_steps,
+                   group['amsgrad'],
+                   beta1,
+                   beta2,
+                   group['lr'],
+                   group['weight_decay'],
+                   group['eps'])
+        return loss
+
+
+class ShardSGD(Optimizer):
+    def __init__(self, params, lr=required, momentum=0, dampening=0,
+                 weight_decay=0, nesterov=False):
+        if lr is not required and lr < 0.0:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if momentum < 0.0:
+            raise ValueError("Invalid momentum value: {}".format(momentum))
+        if weight_decay < 0.0:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+
+        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,
+                        weight_decay=weight_decay, nesterov=nesterov)
+        if nesterov and (momentum <= 0 or dampening != 0):
+            raise ValueError("Nesterov momentum requires a momentum and zero dampening")
+        super(ShardSGD, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(ShardSGD, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('nesterov', False)
+
+    @torch.no_grad()
+    def step(self, gradients):
+        loss = None
+        if gradients is None or len(gradients) == 0:
+            raise RuntimeError("Arguments gradients must not be empty or None")
+        for group in self.param_groups:
+            params_with_grad = []
+            d_p_list = []
+            momentum_buffer_list = []
+            weight_decay = group['weight_decay']
+            momentum = group['momentum']
+            dampening = group['dampening']
+            nesterov = group['nesterov']
+            lr = group['lr']
+            params = group['params']
+
+            for p in params:
+                grad = None
+                for g in gradients:
+                    if p.index == g.index:
+                        grad = g
+                if grad is not None:
+                    params_with_grad.append(p)
+                    d_p_list.append(grad)
+
+                    state = self.state[p]
+                    if 'momentum_buffer' not in state:
+                        momentum_buffer_list.append(None)
+                    else:
+                        momentum_buffer_list.append(state['momentum_buffer'])
+
+            F.sgd(params_with_grad,
+                  d_p_list,
+                  momentum_buffer_list,
+                  weight_decay=weight_decay,
+                  momentum=momentum,
+                  lr=lr,
+                  dampening=dampening,
+                  nesterov=nesterov)
+
+            # update momentum_buffers in state
+            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):
+                state = self.state[p]
+                state['momentum_buffer'] = momentum_buffer
+
+        return loss
diff --git a/torch/iidp/__init__.py b/torch/iidp/__init__.py
new file mode 100644
index 00000000000..33ae82afa8d
--- /dev/null
+++ b/torch/iidp/__init__.py
@@ -0,0 +1,11 @@
+from . import trainer
+from . import utils
+from . import data
+from . import ddp_comm_hooks
+from . import profiler
+
+from . import elastic
+from . import config
+from . import test
+
+from .trainer import IIDPTrainer, AdaptiveIIDPTrainer, ElasticTrainTimer
\ No newline at end of file
diff --git a/torch/iidp/cluster/__init__.py b/torch/iidp/cluster/__init__.py
new file mode 100644
index 00000000000..c147a77dc1d
--- /dev/null
+++ b/torch/iidp/cluster/__init__.py
@@ -0,0 +1,3 @@
+from . import cluster_manager
+from . import resource
+from . import server
diff --git a/torch/iidp/cluster/cluster_manager.py b/torch/iidp/cluster/cluster_manager.py
new file mode 100644
index 00000000000..25c59cb9be1
--- /dev/null
+++ b/torch/iidp/cluster/cluster_manager.py
@@ -0,0 +1,237 @@
+import socket
+import os
+import itertools
+
+import torch
+
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.utils.distributed import get_allgather_value, print_one_rank
+from torch.iidp.cluster.server import GlobalServerInfo, ServerInfo
+
+
+class IIDPClusterManager(object):
+    def __init__(self, gpu_cluster_info_file, available_server_name_list=[], gpu=None, **kwargs):
+        #print_one_rank(f'[torch/iidp/cluster/cluster_manager.py] args - gpu_cluster_info_file: {gpu_cluster_info_file}', 'debug')
+        self.homo_server_list = kwargs.get('homo_servers') or []
+        #print_one_rank(f'[torch/iidp/cluster/cluster_manager.py] args - homo_servers: {self.homo_server_list}', 'debug')
+        self.resource_alloc_unit = kwargs.get('resource_alloc_unit') or '2gpu'
+        print_one_rank(f'[{self.__class__.__name__}] Arguments - '
+                       f'resource_alloc_unit: {self.resource_alloc_unit} | '
+                       f'homo_server_list: {self.homo_server_list}')
+        self.gpu = gpu
+        self.gpu_cluster_info = read_json(gpu_cluster_info_file)
+        if socket.gethostname() not in self.gpu_cluster_info.keys():
+            raise ValueError(
+                f'Current server: {socket.gethostname()} is not registered '
+                f'in gpu cluster info json file: {gpu_cluster_info_file}'
+            )
+        if torch.cuda.get_device_name() != self.gpu_cluster_info[socket.gethostname()]['type']:
+            raise ValueError(
+                f'Registerd GPU type in server {socket.gethostname()} in {gpu_cluster_info_file} '
+                f'```{self.gpu_cluster_info[socket.gethostname()]["type"]}``` is not equal to '
+                f'real GPU type in server: ```{torch.cuda.get_device_name()}```'
+            )
+        server_name_enum = {}
+        for idx, server_name in enumerate(self.gpu_cluster_info.keys()):
+            server_name_enum[idx] = server_name
+            server_name_enum[server_name] = idx
+        #print_one_rank(f'[cluster_manager.py] server_name_enum: {server_name_enum}', 'debug')
+        # Initialize global server info (current servers)
+        self.global_server_info = GlobalServerInfo()
+        all_servername_per_rank_in_process_group = {
+            rank: server_name_enum[server_idx] for rank, server_idx in enumerate(get_allgather_value(server_name_enum[socket.gethostname()], self.gpu))
+        }
+        server_group = {}
+        for rank, server_name in all_servername_per_rank_in_process_group.items():
+            server_group[server_name] = [rank] if server_name not in server_group.keys() else server_group[server_name] + [rank]
+        for name, ranks in server_group.items():
+            self.global_server_info.add(ServerInfo(name, ranks, self.gpu_cluster_info[name]))
+
+        # Initialize available server info (future servers)
+        self.available_server_info = GlobalServerInfo()
+        for server_rank, server_name in enumerate(available_server_name_list):
+            max_num_gpus_in_server = self.gpu_cluster_info[server_name]['number']
+            ranks = [server_rank * max_num_gpus_in_server + rank for rank in range(int(max_num_gpus_in_server))]
+            self.available_server_info.add(ServerInfo(server_name, ranks, self.gpu_cluster_info[server_name]))
+
+        self.candidate_server_infos = []
+        max_num_gpus_in_curr_server = self.gpu_cluster_info[socket.gethostname()]['number']
+        self.alloc_unit_num = 0
+        if self.resource_alloc_unit != 'server':
+            try:
+                self.alloc_unit_num = int(self.resource_alloc_unit.replace('gpu', ''))
+            except:
+                raise ValueError(
+                    '```resource_alloc_unit``` has the format of ```<number of GPUs>gpu``` '
+                    f'e.g, 2gpu, 4gpu, but {self.resource_alloc_unit}')
+            if max_num_gpus_in_curr_server % self.alloc_unit_num != 0 or self.alloc_unit_num > max_num_gpus_in_curr_server:
+                raise ValueError(
+                    f'```resource_alloc_unit```: {self.resource_alloc_unit} -> {self.alloc_unit_num} '
+                    f'do not have the right number of GPUs with number of current server '
+                    f'{socket.gethostname()}: {max_num_gpus_in_curr_server}')
+
+        if self.resource_alloc_unit == 'server':
+            print_one_rank(
+                f'[{self.__class__.__name__}] ********************** RESOURCE_ALLOC_UNIT = SERVER **********************', 'experiment'
+            )
+            if len(self.homo_server_list) > 0:
+                raise ValueError('With resource_alloc_unit as ```server```, Not support ```homo_servers```')
+            self._generate_candidate_server_pool()
+        else:
+            self._generate_candidate_resource_pool()
+
+    def _split_rank_for_simigrad(self, arr, size):
+        arrays = []
+        while len(arr) > size:
+            pice = arr[:size]
+            arrays.append(pice)
+            arr   = arr[size:]
+        arrays.append(arr)
+        return arrays
+
+    def _generate_candidate_resource_pool(self):
+        """Return: List of GlobalServerInfo"""
+        if len(self.homo_server_list) > 0:
+            return self._generate_candidate_resource_pool_with_homogeneous_server()
+        #print_one_rank(f'_generate_candidate_resource_pool() - self.available_server_info: {self.available_server_info}', 'debug')
+        num_gpus_unit = self.alloc_unit_num
+        total_resource_unit_names = []
+        for server_info in self.available_server_info:
+            server_name = server_info.name
+            split_ranks = self._split_rank_for_simigrad(server_info.ranks, num_gpus_unit)
+            for ranks in split_ranks:
+                resource_unit_name = server_name+':'+str(ranks[0])
+                total_resource_unit_names.append(resource_unit_name)
+        #print_one_rank(f'total_resource_unit_names: {total_resource_unit_names}', 'debug')
+        candidate_resource_unit_name_config_set = []
+        for num_unit in range(1, len(total_resource_unit_names)+1):
+            #print_one_rank(f'===================================> num_unit: {num_unit}', 'debug')
+            filtered_combinations = []
+            combinations = list(itertools.combinations(total_resource_unit_names, num_unit))
+            #print_one_rank(f'combinations: {combinations}', 'debug')
+            #print_one_rank(f'Number of combinations: {len(combinations)}', 'debug')
+            registered_name_combination = []
+            for combination in combinations:
+                register_name = []
+                for unit_name_config in combination:
+                    name, rank = unit_name_config.split(':')
+                    register_name.append(name)
+                if register_name not in registered_name_combination:
+                    filtered_combinations.append(combination)
+                    registered_name_combination.append(register_name)
+            #print_one_rank(f'filtered_combinations: {filtered_combinations}', 'debug')
+            #print_one_rank(f'Number of filtered_combinations: {len(filtered_combinations)}', 'debug')
+            candidate_resource_unit_name_config_set.extend(filtered_combinations)
+        #print_one_rank(f'candidate_resource_unit_name_config_set: {candidate_resource_unit_name_config_set}', 'debug')
+        #print_one_rank(f'len(candidate_resource_unit_name_config_set): {len(candidate_resource_unit_name_config_set)}', 'debug')
+        # ex) unit_name_config_combination: ('server1:0', 'server1:2', 'server2:4', 'server2:6', 'server3:8', 'server3:10', 'server4:12', 'server4:14')
+        for unit_name_config_combination in candidate_resource_unit_name_config_set:
+            candidate_server_info = GlobalServerInfo()
+            for unit_name_config in unit_name_config_combination:
+                name, rank = unit_name_config.split(':')
+                ranks = list(range(int(rank), int(rank)+num_gpus_unit))
+                candidate_server_info.add(ServerInfo(name, ranks, self.gpu_cluster_info[name]))
+            self.candidate_server_infos.append(candidate_server_info)
+
+    def _generate_candidate_resource_pool_with_homogeneous_server(self):
+        PRINT_DEBUG_MSG = False
+        if PRINT_DEBUG_MSG:
+            print_one_rank(
+                f'_generate_candidate_resource_pool_with_homogeneous_server() -'
+                f'self.available_server_info: {self.available_server_info}', 'debug')
+        num_gpus_unit = self.alloc_unit_num
+        total_resource_unit_names = []
+        for server_info in self.available_server_info:
+            server_name = server_info.name
+            split_ranks = self._split_rank_for_simigrad(server_info.ranks, num_gpus_unit)
+            for ranks in split_ranks:
+                resource_unit_name = server_name+':'+str(ranks[0])
+                total_resource_unit_names.append(resource_unit_name)
+        if PRINT_DEBUG_MSG:
+            print_one_rank(f'total_resource_unit_names: {total_resource_unit_names}', 'debug')
+        candidate_resource_unit_name_config_set = []
+        for num_unit in range(1, len(total_resource_unit_names)+1):
+            if PRINT_DEBUG_MSG:
+                print_one_rank(f'===================================> num_unit: {num_unit}', 'debug')
+            filtered_combinations = []
+            combinations = list(itertools.combinations(total_resource_unit_names, num_unit))
+            if PRINT_DEBUG_MSG:
+                print_one_rank(f'Naive combinations: {combinations}', 'debug')
+                print_one_rank(f'Number of naive combinations: {len(combinations)}', 'debug')
+            registered_name_combination = []
+            registered_device_combination = []
+            for combination in combinations:
+                register_name = []
+                register_device = []
+                for unit_name_config in combination:
+                    name, rank = unit_name_config.split(':')
+                    register_name.append(name)
+                    register_device.append(self.gpu_cluster_info[name]['type'])
+                if PRINT_DEBUG_MSG:
+                    print_one_rank(f'==========> register_device: {register_device}', 'debug')
+                if register_name not in registered_name_combination and register_device not in registered_device_combination:
+                    filtered_combinations.append(combination)
+                    registered_name_combination.append(register_name)
+                    registered_device_combination.append(register_device)
+            if PRINT_DEBUG_MSG:
+                print_one_rank(f'filtered_combinations: {filtered_combinations}', 'debug')
+                print_one_rank(f'Number of filtered_combinations: {len(filtered_combinations)}', 'debug')
+            candidate_resource_unit_name_config_set.extend(filtered_combinations)
+        if PRINT_DEBUG_MSG:
+            print_one_rank(f'candidate_resource_unit_name_config_set: {candidate_resource_unit_name_config_set}', 'debug')
+            print_one_rank(f'len(candidate_resource_unit_name_config_set): {len(candidate_resource_unit_name_config_set)}', 'debug')
+        # ex) unit_name_config_combination: ('server1:0', 'server1:2', 'server2:4', 'server2:6', 'server3:8', 'server3:10', 'server4:12', 'server4:14')
+        for unit_name_config_combination in candidate_resource_unit_name_config_set:
+            candidate_server_info = GlobalServerInfo()
+            for unit_name_config in unit_name_config_combination:
+                name, rank = unit_name_config.split(':')
+                ranks = list(range(int(rank), int(rank)+num_gpus_unit))
+                candidate_server_info.add(ServerInfo(name, ranks, self.gpu_cluster_info[name]))
+            self.candidate_server_infos.append(candidate_server_info)
+
+    def _generate_candidate_server_pool(self):
+        """Return: List of GlobalServerInfo"""
+        num_gpus_unit = 2
+        total_resource = dict()
+        for server_info in self.available_server_info:
+            server_name = server_info.name
+            split_ranks = self._split_rank_for_simigrad(server_info.ranks, num_gpus_unit)
+            total_resource[server_name] = split_ranks
+        candidate_resource_unit_name_config_set = []
+        for num_server in range(1, len(total_resource)+1):
+            #print_one_rank(f'===================================> num_server: {num_server}', 'debug')
+            filtered_combinations = []
+            combinations = list(itertools.combinations(total_resource.keys(), num_server))
+            #print_one_rank(f'combinations: {combinations}', 'debug')
+            #print_one_rank(f'Number of combinations: {len(combinations)}', 'debug')
+            registered_name_combination = []
+            for combination in combinations:
+                register_name = []
+                unit_combination = set()
+                for server_name in combination:
+                    split_ranks = total_resource[server_name]
+                    for ranks in split_ranks:
+                        unit_name = server_name+':'+str(ranks[0])
+                        unit_combination.add(unit_name)
+                        register_name.append(server_name)
+                if register_name not in registered_name_combination:
+                    filtered_combinations.append(unit_combination)
+                    registered_name_combination.append(register_name)
+            #print_one_rank(f'filtered_combinations: {filtered_combinations}', 'debug')
+            #print_one_rank(f'Number of filtered_combinations: {len(filtered_combinations)}', 'debug')
+            candidate_resource_unit_name_config_set.extend(filtered_combinations)
+        #print_one_rank(f'candidate_resource_unit_name_config_set: {candidate_resource_unit_name_config_set}', 'debug')
+        #print_one_rank(f'len(candidate_resource_unit_name_config_set): {len(candidate_resource_unit_name_config_set)}', 'debug')
+        # ex) == unit_name_config_combination ==
+        # ({'server1:0', 'server1:2'}, {'server2:4', 'server2:6'}, {'server3:8', 'server3:10'}, {'server4:12', 'server4:14'})
+        for unit_name_config_combination in candidate_resource_unit_name_config_set:
+            candidate_server_info = GlobalServerInfo()
+            for unit_name_config in unit_name_config_combination:
+                name, rank = unit_name_config.split(':')
+                ranks = list(range(int(rank), int(rank)+num_gpus_unit))
+                candidate_server_info.add(ServerInfo(name, ranks, self.gpu_cluster_info[name]))
+            self.candidate_server_infos.append(candidate_server_info)
+        """
+        for candidate_server_info in self.candidate_server_infos:
+            print_one_rank(f'RESOURCE_ALLOC_UNIT = SERVER --> Candidate server info: {candidate_server_info}', 'debug')
+        """
\ No newline at end of file
diff --git a/torch/iidp/cluster/resource.py b/torch/iidp/cluster/resource.py
new file mode 100644
index 00000000000..68eb9509d3e
--- /dev/null
+++ b/torch/iidp/cluster/resource.py
@@ -0,0 +1,25 @@
+class ResourceInfo(object):
+    def __init__(self, name, num_gpus_in_server, max_num_gpus_in_server, intra_network_bandwidth, inter_network_bandwidth, tfplos):
+        self.device_name = name
+        self.num_gpus_in_server = num_gpus_in_server
+        self.max_num_gpus_in_server = max_num_gpus_in_server
+        self.intra_network_bandwidth = intra_network_bandwidth # byte/sec
+        self.inter_network_bandwidth = inter_network_bandwidth # byte/sec
+        self.tfplos = tfplos
+
+    def __repr__(self):
+        return str(self.__dict__)
+
+
+class GlobalResourceInfo(object):
+    def __init__(self):
+        self.total_num_servers = 0
+        self.total_num_gpus = 0
+
+    def __call__(self, total_num_servers, total_num_gpus):
+        self.total_num_servers = total_num_servers
+        self.total_num_gpus = total_num_gpus
+        return self
+
+    def __repr__(self):
+        return str(self.__dict__)
diff --git a/torch/iidp/cluster/server.py b/torch/iidp/cluster/server.py
new file mode 100644
index 00000000000..17065c76a53
--- /dev/null
+++ b/torch/iidp/cluster/server.py
@@ -0,0 +1,121 @@
+import torch.distributed as dist
+
+from torch.iidp.cluster.resource import ResourceInfo, GlobalResourceInfo
+
+
+class ServerInfo(object):
+    def __init__(self, name, ranks, resource_info):
+        self.name = name
+        self.ranks = ranks # List of global ranks
+        self.resource_info = ResourceInfo(
+            name=resource_info['type'],
+            num_gpus_in_server=len(ranks),
+            max_num_gpus_in_server=resource_info['number'],
+            intra_network_bandwidth=resource_info['intra_network_bandwidth'],
+            inter_network_bandwidth=resource_info['inter_network_bandwidth'],
+            tfplos=resource_info['tfplos'])
+
+    def __repr__(self):
+        return f'[ServerInfo] name: {self.name} | ranks: {self.ranks} | ' \
+               f'self.resource_info: {self.resource_info.__repr__()}'
+
+
+class GlobalServerInfo(object):
+    def __init__(self):
+        self.local_servers = []
+        self.local_server_idx = 0
+        self.resource_info = GlobalResourceInfo()
+        self.rank_to_server_map = {}
+        self.name_to_server_map = {}
+        self.registered_server_name = []
+
+    @property
+    def global_resource_info(self):
+        return self.resource_info(self.total_num_servers, self.total_num_gpus)
+
+    @property
+    def total_num_servers(self):
+        return len(self.local_servers)
+
+    @property
+    def total_num_gpus(self):
+        total_num_gpus_in_current_local_servers = 0
+        for local_server in self.local_servers:
+            total_num_gpus_in_current_local_servers += local_server.resource_info.num_gpus_in_server
+        return total_num_gpus_in_current_local_servers
+
+    def add(self, local_server: ServerInfo):
+        # Handle if name of server is already registered
+        if local_server.name in self.registered_server_name:
+            if local_server.ranks != self.name_to_server_map[local_server.name].ranks:
+                self._merge_local_servers(local_server.name, local_server.ranks)
+                return
+            else: # Skip add() as it is the same local server info
+                return
+        self.local_servers.append(local_server)
+        self.registered_server_name.append(local_server.name)
+
+        self.name_to_server_map[local_server.name] = local_server
+        for rank in local_server.ranks:
+            self.rank_to_server_map[rank] = local_server
+
+    def _merge_local_servers(self, name, ranks):
+        self.name_to_server_map[name].ranks.extend(
+            rank for rank in ranks \
+                if rank not in self.name_to_server_map[name].ranks
+        )
+        self.name_to_server_map[name].resource_info.num_gpus_in_server = len(self.name_to_server_map[name].ranks)
+        for rank in ranks:
+            self.rank_to_server_map[rank] = self.name_to_server_map[name]
+
+    def __repr__(self):
+        """ For DEBUG
+        repr_str = f'[GlobalServerInfo] total_num_gpus: {self.total_num_gpus} | ' \
+                   f'total_num_servers: {self.total_num_servers} | ' \
+                   f'rank_to_server_map: {self.rank_to_server_map} | ' \
+                   f'name_to_server_map: {self.name_to_server_map} | ' \
+                   f'registered_server_name: {self.registered_server_name} \n'
+        """
+        repr_str = f'[GlobalServerInfo] total_num_gpus: {self.total_num_gpus} | ' \
+                   f'total_num_servers: {self.total_num_servers} | ' \
+                   f'registered_server_name: {self.registered_server_name} | ' \
+                   f'[LocalServerInfos]: '
+        for local_server in self.local_servers:
+            repr_str += (local_server.__repr__() + ' ')
+        return repr_str
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        if self.local_server_idx < self.total_num_servers:
+            local_server = self.local_servers[self.local_server_idx]
+            self.local_server_idx += 1
+            return local_server
+        else:
+            self.local_server_idx = 0
+            raise StopIteration
+
+    def __eq__(self, other):
+        if isinstance(other, GlobalServerInfo):
+            result = self.total_num_servers == other.total_num_servers and \
+                    self.total_num_gpus == other.total_num_gpus and \
+                    self.registered_server_name.sort() == other.registered_server_name.sort()
+            if result == True:
+                # sort by server name
+                sorted_self_local_servers = sorted(self.local_servers, key=lambda x: x.name.lower())
+                sorted_other_local_servers = sorted(other.local_servers, key=lambda x: x.name.lower())
+                for self_server_info, other_server_info in zip(sorted_self_local_servers, sorted_other_local_servers):
+                    self_num_gpus_in_server = self_server_info.resource_info.num_gpus_in_server
+                    other_num_gpus_in_server = other_server_info.resource_info.num_gpus_in_server
+                    if self_server_info.name != other_server_info.name or self_num_gpus_in_server != other_num_gpus_in_server:
+                        result = False
+                        break
+            #print(f'__eq__(): - result: {result}')
+            return result
+        else:
+            #print(f'Class type is not equal: {type(self)} != {type(other)}')
+            return False
+
+    def __ne__(self, other):
+        return not self.__eq__(other)
diff --git a/torch/iidp/config/__init__.py b/torch/iidp/config/__init__.py
new file mode 100644
index 00000000000..62612ee4c94
--- /dev/null
+++ b/torch/iidp/config/__init__.py
@@ -0,0 +1,6 @@
+from . import model
+from . import api
+from . import examples
+
+from .configurator import *
+from .config_utils import *
\ No newline at end of file
diff --git a/torch/iidp/config/api/__init__.py b/torch/iidp/config/api/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/config/api/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/config/api/configuration_solver.py b/torch/iidp/config/api/configuration_solver.py
new file mode 100644
index 00000000000..a08d0e8c2de
--- /dev/null
+++ b/torch/iidp/config/api/configuration_solver.py
@@ -0,0 +1,167 @@
+import warnings
+import time
+
+from torch.iidp.config.configurator import IIDPConfig, IIDPFutureConfigurator
+from torch.iidp.cluster.cluster_manager import IIDPClusterManager
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.test.utils.server import build_mock_server_info, build_global_cluster_by_config_file
+from torch.iidp.test.utils.common_utils import get_possible_batch_size_across_cluster
+
+import argparse
+
+
+parser = argparse.ArgumentParser(description='Configuration Solver API')
+parser.add_argument('--config-file', '-c', type=str, required=True,
+                    help='Configuration file path (json)')
+parser.add_argument('--global-batch-size', '-gbs', default=None, type=int, required=True,
+                    help='Global batch size')
+parser.add_argument('--cluster', type=str, default=None, required=True,
+                    help='Server group in GPU cluster - ' \
+                    f'format: [hostname]:[num_gpus_in_server],[hostname]:[num_gpus_in_server], ..')
+# Optional
+parser.add_argument('--local-batch-size', '-lbs', default=None, type=int,
+                    help='Local batch size to be preserved')
+parser.add_argument('--weight-sync-method', type=str, default='overlap',
+                    choices=['overlap', 'sequential'],
+                    help='Weight synchronization method in IIDP')
+parser.add_argument('--fixed-resource','-f', action='store_true',
+                    help='Option to configure on --cluster setup only')
+
+
+def main():
+    args = parser.parse_args()
+
+    config_params = read_json(args.config_file)
+    # Handle config_params
+    if "batch_size_upper_bound" not in config_params.keys():
+        config_params["batch_size_upper_bound"] = args.global_batch_size
+    if "homo_servers" not in config_params.keys():
+        config_params["homo_servers"] = None
+    if "resource_alloc_unit" not in config_params.keys():
+        config_params["resource_alloc_unit"] = None
+    enable_adjust_lbs = False
+    if "enable_adjust_lbs" not in config_params.keys():
+        if args.local_batch_size is not None:
+            enable_adjust_lbs = False
+        else:
+            enable_adjust_lbs = True
+    else:
+        enable_adjust_lbs = bool(config_params["enable_adjust_lbs"] == "True")
+
+    if enable_adjust_lbs is True:
+        print(f'=====================================================')
+        print(f'[INFO] Dynamic local batch size')
+        print(f'=====================================================')
+        if args.local_batch_size is not None:
+            warnings.warn(
+                f'With dynamic local batch size mode, not fixed with '
+                f'--local-batch-size {args.local_batch_size}'
+            )
+    else:
+        print(f'=====================================================')
+        print(f'[INFO] Static local batch size')
+        print(f'=====================================================')
+
+    gpu_cluster_info = read_json(config_params["gpu_cluster_info"])
+
+    # Build Mock global and available server info
+    if args.cluster == 'all':
+        args.cluster = build_global_cluster_by_config_file(
+                config_params["available_servers"], gpu_cluster_info)
+    mock_available_server_info, mock_global_server_group = build_mock_server_info(args.cluster, gpu_cluster_info)
+    # Make arbitrary current global server info
+    server_list = args.cluster.split(',')
+    if len(server_list) > 1:
+        num_total_servers = len(server_list)
+        mock_current_cluster = ','.join(args.cluster.split(',')[:num_total_servers//2])
+    else:
+        mock_current_cluster = args.cluster
+    #print(f'[TEST] mock_current_cluster: {mock_current_cluster}')
+    mock_global_server_info, _ = build_mock_server_info(mock_current_cluster, gpu_cluster_info)
+
+    #print(f'[TEST] mock_available_server_info: {mock_available_server_info}')
+    mock_available_server_name_list = mock_global_server_group.keys()
+    mock_cluster_manager = IIDPClusterManager(
+            config_params["gpu_cluster_info"], mock_available_server_name_list,
+            homo_servers=config_params["homo_servers"],
+            resource_alloc_unit=config_params["resource_alloc_unit"])
+
+    #print('\n=============================== Mock Test ==================================\n')
+    mock_cluster_manager.global_server_info = mock_global_server_info
+    mock_cluster_manager.available_server_info = mock_available_server_info
+    mock_cluster_manager.candidate_server_infos = []
+    mock_cluster_manager._generate_candidate_resource_pool()
+
+    # Get local batch size
+    if args.local_batch_size is None:
+        try:
+            args.local_batch_size = get_possible_batch_size_across_cluster(
+                    config_params["comp_profile_dir"], list(mock_available_server_name_list))
+        except Exception as e:
+            print(e)
+            print('[ERROR] Argument --cluster must be re-configured')
+            exit(1)
+    #print(f'[DEBUG] args.local_batch_size: {args.local_batch_size}')
+
+    # Build Mock IIDPFutureConfigurator
+    local_config = IIDPConfig(args.local_batch_size, 1, 1, args.weight_sync_method)
+    mock_future_configurator = IIDPFutureConfigurator(
+        config_params["comp_profile_dir"],
+        config_params["comm_profile_dir"],
+        config_params["bucket_profile_dir"],
+        config_params["memory_profile_dir"],
+        local_config,
+        mock_cluster_manager.candidate_server_infos,
+        args.global_batch_size,
+        enable_adjust_lbs
+    )
+    mock_future_configurator.prepare(verbose=False)
+    # Search space of GPU configuration
+    best_throughput = 0
+    best_result = []
+    best_lbs = 0
+    print('[INFO] ====================== Search start! ======================\n')
+    search_start_time = time.time()
+    for server_id, candidate_server_info in enumerate(mock_cluster_manager.candidate_server_infos):
+        if args.fixed_resource is True and mock_cluster_manager.available_server_info != candidate_server_info:
+            continue
+        mock_future_configurator.update(server_id, 0, 0)
+        for local_batch_size, configurator in mock_future_configurator.configurators.items():
+            result = []
+            total_num_workers = int(args.global_batch_size / local_batch_size)
+            #print(f'[TEST] GBS: {args.global_batch_size} | LBS: {local_batch_size} | M: {total_num_workers}')
+            if args.global_batch_size != (local_batch_size * total_num_workers):
+                print(f'[WARNING] GBS: {args.global_batch_size} != LBS: {local_batch_size} * M: {total_num_workers} ==> skip!')
+                continue
+            if total_num_workers >= configurator.total_num_gpus:
+                result = configurator.dp_solver.solve(total_num_workers)
+                print(f'[TEST] LBS: {local_batch_size} | DP solution - [throughput, iteration time, number of workers/2, configuration set]: {result}')
+                if result[2]*2 != total_num_workers:
+                    print(f"[WARNING] resulting number of total workers: {result[2]*2}, but required one: {total_num_workers} ==> skip!")
+                    continue
+            else:
+                print(f'[TEST] GBS: {args.global_batch_size} | LBS: {local_batch_size} | '
+                      f'total_num_workers: {total_num_workers} < configurator.total_num_gpus: {configurator.total_num_gpus}')
+            if len(result)> 0 and result[0] > best_throughput:
+                best_throughput = result[0]
+                best_result = result
+                best_lbs = local_batch_size
+                print(f'** [DEBUG] intermediate solution - LBS: {best_lbs} | '
+                      f'[throughput, iteration time, number of workers/2, configuration set]: {best_result} **')
+    if len(best_result) == 0:
+        print(f'\n=====================================================')
+        print(f'[INFO] No solution - GBS: {args.global_batch_size}')
+        print(f'=====================================================\n')
+    else:
+        print(f'\n=====================================================')
+        print(f'[INFO] Solution - GBS: {args.global_batch_size} | LBS: {best_lbs} | config: {best_result[-1]}')
+        print(f'=====================================================')
+        print(f'[throughput, iteration time, number of workers/2, configuration set]: {best_result}')
+        print(f'=====================================================\n')
+    print(f'=====================================================')
+    print(f'[INFO] Search time (sec): {time.time()-search_start_time:.3f}')
+    print(f'=====================================================')
+
+
+if __name__ == '__main__':
+    main()
diff --git a/torch/iidp/config/api/user_config_checker.py b/torch/iidp/config/api/user_config_checker.py
new file mode 100644
index 00000000000..312f556da02
--- /dev/null
+++ b/torch/iidp/config/api/user_config_checker.py
@@ -0,0 +1,18 @@
+from torch.iidp.config.config_utils import check_user_config_is_valid
+
+import argparse
+
+parser = argparse.ArgumentParser(description='Configuration Checker API')
+parser.add_argument('--config-file', '-c', type=str, required=True,
+                    help='Configuration file path (json)')
+
+
+def main():
+    args = parser.parse_args()
+
+    print(f'===================== Checking config JSON file: {args.config_file} ===============================')
+    check_user_config_is_valid(args.config_file)
+
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
diff --git a/torch/iidp/config/config_utils.py b/torch/iidp/config/config_utils.py
new file mode 100644
index 00000000000..57ae1af880d
--- /dev/null
+++ b/torch/iidp/config/config_utils.py
@@ -0,0 +1,128 @@
+import torch
+
+from torch.iidp.utils.distributed import print_one_rank
+from torch.iidp.utils.json_utils import read_json
+
+import os
+import socket
+
+
+REQUIRED_CONFIG_JSON_KEYS = [
+    'memory_profile_dir',
+    'comp_profile_dir',
+    'comm_profile_dir',
+    'bucket_profile_dir',
+    'gpu_cluster_info',
+    'available_servers',
+    'batch_size_lower_bound',
+    'batch_size_upper_bound'
+]
+
+REQUIRED_CONFIG_FILES = [
+    'memory_profile_dir',
+    'comp_profile_dir',
+    'comm_profile_dir',
+    'bucket_profile_dir',
+    'gpu_cluster_info'
+]
+
+
+def print_table(table, len):
+    for i in range(0, len):
+        print_one_rank(str(table[i]))
+
+
+def check_server_profile_data_exists(profile_dir):
+    # At least one profile data exists for current server
+    is_server_profile_data_exist = False
+    for lbs in os.listdir(profile_dir):
+        static_lbs_profile_dir = os.path.join(profile_dir, lbs)
+        for server_name in os.listdir(static_lbs_profile_dir):
+            if server_name == socket.gethostname():
+                is_server_profile_data_exist = True
+    return is_server_profile_data_exist
+
+
+def sorted_listdir(dir_path):
+    return sorted(os.listdir(dir_path), key=lambda x: int(x))
+
+
+def sorted_config_map_by_rank(config_map):
+    return {key:value for key, value in \
+                sorted(config_map.items(), key=lambda item: int(item[0]))}
+
+
+def check_user_config_is_valid(config_file):
+    config_params = read_json(config_file)
+    print_one_rank(f'[INFO] configuration parameters: {config_params}')
+    if config_params.get('enable_adjust', None) is None:
+        config_params["enable_adjust"] = True
+    if isinstance(config_params["enable_adjust"], str): # handle to parse from json file
+        config_params["enable_adjust"] = bool(config_params["enable_adjust"] == "True")
+    if config_params["enable_adjust"] is False:
+        print_one_rank('[PASS] If enable_adjust = False, no need to check configuration')
+        return
+
+    # [CHECK 1] Required JSON keys
+    is_config_json_has_required_keys = set(REQUIRED_CONFIG_JSON_KEYS).issubset(set(config_params.keys()))
+    if is_config_json_has_required_keys is False:
+        missing_keys = ','.join(
+            list(filter(lambda elem: elem not in list(config_params.keys()), REQUIRED_CONFIG_JSON_KEYS))
+        )
+        raise ValueError(
+            f'[FAIL] Configuration JSON \"{config_file}\" misses the required keys: ```{missing_keys}``` '
+            f'among required keys: {REQUIRED_CONFIG_JSON_KEYS}')
+    else:
+        print_one_rank(f'[PASS] Configuration JSON \"{config_file}\" has all requied JSON keys: {REQUIRED_CONFIG_JSON_KEYS}')
+
+    # [CHECK 2] Required profile dir exists
+    for key in REQUIRED_CONFIG_FILES:
+        if os.path.exists(config_params[key]) is False:
+            raise ValueError(
+                f'[FAIL] File \"{config_params[key]}\" must exists')
+        else:
+            print_one_rank(f'[PASS] \"{config_params[key]}\" exists')
+
+    # [CHECK 3] Structure of Comp and mem profile dir
+    comp_profile_dir = config_params['comp_profile_dir']
+    memory_profile_dir = config_params['memory_profile_dir']
+    # Check the range of local batch sizes
+    if len(os.listdir(comp_profile_dir)) != len(os.listdir(memory_profile_dir)):
+        raise ValueError(
+            f'[FAIL] Computation and memory profile data for range of local batch size '
+            f'must be equal, but comp: {sorted_listdir(comp_profile_dir)} | mem: {sorted_listdir(memory_profile_dir)}')
+
+    # Check the same server profile data in computation and memory profile dir
+    for lbs in sorted_listdir(memory_profile_dir):
+        static_lbs_comp_profile_dir = os.path.join(comp_profile_dir, lbs)
+        static_lbs_mem_profile_dir = os.path.join(memory_profile_dir, lbs)
+        if os.listdir(static_lbs_comp_profile_dir) != os.listdir(static_lbs_mem_profile_dir):
+            raise ValueError(
+                f'[FAIL] For static LBS of {lbs}, server profile data is not consistent among comp and memory profile dir!\n'
+                f'comp profile dir - {static_lbs_comp_profile_dir} : {os.listdir(static_lbs_comp_profile_dir)}\n'
+                f'memory profile dir - {static_lbs_mem_profile_dir} : {os.listdir(static_lbs_mem_profile_dir)}')
+
+    # Check at least one profile data exists on current server. If not, the current server cannot be registered in available_servers
+    if check_server_profile_data_exists(comp_profile_dir) is False:
+        raise ValueError(f'[FAIL] No such computation profile data for {socket.gethostname()} '
+                         f'in {comp_profile_dir}')
+    if check_server_profile_data_exists(memory_profile_dir) is False:
+        raise ValueError(f'[FAIL] No such memory profile data for {socket.gethostname()} '
+                         f'in {memory_profile_dir}')
+    print_one_rank(f'[PASS] \"{comp_profile_dir}\" and \"{memory_profile_dir}\" has the right structure for configuration solver')
+
+    # [CHECK 4] GPU cluster info JSON data
+    gpu_cluster_info_file = config_params['gpu_cluster_info']
+    gpu_cluster_info = read_json(gpu_cluster_info_file)
+    if socket.gethostname() not in gpu_cluster_info.keys():
+        raise ValueError(
+            f'Current server: {socket.gethostname()} is not registered '
+            f'in gpu cluster info json file: {gpu_cluster_info_file}'
+        )
+    if torch.cuda.get_device_name() != gpu_cluster_info[socket.gethostname()]['type']:
+        raise ValueError(
+            f'Registerd GPU type in server {socket.gethostname()} in {gpu_cluster_info_file} '
+            f'```{gpu_cluster_info[socket.gethostname()]["type"]}``` is not equal to '
+            f'real GPU type in server: ```{torch.cuda.get_device_name()}```'
+        )
+    print_one_rank(f'[PASS] \"{gpu_cluster_info_file}\" registers the right GPU hardware for configuration solver')
\ No newline at end of file
diff --git a/torch/iidp/config/configurator.py b/torch/iidp/config/configurator.py
new file mode 100644
index 00000000000..56a42fec8ce
--- /dev/null
+++ b/torch/iidp/config/configurator.py
@@ -0,0 +1,896 @@
+import os
+import math
+import warnings
+
+import torch
+
+from torch.iidp.utils.distributed import get_allgather_value, print_one_rank
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.config.model.throughput.throughput_model import ThroughputModel
+from torch.iidp.cluster.resource import ResourceInfo, GlobalResourceInfo
+from torch.iidp.profiler.profiler import MAX_MEM_PROFILE_FILE_NAME
+from torch.iidp.config.config_utils import print_table, sorted_listdir, sorted_config_map_by_rank
+
+
+class IIDPConfig(object):
+    def __init__(self, lbs, num_models, accum_step, weight_sync_method):
+        self.lbs = lbs # static
+        self.num_models = num_models
+        self.accum_step = accum_step
+        self.weight_sync_method = weight_sync_method # static
+
+
+class IIDPConfigurator(object):
+    def __init__(self, comp_profile_dir, comm_profile_dir, bucket_profile_dir,
+                 memory_profile_dir, local_config, global_server_info,
+                 max_global_batch_size=-1, is_dynamic_local_batch_size=False, gpu=None):
+        if not isinstance(max_global_batch_size, int) or max_global_batch_size < 0:
+            raise TypeError(
+                f'Argument ```max_global_batch_size``` must be positive integer, '
+                f'but {max_global_batch_size} and type: {type(max_global_batch_size)}')
+        self.max_global_batch_size = max_global_batch_size
+        self.global_server_info = global_server_info
+        self.is_dynamic_local_batch_size = is_dynamic_local_batch_size
+        self.gpu = gpu
+        self.total_num_gpus = self.global_server_info.total_num_gpus
+
+        self.static_lbs = local_config.lbs if self.is_dynamic_local_batch_size is False else -1
+
+        self.iidp_config_map_in_cluster, self.updated_iidp_config_map = {}, {}
+        self._init_config_map_in_cluster(local_config)
+
+        all_server_names = []
+        for server_info in self.global_server_info:
+            all_server_names.append(server_info.name)
+        self.configurators = {}
+
+        if len(sorted_listdir(comp_profile_dir)) != len(sorted_listdir(memory_profile_dir)):
+            raise ValueError(
+                f'[ERROR][{self.__class__.__name__}] '
+                f'Computation and memory profile data for range of local batch size '
+                f'must be equal, but comp: {sorted_listdir(comp_profile_dir)} | mem: {sorted_listdir(memory_profile_dir)}')
+        # Create memory profile info (type: dict)
+        memory_profile_info = {}
+        for lbs in sorted_listdir(memory_profile_dir):
+            if not lbs in memory_profile_info.keys():
+                memory_profile_info[lbs] = {}
+            static_lbs_mem_profile_dir = os.path.join(memory_profile_dir, lbs)
+            static_lbs_comp_profile_dir = os.path.join(comp_profile_dir, lbs)
+            # Check the same server profile data in computation and memory profile dir
+            if os.listdir(static_lbs_comp_profile_dir) != os.listdir(static_lbs_mem_profile_dir):
+                raise ValueError(
+                    f'[ERROR] For static LBS of {lbs}, server profile data is not consistent among comp and memory profile dir!\n'
+                    f'comp profile dir - {static_lbs_comp_profile_dir} : {os.listdir(static_lbs_comp_profile_dir)}\n'
+                    f'memory profile dir - {static_lbs_mem_profile_dir} : {os.listdir(static_lbs_mem_profile_dir)}')
+            for server_name in os.listdir(static_lbs_mem_profile_dir):
+                max_memory_profile_file = os.path.join(
+                    static_lbs_mem_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+                memory_profile_json_data = read_json(max_memory_profile_file)
+                memory_profile_info[lbs][memory_profile_json_data['gpu_type']] = memory_profile_json_data['max_num_models']
+
+        # Instantiate configurator for static local batch size
+        for lbs in sorted_listdir(comp_profile_dir):
+            local_batch_size = int(lbs)
+            if self.is_dynamic_local_batch_size is False and local_batch_size != local_config.lbs:
+                continue
+            static_lbs_comp_profile_dir = os.path.join(comp_profile_dir, lbs)
+            # Check current local batch size can be supported by current global servers
+            print_one_rank(f'[{self.__class__.__name__}] all_server_names: {all_server_names} | '
+                    f'static_lbs_comp_profile_dir: {static_lbs_comp_profile_dir} | '
+                    f'os.listdir(static_lbs_comp_profile_dir): {os.listdir(static_lbs_comp_profile_dir)}', 'debug')
+            if not set(all_server_names).issubset(set(os.listdir(static_lbs_comp_profile_dir))):
+                print_one_rank(
+                    f'[{self.__class__.__name__}] local_batch_size: {local_batch_size} '
+                    f'is not supported by current cluster: {self.global_server_info} '
+                    f'==> skip it for IIDP configuration'
+                )
+                continue
+            if max_global_batch_size//local_batch_size < self.total_num_gpus:
+                print_one_rank(
+                    f'[{self.__class__.__name__}] local_batch_size: {local_batch_size} '
+                    f'is not satisfied with current total number of GPUs: {self.total_num_gpus} '
+                    f'==> skip it for IIDP configuration'
+                )
+                continue
+            static_lbs_memory_profile_info = memory_profile_info[lbs]
+            max_num_workers = max_global_batch_size//local_batch_size+1
+            self.configurators[local_batch_size] = IIDPStaticLocalBatchSizeConfigurator(
+                static_lbs_comp_profile_dir, comm_profile_dir, bucket_profile_dir,
+                static_lbs_memory_profile_info, local_batch_size,
+                local_config.weight_sync_method, global_server_info, max_num_workers
+            )
+        if local_config.lbs not in self.configurators.keys():
+            raise ValueError(
+                f'No such profile computation data for local batch size: '
+                f'{local_config.lbs}, but existing data: {self.configurators.keys()}'
+            )
+
+    def _init_config_map_in_cluster(self, local_config):
+        all_num_models_in_process_group = {
+            rank: value for rank, value in enumerate(get_allgather_value(local_config.num_models, self.gpu))
+        }
+        all_accum_step_in_process_group = {
+            rank: value for rank, value in enumerate(get_allgather_value(local_config.accum_step, self.gpu))
+        }
+        for rank, (num_models, accum_step) in enumerate(
+                zip(all_num_models_in_process_group.values(), all_accum_step_in_process_group.values())):
+            self.iidp_config_map_in_cluster[rank] = (num_models, accum_step)
+        #print(f'[DEBUG] self.iidp_config_map_in_cluster: {self.iidp_config_map_in_cluster}')
+
+    def update(self):
+        """
+        Lazy update - After IIDP trainer actually updates all of states,
+        configuration map will be updated by being called update()
+        """
+        if len(self.updated_iidp_config_map) == 0:
+            raise ValueError(f'self.updated_iidp_config_map must not be empty, '
+                             f'but {self.updated_iidp_config_map} => '
+                             f'solve_placement() may return empty config map')
+        self.iidp_config_map_in_cluster = self.updated_iidp_config_map
+
+    def solve_placement(self, global_batch_size, current_global_batch_size):
+        if self.is_dynamic_local_batch_size is False: # [EXPERIMENTAL] TODO: remove
+            warnings.warn(
+                "If dynamic local batch size is False, argument global_batch_size "
+                "indicates total number of workers = (global batch size / local batch size)"
+            )
+            # NOTE: If dynamic local batch size is False, argument ```global_batch_size```
+            # indicates total number of workers = (global batch size / local batch size)
+            total_num_workers = global_batch_size
+            new_iidp_config_map = {}
+            _, _, solved_iidp_config_map = \
+                self.configurators[self.static_lbs].solve_dynamic_programming(total_num_workers)
+            if solved_iidp_config_map == {}:
+                return new_iidp_config_map
+            for rank in range(self.total_num_gpus):
+                new_num_models = solved_iidp_config_map[rank][0] - self.iidp_config_map_in_cluster[rank][0]
+                new_accum_step = solved_iidp_config_map[rank][1] - self.iidp_config_map_in_cluster[rank][1]
+                if new_num_models == 0 and new_accum_step == 0:
+                    continue
+                new_iidp_config_map[rank] = (new_num_models, new_accum_step)
+            if new_iidp_config_map:
+                self.updated_iidp_config_map = solved_iidp_config_map
+            return new_iidp_config_map
+
+        new_iidp_config_map = {} # return value
+        best_throughput = -1
+        best_solved_iidp_config_map = {}
+        best_local_batch_size = 0
+        best_total_num_workers = 0
+        new_global_batch_size = 0
+        for local_batch_size, configurator in self.configurators.items():
+            print_one_rank(
+                f'[{self.__class__.__name__}] solve_placement() '
+                f'global_batch_size: {global_batch_size} | local_batch_size: {local_batch_size} | '
+                f'round(global_batch_size/local_batch_size): {round(global_batch_size/local_batch_size)}'
+            )
+            if global_batch_size > current_global_batch_size: # Increase global batch size
+                total_num_workers = round(global_batch_size/local_batch_size)
+                total_num_workers += (total_num_workers % 2) # SimiGrad constraint
+                if local_batch_size*total_num_workers <= current_global_batch_size:
+                    continue
+            else:
+                total_num_workers = round(global_batch_size/local_batch_size)
+                total_num_workers -= (total_num_workers % 2) # SimiGrad constraint
+                if local_batch_size*total_num_workers >= current_global_batch_size:
+                    continue
+            """ Ceil / Floor
+            print_one_rank(
+                f'[{self.__class__.__name__}] solve_placement() '
+                f'global_batch_size: {global_batch_size} | local_batch_size: {local_batch_size} | '
+                f'math.ceil(global_batch_size/local_batch_size): {math.ceil(global_batch_size/local_batch_size)}'
+            )
+            if is_increase_batch_size:
+                total_num_workers = math.ceil(global_batch_size/local_batch_size)
+                total_num_workers += (total_num_workers % 2) # SimiGrad constraint
+            else:
+                total_num_workers = math.floor(global_batch_size/local_batch_size)
+                total_num_workers -= (total_num_workers % 2) # SimiGrad constraint
+            """
+            #print_one_rank(f'[{self.__class__.__name__}] solve_placement() '
+            #               f'local_batch_size: {local_batch_size} | '
+            #               f'total_num_workers: {total_num_workers}', 'debug')
+            if total_num_workers < self.total_num_gpus:
+                print_one_rank(f'[{self.__class__.__name__}] solve_placement() '
+                               f'local_batch_size: {local_batch_size} | '
+                               f'total_num_workers: {total_num_workers} | '
+                               f'self.total_num_gpus: {self.total_num_gpus} ==> continue! ', 'debug')
+                continue
+            throughput, _, solved_iidp_config_map = configurator.solve_dynamic_programming(total_num_workers)
+            print_one_rank(f'[{self.__class__.__name__}] solve_placement() '
+                           f'local_batch_size: {local_batch_size} | '
+                           f'total_num_workers: {total_num_workers} | '
+                           f'throughput: {throughput} | '
+                           f'solved_iidp_config_map: {solved_iidp_config_map}', 'debug')
+            if solved_iidp_config_map == {}:
+                continue
+            if throughput > best_throughput:
+                best_local_batch_size = local_batch_size
+                best_total_num_workers = total_num_workers
+                best_throughput = throughput
+                best_solved_iidp_config_map = solved_iidp_config_map
+                new_global_batch_size = best_local_batch_size * best_total_num_workers
+
+        if best_solved_iidp_config_map == {}:
+            return new_iidp_config_map, best_local_batch_size, new_global_batch_size
+        print_one_rank('==================================================================')
+        print_one_rank(
+            f'[{self.__class__.__name__}] solve_placement()  ** best config ** | '
+            f'local_batch_size: {best_local_batch_size} | '
+            f'total_num_workers: {best_total_num_workers} | '
+            f'throughput: {best_throughput:.2f} | '
+            f'solved_iidp_config_map: {best_solved_iidp_config_map}', 'info')
+        print_one_rank('==================================================================')
+        try:
+            for rank in range(self.total_num_gpus):
+                new_num_models = best_solved_iidp_config_map[rank][0] - self.iidp_config_map_in_cluster[rank][0]
+                new_accum_step = best_solved_iidp_config_map[rank][1] - self.iidp_config_map_in_cluster[rank][1]
+                if new_num_models == 0 and new_accum_step == 0:
+                    continue
+                new_iidp_config_map[rank] = (new_num_models, new_accum_step)
+        except Exception as e:
+            print_one_rank(f'[{self.__class__.__name__}] solve_placement() | rank: {rank} | '
+                            f'best_solved_iidp_config_map: {best_solved_iidp_config_map} | '
+                            f'self.iidp_config_map_in_cluster: {self.iidp_config_map_in_cluster}', 'debug')
+            raise e
+        if new_iidp_config_map:
+            self.updated_iidp_config_map = best_solved_iidp_config_map
+        return new_iidp_config_map, best_local_batch_size, new_global_batch_size
+
+
+class IIDPStaticLocalBatchSizeConfigurator(object):
+    def __init__(self, comp_profile_dir, comm_profile_dir, bucket_profile_dir,
+                 memory_profile_info, local_batch_size, weight_sync_method, global_server_info, max_num_workers=-1):
+        self.comp_profile_dir = comp_profile_dir
+        self.comm_profile_dir = comm_profile_dir
+        self.bucket_profile_dir = bucket_profile_dir
+        self.global_server_info = global_server_info
+        self.total_num_gpus = self.global_server_info.total_num_gpus
+        self.weight_sync_method = weight_sync_method
+        self.local_batch_size = local_batch_size
+        self.throughput_models = {}
+        self.all_max_num_local_models_in_process_group = memory_profile_info
+        self.max_num_workers = max_num_workers
+
+        self._build_throughput_model()
+
+        self._init_dp_solver()
+
+    def _build_throughput_model(self):
+        """
+        Assumption: Profile data of all servers must be placed on 'comp_profile_dir/{server name}'
+        """
+        for server_info in self.global_server_info:
+            local_comp_profile_dir = os.path.join(self.comp_profile_dir, server_info.name)
+            if server_info.name not in self.throughput_models.keys():
+                self.throughput_models[server_info.name] = \
+                    ThroughputModel(local_comp_profile_dir, self.comm_profile_dir, self.bucket_profile_dir)
+
+    def _init_dp_solver(self):
+        self.dp_solver = DynamicProgrammingSolver(
+            self.local_batch_size,
+            self.weight_sync_method,
+            self.throughput_models,
+            self.all_max_num_local_models_in_process_group,
+            self.global_server_info,
+            self.max_num_workers
+        )
+
+    def estimate_time(self, server_name, num_models, accum_step,
+                      resource_info: ResourceInfo, global_resource_info: GlobalResourceInfo):
+        """Estimate local server iteration time"""
+        iter_time, _ = self.throughput_models[server_name].evaluate(
+            num_models, accum_step, self.weight_sync_method, resource_info, global_resource_info)
+        return iter_time, _
+
+    def estimate_throughput(self, total_num_models: int, iter_time: float):
+        global_batch_size = self.local_batch_size * total_num_models
+        thp = global_batch_size / iter_time
+        return thp
+
+    def update(self):
+        """
+        Lazy update - After IIDP trainer actually updates all of states,
+        configuration map will be updated by being called update()
+        """
+        self.iidp_config_map_in_cluster = self.updated_iidp_config_map
+
+    def solve_dynamic_programming(self, total_num_workers):
+        """
+        Args:
+            total_num_workers (int): Total number of virtual workers to configure GBS
+
+        Returns:
+            List: [throughput: float, iteration time: float, configuration map: dict - {rank: (num_models, accum_step)}]
+        """
+        throughput, iter_time, new_config_map = -1, -1, {}
+        try:
+            throughput, iter_time, _, new_config_set = self.dp_solver.solve(total_num_workers)
+        except: # No solution
+            return [throughput, iter_time, new_config_map]
+        #print_one_rank(f'[DEBUG] configurator - new_config_set by DP solver: {new_config_set}')
+        new_config_map = sorted_config_map_by_rank(
+                            self.dp_solver.generate_config_map(new_config_set))
+        return [throughput, iter_time, new_config_map]
+
+
+class DynamicProgrammingSolver(object):
+    def __init__(self, local_batch_size, weight_sync_method, throughput_models,
+                 all_max_num_models_info, global_server_info, max_num_workers=-1):
+        self.local_batch_size = local_batch_size
+        self.weight_sync_method = weight_sync_method
+        self.throughput_models = throughput_models
+        # NOTE: all_max_num_local_models_in_process_group = {'device name (str)': max number of VSWs (int)}
+        self.all_max_num_local_models_in_process_group = all_max_num_models_info
+        self.global_server_info = global_server_info
+        self.total_num_gpus = self.global_server_info.global_resource_info.total_num_gpus
+        self.max_num_workers = max_num_workers
+        self.A = self.create_table(max_num_workers)
+
+    def _split_rank_for_simigrad(self, arr, size):
+        arrays = []
+        while len(arr) > size:
+            pice = arr[:size]
+            arrays.append(pice)
+            arr   = arr[size:]
+        arrays.append(arr)
+        return arrays
+
+    def _generate_config_name(self, server_name, ranks, num_models, accum_step):
+        """e.g, server1:0,VSW:3,GA:1 -> server1: ranks: [0, 1] (VSW, GA) = (3,1)"""
+        return server_name+':'+str(ranks[0])+','+'VSW:'+str(num_models)+','+'GA:'+str(accum_step)
+
+    def generate_config_map(self, config_set: list):
+        config_map = {} # {rank: (num_models, accum_step)}
+        for config_name in config_set:
+            region_str, num_models_str, accum_step_str = config_name.split(',')
+            head_rank = self.convert_config_str_to_int(region_str)
+            num_models = self.convert_config_str_to_int(num_models_str)
+            accum_step = self.convert_config_str_to_int(accum_step_str)
+            # For SimiGrad
+            config_map[head_rank] = (num_models, accum_step)
+            config_map[head_rank+1] = (num_models, accum_step)
+        return config_map
+
+    def estimate_throughput(self, total_num_models: int, iter_time: float):
+        global_batch_size = self.local_batch_size * total_num_models
+        thp = global_batch_size / iter_time
+        return thp
+
+    def get_pruned_table_by_hash(self, A):
+        """
+            Table element: [iter time, number of workers(= number of VSWs * GA), config_name]
+            NOTE: Important assumption - sort by iteration time in increasing order
+            Prune by Hashing => The first unique config has the fastest iter time
+        """
+        """
+        print('========= Before pruning ==============')
+        print_table(A, len(A))
+        print('=======================================')
+        """
+        pruned_A = []
+        pruned_A_hashmap = {}
+        for A_elem in A:
+            _, num_worker, config_name = A_elem
+            if num_worker == 1:
+                pruned_A.append(A_elem)
+            else:
+                config_region = config_name.split(',')[0]
+                hash_key = str(num_worker) + config_region
+                if hash_key not in pruned_A_hashmap:
+                    pruned_A_hashmap[hash_key] = A_elem
+        pruned_A.extend(pruned_A_hashmap.values())
+        """
+        print('========= After pruning ==============')
+        print(f'Number of search space '
+            f'before pruning: {len(A)} | '
+            f'after pruning: {len(pruned_A)}')
+        print_table(pruned_A, len(pruned_A))
+        """
+        return pruned_A
+
+    def get_pruned_table(self, A):
+        pruned_A = []
+        #print('========> Before pruning ==============')
+        #print_table(A, len(A))
+
+        for A_elem in A:
+            iter_time, num_worker, config_name = A_elem
+            if num_worker == 1:
+                pruned_A.append(A_elem)
+            else:
+                remove_idx = -1
+                is_append = True
+                # Search the candidate of filtering
+                for idx, pruned_data in enumerate(pruned_A):
+                    config_region = config_name.split(',')[0]
+                    # Pruning space: Same number of virtual workers within the same regions (2-GPU unit allocation)
+                    if config_region in pruned_data[2] and num_worker == pruned_data[1]:
+                        # 1-1) Add a new data with faster time among the same processing configuration
+                        if iter_time < pruned_data[0]:
+                            remove_idx = idx
+                            pruned_A.append(A_elem)
+                            break
+                        # 2) Not add data with slower time
+                        else:
+                            is_append = False
+
+                if is_append:
+                    if remove_idx == -1: # 3) Add a new data
+                        pruned_A.append(A_elem)
+                    else: # 1-2) Remove data for previous slower configuration
+                        pruned_A.remove(pruned_A[remove_idx])
+        """
+        print('========> After pruning ==============')
+        print(f'Number of search space '
+            f'before pruning: {len(A)} | '
+            f'after pruning: {len(pruned_A)}')
+        print_table(pruned_A, len(pruned_A))
+        """
+        return pruned_A
+
+    def create_table(self, max_num_workers=-1):
+        """Table: [iter time, number of workers(= number of VSWs * GA), config_name]"""
+        if max_num_workers < self.total_num_gpus:
+            raise ValueError(f"Argument max_num_workers: {max_num_workers} < self.total_num_gpus: {self.total_num_gpus}")
+        #print(f'[DEBUG] create_table() - max_num_workers: {max_num_workers}')
+        A = []
+        MAX_GA_STEPS = 1000
+        #print(f'[DEBUG] MAX_GA_STEPS: {MAX_GA_STEPS}')
+        for server_info in self.global_server_info:
+            server_name = server_info.name
+            gpu_type = server_info.resource_info.device_name
+            split_ranks = self._split_rank_for_simigrad(server_info.ranks, 2)
+            for i, ranks in enumerate(split_ranks):
+                if os.getenv("EASYSCALE") == "1" or os.getenv("SIMIGRAD") == "1":
+                    max_num_models = 1
+                else:
+                    try:
+                        max_num_models = self.all_max_num_local_models_in_process_group[gpu_type]
+                    except Exception as e:
+                        print(f'self.all_max_num_local_models_in_process_group: {self.all_max_num_local_models_in_process_group}')
+                        print(f'self.global_server_info: {self.global_server_info}')
+                        raise e
+
+                if max_num_workers > 0:
+                    min_running_workers = self.total_num_gpus-2 # -2 means excluding current 2-GPUs allocation
+                    pruned_max_num_models = max(min((max_num_workers-min_running_workers)//2, max_num_models), 1)
+                    assert pruned_max_num_models >= 1
+                    max_num_models = pruned_max_num_models
+                for num_models in range(1, max_num_models+1):
+                    if max_num_workers > 0:
+                        pruned_max_accum_step = min(MAX_GA_STEPS, ((max_num_workers-min_running_workers)//2//num_models))
+                        assert pruned_max_accum_step > 0, f"{pruned_max_accum_step} | {num_models}"
+                        max_accum_step = pruned_max_accum_step
+                    else:
+                        max_accum_step = MAX_GA_STEPS
+                    """
+                    print_one_rank(f'[DEBUG] =============> create_table() | '
+                                   f'server_name: {server_name} | '
+                                   f'gpu_type: {gpu_type} | '
+                                   f'max_num_models: {max_num_models} | '
+                                   f'max_accum_step: {max_accum_step}')
+                    """
+                    for accum_step in range(1, max_accum_step+1):
+                        iter_time, _ = self.throughput_models[server_name].evaluate(
+                            num_models, accum_step, self.weight_sync_method,
+                            server_info.resource_info, self.global_server_info.global_resource_info
+                        )
+                        config_name = self._generate_config_name(server_name, ranks, num_models, accum_step)
+                        A.append([iter_time, num_models * accum_step, config_name])
+        #print_one_rank(f'[DEBUG] create_table() - A: {A}')
+        #print_table(A, len(A))
+        # NOTE: As at least one worker must exists on every GPUs, one worker must be put ahead of table
+        A.sort(key=lambda x: x[1])
+        A_with_one_worker = A[:self.total_num_gpus//2]
+        A_over_one_worker = A[self.total_num_gpus//2:]
+        # NOTE: Important assumption - sort by iteration time in increasing order
+        A_with_one_worker.sort(key=lambda x: x[0])
+        A_over_one_worker.sort(key=lambda x: x[0])
+        #print_table(A_with_one_worker, len(A_with_one_worker))
+        #print_table(A_over_one_worker, len(A_over_one_worker))
+        A = A_with_one_worker + A_over_one_worker
+        #print_table(A, len(A))
+        # ==== case that is NOT required to allocate all of GPUs in global server info =======
+        # NOTE: Important assumption - sort by iteration time in increasing order
+        #A.sort(key=lambda x: x[0])
+        # ====================================================================================
+        return self.get_pruned_table_by_hash(A)
+
+    def convert_config_str_to_int(self, config_str):
+        return int(config_str.split(':')[-1])
+
+    def _combine_same_region_config(self, prev_candidate_config_set: list, curr_config_name: str):
+        if not isinstance(prev_candidate_config_set, list):
+            raise TypeError(f"prev_candidate_config_set must be list type, but {type(prev_candidate_config_set)}")
+        #print(f'[DEBUG] _combine_same_region_config() - prev_candidate_config_set: {prev_candidate_config_set} | curr_config_name: {curr_config_name}')
+        curr_region, curr_num_models_str, curr_accum_step_str = curr_config_name.split(',')
+        curr_num_models = self.convert_config_str_to_int(curr_num_models_str)
+        curr_accum_step = self.convert_config_str_to_int(curr_accum_step_str)
+        new_config_set = []
+        for config_name in prev_candidate_config_set:
+            prev_region = config_name.split(',')[0]
+            if prev_region != curr_region:
+                new_config_set.append(config_name)
+        new_curr_config_name = ','.join([curr_region, 'VSW:' +str(curr_num_models), 'GA:'+str(curr_accum_step)])
+        new_config_set.append(new_curr_config_name)
+        new_config_set.sort()
+        return new_config_set
+
+    def _get_curr_config_set(self, prev_candidate_config_set: list, curr_config_name: str) -> list:
+        return self._combine_same_region_config(prev_candidate_config_set, curr_config_name)
+
+    def _get_curr_num_workers(self, config_set: list):
+        total_num_models = 0
+        for config_name in config_set:
+            _, curr_num_models_str, curr_accum_step_str = config_name.split(',')
+            curr_num_models = self.convert_config_str_to_int(curr_num_models_str)
+            curr_accum_step = self.convert_config_str_to_int(curr_accum_step_str)
+            total_num_models += (curr_num_models*curr_accum_step)
+        return total_num_models
+
+    def solve(self, total_num_workers):
+        PRINT_DEBUG = False
+        #print(f'[INFO] solve() - total_num_workers: {total_num_workers}')
+        #print_table(self.A, len(self.A))
+        """
+        NOTE: [Important] With SimiGrad, number of workers must be double because one configuration assumes allocation of 2 GPUs
+        A[i][0] = iter time, A[i][1] = number of (virtual) workers, A[i][2] = config name on one allocation
+        [Dynamic Programming] Table for DP
+        each element has [iter_time, config_name]
+        col: Candidate ranks to be assigned new virtual workers
+        row: Candidate number of virtual workers to be assigned
+        -----------------------------------------------
+                            |                real idx (number of virtual workers)                |
+        ____________________|                               0                                    | 1(2) | 2(4) | 3(6) ..
+        server1:0,VSW:1,GA:1| [throughput, iter time, number of workers, config set: List]   ..  |
+        server1:2,VSW:1,GA:1|                                                                    |
+        server2:4,VSW:1,GA:1|
+        server2:6,VSW:1,GA:1|
+        -----------------------------------------------
+        'config_name' is a unit of 2 GPUs assignment
+            e.g, config_name = server1:0,VSW:3,GA:1 ==> server1:[0,1] -> (VSW, GA) = (3,1)
+        DP element: [throughput, iter time, number of workers, config set]
+        DP[i][j][0] = throughput, DP[i][j][1] = iter_time, DP[i][j][2] = number of (virtual) workers, DP[i][j][3] = [config_name, ..]
+        """
+        if len(self.A) == 0:
+            print('[INFO] No table for Dynamic Programming')
+            return
+        dp_row = total_num_workers//2+1 # Purpose of +1 is that the row index of DP table indicates the number of current workers
+        dp_col = len(self.A)
+        if PRINT_DEBUG is True:
+            print_one_rank('============ DP table ============')
+            print_one_rank(f'dp_col: {dp_col} | dp_row: {dp_row}')
+            print_table(self.A, dp_col)
+            print_one_rank('==================================')
+
+        # DP element: [throughput, iter time, number of workers, config set]
+        dp_elem = [0, 0, 0, []]
+        DP = [[dp_elem for _ in range(dp_row)] for _ in range(dp_col)]
+        # Initialize table for DP
+        for j in range(1, dp_row):
+            if self.A[0][1] <= j:
+                iter_time = self.A[0][0]
+                num_workers = self.A[0][1]
+                config_name = [self.A[0][2]]
+                thp = self.estimate_throughput(num_workers*2, iter_time)
+                DP[0][j] = [thp, iter_time, num_workers, config_name]
+
+        # [ Main algorithm ]
+        # NOTE: Assumption: A - sorted by iteration time in increasing order (reference: create_table())
+        # previous configuration with prev_max_workers has optimal sub-structure => DP[i-1][prev_max_workers]
+        prev_max_workers = 1
+        for i in range(1, dp_col): # i: Candidate configuration of GPU allocation
+            if self.A[i][1] > dp_row: # Number of workers in a new configuration (self.A[i][1]) is over than the required total number of workers (dp_row)
+                #break
+                continue
+            # Update current DP table to previous optimal configuration (<=prev_max_workers)
+            if i == 1:
+                DP[i][prev_max_workers] = DP[i-1][prev_max_workers]
+                prev_max_workers+=1
+            for j in range(1, prev_max_workers):
+                DP[i][j] = DP[i-1][j]
+            # Traverse right direction (toward increasing number of workers)
+            for j in range(prev_max_workers, dp_row): # j: Candidate number of (virtual) workers
+                curr_config_thp, prev_config_thp = 0, 0
+                # [ Main logic for DP ] - combine previous set with a new configuration and compute objective value (throughput)
+                curr_config_set = self._get_curr_config_set(DP[i-1][j][3], self.A[i][2])
+                curr_max_iter_time = max(self.A[i][0], DP[i-1][j][1])
+                curr_num_workers = self._get_curr_num_workers(curr_config_set)
+                curr_config_thp = self.estimate_throughput(curr_num_workers*2, curr_max_iter_time)
+                """
+                print(f'[DEBUG] ===========> i: {i} | j: {j} | new config name: {self.A[i][2]} | '
+                        f'curr config set: {curr_config_set} | '
+                        f'curr_max_iter_time: {curr_max_iter_time} | '
+                        f'curr num workers: {curr_num_workers} | '
+                        f'curr_config_thp: {curr_config_thp}')
+                """
+                prev_config_thp = DP[i-1][j][0]
+                #print(f'[DEBUG] curr thp: {curr_config_thp} | prev config set: {DP[i-1][j][3]} = prev thp: {prev_config_thp}')
+                # [ Main logic for DP ] - compare objective value (throughput) in previous optimal sub-problem
+                if (curr_config_thp > prev_config_thp and curr_num_workers < j) or curr_num_workers == j:
+                    DP[i][j] = [curr_config_thp, curr_max_iter_time, curr_num_workers, curr_config_set]
+                    #print(f'[DEBUG] i: {i} | j: {j} | current new config update DP[i][j] = {DP[i][j]}')
+                    if curr_num_workers == j:
+                        prev_max_workers = DP[i][j][2] + 1
+                        for k in range(j+1, dp_row):
+                            DP[i][k] = DP[i][j]
+                        break
+                else:
+                    DP[i][j] = DP[i-1][j]
+            if PRINT_DEBUG is True:
+                print(f'************************** i: {i} *******************************')
+                print(f'************************** config: {self.A[i][-1]} *******************************')
+                for k in range(dp_row):
+                    print(f'j:{k} - {DP[i][k][-1]} | {DP[i][k][2]}') # print configuration
+                print('******************************************************************')
+
+        if PRINT_DEBUG is True:
+            print('[DEBUG] solve() ============================= Final DP table =============================')
+            print(f'[DEBUG] i: {i} | dp_col: {dp_col} | dp_row: {dp_row}')
+            print_one_rank('============ DP table ============')
+            print_one_rank(f'dp_col: {dp_col} | dp_row: {dp_row}')
+            print_table(self.A, dp_col)
+            print_one_rank('==================================')
+
+        solution = None
+        for s in range(dp_col, 0, -1):
+            is_total_num_workers_required = (DP[s-1][dp_row-1][2] == dp_row-1)
+            is_num_gpu_allocation_required = (len(DP[s-1][dp_row-1][-1]) == self.total_num_gpus//2)
+            if is_total_num_workers_required and is_num_gpu_allocation_required:
+                solution = DP[s-1][dp_row-1]
+                break
+        if solution is None:
+            raise AssertionError(f'[ERROR] DP Solution for total_num_workers: {total_num_workers} does not exist')
+        return solution
+
+
+class IIDPFutureConfigurator(object):
+    def __init__(self, comp_profile_dir, comm_profile_dir, bucket_profile_dir,
+                 memory_profile_dir, local_config, candidate_global_server_infos,
+                 max_global_batch_size=-1, is_dynamic_local_batch_size=False,
+                 gpu=None, utility_type='memory'):
+        self.comp_profile_dir = comp_profile_dir
+        self.comm_profile_dir = comm_profile_dir
+        self.bucket_profile_dir = bucket_profile_dir
+
+        if len(sorted_listdir(comp_profile_dir)) != len(sorted_listdir(memory_profile_dir)):
+            raise ValueError(
+                f'[ERROR][{self.__class__.__name__}] '
+                f'Computation and memory profile data for range of local batch size '
+                f'must be equal, but comp: {sorted_listdir(comp_profile_dir)} | mem: {sorted_listdir(memory_profile_dir)}')
+        # Create memory profile info (type: dict)
+        self.memory_profile_info = {}
+        for lbs in sorted_listdir(memory_profile_dir):
+            if not lbs in self.memory_profile_info.keys():
+                self.memory_profile_info[lbs] = {}
+            static_lbs_mem_profile_dir = os.path.join(memory_profile_dir, lbs)
+            # Check the same server profile data in computation and memory profile dir
+            if os.listdir(os.path.join(self.comp_profile_dir, lbs)) != os.listdir(static_lbs_mem_profile_dir):
+                raise ValueError(
+                    f'[ERROR] For static LBS of {lbs}, server profile data is not consistent among comp and memory profile dir!\n'
+                    f'comp profile dir - {os.path.join(self.comp_profile_dir, lbs)} : {os.listdir(os.path.join(self.comp_profile_dir, lbs))}\n'
+                    f'memory profile dir - {static_lbs_mem_profile_dir} : {os.listdir(static_lbs_mem_profile_dir)}')
+            for server_name in os.listdir(static_lbs_mem_profile_dir):
+                max_memory_profile_file = os.path.join(
+                    static_lbs_mem_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+                memory_profile_json_data = read_json(max_memory_profile_file)
+                self.memory_profile_info[lbs][memory_profile_json_data['gpu_type']] = memory_profile_json_data['max_num_models']
+
+        self.local_config = local_config
+        if not isinstance(max_global_batch_size, int) or max_global_batch_size < 0:
+            raise TypeError(
+                f'Argument ```max_global_batch_size``` must be positive integer, '
+                f'but {max_global_batch_size} and type: {type(max_global_batch_size)}')
+        self.max_global_batch_size = max_global_batch_size
+        # NOTE: candidate global server info is defined by list type in [torch/iidp/cluster/cluster_manager.py]
+        if not isinstance(candidate_global_server_infos, list):
+            candidate_global_server_infos = [candidate_global_server_infos]
+        self.candidate_global_server_infos = candidate_global_server_infos
+        self.is_dynamic_local_batch_size = is_dynamic_local_batch_size
+        self.gpu = gpu
+
+        self.static_lbs = local_config.lbs if self.is_dynamic_local_batch_size is False else -1
+        # [Set of IIDPConfigurator for all global server info]
+        # built once by prepare() at the initial phase of elastic training
+        self.all_candidate_server_configurators = {}
+        # [IIDPConfigurator for each global server info]
+        # IIDPConfigurator => set of IIDPStaticLocalBatchSizeConfigurator
+        # updated by update()
+        # used in estimate_time_and_utility()
+        self.configurators = {}
+        self.utility_type = utility_type
+
+        self._prepared = False
+        self.current_global_batch_size = 0
+        self.current_local_batch_size = 0
+        self._update_lock = False
+
+    def state_dict(self):
+        return self.all_candidate_server_configurators
+
+    def prepare(self, verbose=True):
+        if len(self.all_candidate_server_configurators) == 0:
+            self._init_configurators(verbose)
+        self._prepared = True
+
+    def _init_configurators(self, verbose=True):
+        print_one_rank(
+            '==============================================================\n'
+            f'[{self.__class__.__name__}] Start to initialize configurators for candidate servers\n'
+            f'[{self.__class__.__name__}] Total number of candidate severs to build: {len(self.candidate_global_server_infos)}\n'
+            f'[{self.__class__.__name__}] verbose: {verbose}\n'
+            f'[{self.__class__.__name__}] It might take time ..\n'
+            '=============================================================='
+        )
+        for server_id, global_server_info in enumerate(self.candidate_global_server_infos):
+            total_num_gpus = global_server_info.total_num_gpus
+            self.all_candidate_server_configurators[server_id] = {}
+            configurators = {}
+            all_server_names = []
+            for server_info in global_server_info:
+                all_server_names.append(server_info.name)
+            # Instantiate configurator for static local batch size
+            for lbs in sorted_listdir(self.comp_profile_dir):
+                try:
+                    local_batch_size = int(lbs)
+                except:
+                    print_one_rank(
+                        f'[{self.__class__.__name__}] init_configurators() '
+                        f'Computation profile dir structure is not suitable for local batch size: '
+                        f'{sorted_listdir(self.comp_profile_dir)}', 'error')
+                    exit(1)
+                if self.is_dynamic_local_batch_size is False and local_batch_size != self.local_config.lbs:
+                    if verbose is True:
+                        print_one_rank(
+                            f'[{self.__class__.__name__}] init_configurators() '
+                            f'self.is_dynamic_local_batch_size: {self.is_dynamic_local_batch_size} | '
+                            f'self.local_config.lbs: {self.local_config.lbs} | '
+                            f'local_batch_size: {local_batch_size} ===> continue!!', 'debug')
+                    continue
+                static_lbs_comp_profile_dir = os.path.join(self.comp_profile_dir, lbs)
+                # Check current local batch size can be supported by current global servers
+                #print_one_rank(f'[{self.__class__.__name__}] all_server_names: {all_server_names} | '
+                #        f'static_lbs_comp_profile_dir: {static_lbs_comp_profile_dir} | '
+                #        f'os.listdir(static_lbs_comp_profile_dir): {os.listdir(static_lbs_comp_profile_dir)}', 'debug')
+                if not set(all_server_names).issubset(set(os.listdir(static_lbs_comp_profile_dir))):
+                    if verbose is True:
+                        print_one_rank(
+                            f'[{self.__class__.__name__}] init_configurators() '
+                            f'local_batch_size: {local_batch_size} is not supported '
+                            f'by current cluster: {global_server_info} ==> skip it for IIDP configuration'
+                        )
+                    continue
+                if self.max_global_batch_size//local_batch_size < total_num_gpus:
+                    if verbose is True:
+                        print_one_rank(
+                            f'[{self.__class__.__name__}] init_configurators() '
+                            f'local_batch_size: {local_batch_size} '
+                            f'is not satisfied with current total number of GPUs: {total_num_gpus} '
+                            f'==> skip it for IIDP configuration'
+                        )
+                    continue
+                static_lbs_memory_profile_info = self.memory_profile_info[lbs]
+                max_num_workers = self.max_global_batch_size//local_batch_size+1
+                configurators[local_batch_size] = IIDPStaticLocalBatchSizeConfigurator(
+                    static_lbs_comp_profile_dir, self.comm_profile_dir, self.bucket_profile_dir,
+                    static_lbs_memory_profile_info, local_batch_size,
+                    self.local_config.weight_sync_method, global_server_info, max_num_workers
+                )
+            self.all_candidate_server_configurators[server_id] = configurators
+
+            if verbose is True:
+                final_result_log_str = \
+                    f'[{server_id} / {len(self.candidate_global_server_infos)}] ' \
+                    f'server id: {server_id} | ' \
+                    f'all_server_names: {all_server_names} | ' \
+                    f'total number of GPUs: {global_server_info.total_num_gpus}'
+                length = len(final_result_log_str) + 1
+                print_one_rank('=' * length)
+                print_one_rank(final_result_log_str)
+                print_one_rank('=' * length)
+
+        # NOTE: Check if at least one of the candidate servers can support the initial local batch size
+        is_support_initial_lbs = False
+        for server_id, configurators in self.all_candidate_server_configurators.items():
+            if self.local_config.lbs in configurators.keys():
+                is_support_initial_lbs = True
+        if is_support_initial_lbs is False:
+            raise ValueError(
+                f'No candidate server to support such initial local batch size: {self.local_config.lbs}'
+            )
+        print_one_rank(
+            '==============================================================\n'
+            f'[{self.__class__.__name__}] Finish to initialize configurators for candidate servers\n'
+            '=============================================================='
+        )
+
+    def update(self, server_id, local_batch_size, global_batch_size):
+        if self._prepared is False:
+            raise RuntimeError(
+                f'[ERROR][{self.__class__.__name__}] update() must be called '
+                f'after prepare() is called'
+            )
+        # Update current local & global batch size -> must be preserved for next epoch
+        self.current_global_batch_size = global_batch_size
+        self.current_local_batch_size = local_batch_size
+        # Update configurators for each static local batch size with global resource
+        self.configurators = self.all_candidate_server_configurators[server_id]
+        self._update_lock = True
+
+    def estimate_time_and_utility(self, global_batch_size, iteration, remaining_num_dataset):
+        if self._update_lock is False:
+            raise RuntimeError(
+                f'[ERROR][{self.__class__.__name__}] estimate_time_and_utility() must be called '
+                f'after update() is called'
+            )
+        min_duration, ret_utility, ret_solved_iidp_config_map, \
+            ret_expected_gbs, ret_expected_step = math.inf, -1, {}, global_batch_size, 0
+        best_local_batch_size = self.current_local_batch_size # for debugging
+        for local_batch_size, configurator in self.configurators.items():
+            # NOTE: current LBS and GBS must be preserved for next epoch
+            if global_batch_size == self.current_global_batch_size:
+                if self.current_local_batch_size != local_batch_size:
+                    continue
+                total_num_workers = global_batch_size // self.current_local_batch_size
+                if total_num_workers >= configurator.total_num_gpus and iteration > 0:
+                    _, iter_time, solved_iidp_config_map = configurator.solve_dynamic_programming(total_num_workers)
+                    if solved_iidp_config_map == {}: # Candidate global server cannot support current global batch size
+                        return min_duration, ret_utility, ret_solved_iidp_config_map, ret_expected_gbs, ret_expected_step
+                    #print_one_rank(f'[{self.__class__.__name__}] estimate_time_and_utility - solved_iidp_config_map: {solved_iidp_config_map}', 'debug')
+                    if remaining_num_dataset - global_batch_size*iteration < 0:
+                        expected_step = (remaining_num_dataset // global_batch_size) + 1
+                    else:
+                        expected_step = iteration
+                    if expected_step <= 0:
+                        return min_duration, ret_utility, ret_solved_iidp_config_map, ret_expected_gbs, ret_expected_step
+                    duration = iter_time * expected_step
+                    utility = self.utility_func(local_batch_size, solved_iidp_config_map)
+                    return duration, utility, solved_iidp_config_map, global_batch_size, expected_step
+                else:
+                    return min_duration, ret_utility, ret_solved_iidp_config_map, ret_expected_gbs, ret_expected_step
+            else:
+                if global_batch_size > self.current_global_batch_size: # Increasing global batch size
+                    total_num_workers = round(global_batch_size/local_batch_size)
+                    total_num_workers += (total_num_workers % 2) # SimiGrad constraint
+                    if local_batch_size*total_num_workers < self.current_global_batch_size:
+                        continue
+                else:
+                    total_num_workers = round(global_batch_size/local_batch_size)
+                    total_num_workers -= (total_num_workers % 2) # SimiGrad constraint
+                    if local_batch_size*total_num_workers > self.current_global_batch_size:
+                        continue
+                if total_num_workers >= configurator.total_num_gpus and iteration > 0:
+                    _, iter_time, solved_iidp_config_map = configurator.solve_dynamic_programming(total_num_workers)
+                    if solved_iidp_config_map == {}:
+                        continue
+                    expected_gbs = total_num_workers * local_batch_size
+                    if remaining_num_dataset - expected_gbs*iteration < 0:
+                        expected_step = (remaining_num_dataset // expected_gbs) + 1
+                    else:
+                        expected_step = iteration
+                    if expected_step <= 0:
+                        continue
+                    duration = iter_time * expected_step
+                    utility = self.utility_func(local_batch_size, solved_iidp_config_map)
+                    if duration < min_duration:
+                        min_duration = duration
+                        ret_utility = utility
+                        ret_solved_iidp_config_map = solved_iidp_config_map
+                        ret_expected_gbs = expected_gbs
+                        ret_expected_step = expected_step
+                        best_local_batch_size = local_batch_size
+        #print_one_rank(f'[{self.__class__.__name__}] best local batch size: {best_local_batch_size}', 'debug')
+        return min_duration, ret_utility, ret_solved_iidp_config_map, \
+                ret_expected_gbs, ret_expected_step
+
+    def utility_func(self, local_batch_size, new_config_map):
+        if self.utility_type == 'memory': # TODO: If model is GA-effective, memory util is low
+            gpu_util_in_cluster = []
+            #print_one_rank(f'utility_func() - self.global_server_info: {self.global_server_info}', 'debug')
+            for rank, (num_models, _) in new_config_map.items():
+                server_info = self.configurators[local_batch_size].global_server_info.rank_to_server_map[rank]
+                gpu_type = server_info.resource_info.device_name
+                max_num_models = self.configurators[local_batch_size].all_max_num_local_models_in_process_group[gpu_type]
+                gpu_util_in_cluster.append(num_models/max_num_models)
+            return round(sum(gpu_util_in_cluster) / len(gpu_util_in_cluster), 2)
diff --git a/torch/iidp/config/examples/__init__.py b/torch/iidp/config/examples/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/config/examples/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/config/examples/config_utils.py b/torch/iidp/config/examples/config_utils.py
new file mode 100644
index 00000000000..8c524421775
--- /dev/null
+++ b/torch/iidp/config/examples/config_utils.py
@@ -0,0 +1,29 @@
+MODEL_TITLE_MAP = {
+    'resnet18_cifar10': 'ResNet-18 on CIFAR10',
+    'resnet50': 'ResNet-50 on ImageNet',
+    'rcnn': 'Faster-R-CNN on MS-COCO',
+    'gnmt': 'GNMT on WMT16',
+    'bert': 'BERT-base on SQuAD v1.1',
+    'gpt2': 'GPT-2 on WikiText-2',
+    'vit': 'ViT on ImageNet'
+}
+
+REGISTERED_MODELS_FOR_DATASET = ['resnet50', 'vit', 'rcnn', 'gnmt', 'resnet18_cifar10']
+
+DATASET_NAME_MAP = {
+    'resnet50': 'imagenet',
+    'vit': 'imagenet',
+    'rcnn': 'coco',
+    'gnmt': 'wmt16',
+    'resnet18_cifar10': 'cifar10'
+}
+
+
+def NUM_DATASET(model_name):
+    dataset_num = {
+        'imagenet': 1281167,
+        'coco': 117266,
+        'wmt16': 3498161,
+        'cifar10': 50000
+    }
+    return dataset_num[DATASET_NAME_MAP[model_name]]
diff --git a/torch/iidp/config/model/__init__.py b/torch/iidp/config/model/__init__.py
new file mode 100644
index 00000000000..9342e4c0bb9
--- /dev/null
+++ b/torch/iidp/config/model/__init__.py
@@ -0,0 +1,4 @@
+from . import comp
+from . import comm
+from . import throughput
+from . import global_batch_size
\ No newline at end of file
diff --git a/torch/iidp/config/model/comm/README.md b/torch/iidp/config/model/comm/README.md
new file mode 100644
index 00000000000..0ac9a1ada53
--- /dev/null
+++ b/torch/iidp/config/model/comm/README.md
@@ -0,0 +1,13 @@
+# All-reduce (communication) Regression Model
+
+## Setup
+```
+pip install -r requirements.txt
+```
+
+## How to use
+### Pre-requisites
+- Generate all-reduce profile data samples (refer to README.md in examples/experiments/profiler/comm)
+```
+python allreduce_model_test.py --comm-profile-dir {profile dir path}
+```
diff --git a/torch/iidp/config/model/comm/__init__.py b/torch/iidp/config/model/comm/__init__.py
new file mode 100644
index 00000000000..e3ff77f0df5
--- /dev/null
+++ b/torch/iidp/config/model/comm/__init__.py
@@ -0,0 +1 @@
+from . import allreduce_model
\ No newline at end of file
diff --git a/torch/iidp/config/model/comm/allreduce_model.py b/torch/iidp/config/model/comm/allreduce_model.py
new file mode 100644
index 00000000000..e331422a984
--- /dev/null
+++ b/torch/iidp/config/model/comm/allreduce_model.py
@@ -0,0 +1,66 @@
+import os
+
+import numpy as np
+from sklearn.linear_model import LinearRegression
+
+import matplotlib.pyplot as plt
+
+
+class AllreduceModel(object):
+    def __init__(self):
+        self.a = 0
+        self.b = 0
+
+    def train(self, x_data, y_data):
+        #print(f'[INFO] x_data: {x_data}')
+        #print(f'[INFO] y_data: {y_data}')
+
+        x = np.array(x_data).reshape((-1, 1))
+        y = np.array(y_data)
+
+        # train model
+        model = LinearRegression()
+        model.fit(x, y)
+
+        self.a = model.coef_[0]
+        self.b = model.intercept_
+        #print('============= Trained Regression Model =============')
+        #print(f"slope (a): {self.a}")
+        #print(f"intercept (b): {self.b}")
+        #print('====================================================')
+
+    def evaluate(self, x):
+        """param x:float - theoretical all-reduce time w.r.t bucket size"""
+        return self.a*x + self.b
+
+    def plot(self, x_data, y_data, file_path='allreduce_model.png', title='All-reduce model'):
+        plt.clf()
+        plt.scatter(x_data, y_data, c='tab:red', marker='o', label='Data')
+        y_pred = []
+        for x in x_data:
+            y_pred.append(self.evaluate(x))
+        plt.plot(x_data, y_pred, c='tab:blue', ls='-', label='Model')
+        plt.xlabel('Theoretical all-reduce time (ms)')
+        plt.ylabel('Real all-reduce time (ms)')
+        plt.legend()
+        plt.title(title)
+        plt.savefig(file_path)
+
+    def save(self, train_result_file_path='train_result/model_params.txt'):
+        os.makedirs(os.path.dirname(train_result_file_path), exist_ok=True)
+        with open(train_result_file_path, 'w') as f:
+            f.write(f'{self.a},{self.b}')
+
+    def load(self, param_file_path):
+        try:
+            with open(param_file_path, 'r') as f:
+                line = f.readline()
+                self.a = float(line.split(',')[0])
+                self.b = float(line.split(',')[1])
+        except Exception as e:
+            print(e)
+            print(f'[ERROR] file path: {param_file_path}')
+        print(f'Load Model parameters => a: {self.a} | b: {self.b}')
+
+    def __repr__(self):
+        return f'a: {self.a} | b: {self.b}'
diff --git a/torch/iidp/config/model/comm/allreduce_model_test.py b/torch/iidp/config/model/comm/allreduce_model_test.py
new file mode 100644
index 00000000000..3aa46525c19
--- /dev/null
+++ b/torch/iidp/config/model/comm/allreduce_model_test.py
@@ -0,0 +1,60 @@
+import argparse
+import os
+
+from torch.iidp.config.model.comm.allreduce_model import AllreduceModel
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(description='Allreduce Regression Model Test code')
+    parser.add_argument('--comm-profile-dir', type=str, default=None, required=True,
+                        help='(intra, inter) allreduce profile data directory')
+
+    args = parser.parse_args()
+
+    intra_allreduce_model, inter_allreduce_model = AllreduceModel(), AllreduceModel()
+    for comm_profile_file in os.listdir(args.comm_profile_dir):
+        comm_profile_file_path = os.path.join(args.comm_profile_dir, comm_profile_file)
+        print(f'[INFO] profile data file path: {comm_profile_file_path}')
+        x_data, y_data = [], []
+        with open(comm_profile_file_path, 'r') as f:
+            for line in f.readlines():
+                x_data.append(float(line.split(',')[0]))
+                y_data.append(float(line.split(',')[1]))
+        if 'intra' in comm_profile_file:
+            intra_allreduce_model.train(x_data, y_data)
+            intra_allreduce_model.plot(x_data, y_data, 'intra_allreduce_model.png', 'Intra-node All-reduce Model')
+        elif 'inter' in comm_profile_file:
+            inter_allreduce_model.train(x_data, y_data)
+            inter_allreduce_model.plot(x_data, y_data, 'inter_allreduce_model.png', 'Inter-node All-reduce Model')
+        else:
+            raise ValueError(
+                f'allreduce profile filename must inclue inter or intra term: {comm_profile_file}')
+
+    bucket_cap = 126 * 1024 * 1024
+    total_num_gpus = 4
+    bandwidth = 15750000000
+    ideal_allreduce_time = (bucket_cap * 4 * (total_num_gpus-1)/total_num_gpus) / bandwidth * 1000
+    predicted_allreduce_time = inter_allreduce_model.evaluate(ideal_allreduce_time)
+    print('Theoretical intra-node all-reduce time (ms):', ideal_allreduce_time)
+    print('Predicted intra-node all-reduce time (ms):', predicted_allreduce_time)
+
+    bucket_cap = 126 * 1024 * 1024
+    total_num_gpus = 8
+    bandwidth = 7000000000
+    ideal_allreduce_time = (bucket_cap * 4 * (total_num_gpus-1)/total_num_gpus) / bandwidth * 1000
+    predicted_allreduce_time = inter_allreduce_model.evaluate(ideal_allreduce_time)
+    print('Theoretical inter-node all-reduce time (ms):', ideal_allreduce_time)
+    print('Predicted inter-node all-reduce time (ms):', predicted_allreduce_time)
+
+    print('... test save() ...')
+    intra_allreduce_model.save('train_result/intra_model_params.txt')
+    inter_allreduce_model.save('train_result/inter_model_params.txt')
+    print('... test save() done! ...')
+
+    print('... test load() ...')
+    intra_allreduce_model.a = 0
+    intra_allreduce_model.b = 0
+    print('Initializing model:', intra_allreduce_model)
+    intra_allreduce_model.load('train_result/intra_model_params.txt')
+    print('Loaded model:', intra_allreduce_model)
+    print('... test load() done! ...')
diff --git a/torch/iidp/config/model/comm/requirements.txt b/torch/iidp/config/model/comm/requirements.txt
new file mode 100644
index 00000000000..ff88936c77d
--- /dev/null
+++ b/torch/iidp/config/model/comm/requirements.txt
@@ -0,0 +1 @@
+scikit-learn
\ No newline at end of file
diff --git a/torch/iidp/config/model/comp/__init__.py b/torch/iidp/config/model/comp/__init__.py
new file mode 100644
index 00000000000..feeceec68f5
--- /dev/null
+++ b/torch/iidp/config/model/comp/__init__.py
@@ -0,0 +1 @@
+from . import comp_model
\ No newline at end of file
diff --git a/torch/iidp/config/model/comp/comp_model.py b/torch/iidp/config/model/comp/comp_model.py
new file mode 100644
index 00000000000..90ede4163d8
--- /dev/null
+++ b/torch/iidp/config/model/comp/comp_model.py
@@ -0,0 +1,70 @@
+import numpy as np
+import matplotlib.pyplot as plt
+
+
+class StreamParallelThroughputModel(object):
+    def __init__(self):
+        self.a = 0
+        self.b = 0
+        self.x_data = []
+        self.y_data = []
+
+    def train(self, x_data, y_data):
+        """
+        Arguments:
+            x_data: List of number of VSWs
+            y_data: List of throughput normalized by 1 VSW
+        """
+        #print(f'[INFO] x_data: {x_data}')
+        #print(f'[INFO] y_data: {y_data}')
+        self.x_data = x_data
+        self.y_data = y_data
+        x_data = np.array(x_data)
+        y_data = np.array(y_data)
+        xlog_data = np.log(x_data)
+        self.a, self.b = np.polyfit(xlog_data, y_data, 1)
+
+        #print('============= Trained Logarithmic Model =============')
+        #print(f"slope (a): {self.a}")
+        #print(f"intercept (b): {self.b}")
+        #print('====================================================')
+
+    def evaluate(self, x):
+        """
+        Arguments:
+            x: Number of VSWs
+        Return:
+            Normalized throughput
+        """
+        y = self.a * np.log(x) + self.b
+        return y
+
+    def plot(self, x_data, y_data, file_path='comp_model.png', title='Stream parallel throughput model'):
+        plt.clf()
+        plt.scatter(x_data, y_data, c='tab:red', marker='o', label='Data')
+        x_new, y_pred = [], []
+        for x in range(1, x_data[-1]+1):
+            x_new.append(x)
+            y_pred.append(self.evaluate(x))
+        plt.plot(x_new, y_pred, c='tab:blue', ls='-', label='Model')
+        plt.xlabel('Number of VSWs')
+        plt.ylabel('Normalized throughput')
+        plt.legend()
+        plt.title(title)
+        plt.savefig(file_path)
+
+    def __repr__(self):
+        pass
+
+
+class TrueThroughputModel(object):
+    def __init__(self):
+        self.true_thp_model = []
+
+    def train(self, x_data, y_data):
+        #print(f'[INFO] x_data: {x_data}')
+        #print(f'[INFO] y_data: {y_data}')
+        self.true_thp_model = y_data
+
+    def evaluate(self, x):
+        return self.true_thp_model[x-1]
\ No newline at end of file
diff --git a/torch/iidp/config/model/comp/comp_model_test.py b/torch/iidp/config/model/comp/comp_model_test.py
new file mode 100644
index 00000000000..ad5a6b3cd36
--- /dev/null
+++ b/torch/iidp/config/model/comp/comp_model_test.py
@@ -0,0 +1,54 @@
+import argparse
+import os
+import json
+import socket
+
+from torch.iidp.config.model.comp.comp_model import StreamParallelThroughputModel
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(description='Computation Model Test code')
+    parser.add_argument('--comp-profile-dir', type=str, default=None, required=True,
+                        help='computation profile data directory')
+
+    args = parser.parse_args()
+
+    stream_parallel_model = StreamParallelThroughputModel()
+
+    x_data, y_data = [], []
+    init_thp = 0
+    # NOTE: file must be sorted by number of models in increasing order
+    for comp_profile_file in sorted(os.listdir(args.comp_profile_dir)):
+        comp_profile_file_path = os.path.join(args.comp_profile_dir, comp_profile_file)
+        print(f'[INFO] profile data file path: {comp_profile_file_path}')
+        try:
+            with open(comp_profile_file_path, 'r') as jf:
+                json_data = json.load(jf)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+        lbs = json_data['lbs']
+        num_models = json_data['num_models']
+        fwd_time = json_data['fwd_time']
+        bwd_time = json_data['bwd_time']
+        fwd_bwd_time = (fwd_time + bwd_time) / 1000 # convert to ms
+        if num_models == 1:
+            init_thp = round((lbs * num_models) / fwd_bwd_time, 2)
+            print(f'[INFO] initial throughput = {init_thp}')
+        thp = (lbs * num_models) / fwd_bwd_time
+        norm_thp = round(thp / init_thp, 3)
+        x_data.append(num_models)
+        y_data.append(norm_thp)
+    print(x_data, y_data)
+    stream_parallel_model.train(x_data, y_data)
+    model_name = json_data['model']
+    gpu_type = json_data['gpu_type']
+    stream_parallel_model.plot(x_data, y_data, f"{socket.gethostname()}_{model_name}_{lbs}.png", f"{model_name} ({lbs}) on {gpu_type}")
+
+    example_num_models = 3
+    predicted_norm_thp = stream_parallel_model.evaluate(example_num_models)
+    print(f'Predicted normalized throughput = {round(predicted_norm_thp, 2)}')
+    print(f'Predicted throughput (inputs/sec) = {round(predicted_norm_thp * init_thp, 2)}')
+    predicted_fwd_bwd_time = (lbs * example_num_models) / (predicted_norm_thp * init_thp)
+    print(f'Predicted forward + backward time (sec) on number of VSWs: {example_num_models} = {round(predicted_fwd_bwd_time, 4)}')
+
diff --git a/torch/iidp/config/model/global_batch_size/__init__.py b/torch/iidp/config/model/global_batch_size/__init__.py
new file mode 100644
index 00000000000..b48118cc3b2
--- /dev/null
+++ b/torch/iidp/config/model/global_batch_size/__init__.py
@@ -0,0 +1,3 @@
+from . import ensemble_method
+from . import gaussian_process
+from . import exponential_smoothing
\ No newline at end of file
diff --git a/torch/iidp/config/model/global_batch_size/ensemble_method.py b/torch/iidp/config/model/global_batch_size/ensemble_method.py
new file mode 100644
index 00000000000..e0f0837b922
--- /dev/null
+++ b/torch/iidp/config/model/global_batch_size/ensemble_method.py
@@ -0,0 +1,41 @@
+import os
+import numpy as np
+
+
+class EnsembleMethod(object):
+    def __init__(self, models=[], rates=[]):
+        self.models = models
+        self.rates = rates
+        if len(self.models) == 0 or len(self.rates) == 0:
+            raise ValueError(
+                f'len(self.models) and len(self.rates) must > 0, '
+                f'but {len(self.models)} and {len(self.rates)}')
+        if len(self.models) != len(self.rates):
+            raise ValueError(
+                f'len(self.models) must be equal to len(self.rates), '
+                f'but {len(self.models)} and {len(self.rates)}')
+        if sum(rates) != 1:
+            raise ValueError(
+                f'sum value of rates must be 1, but {sum(rates)}'
+            )
+
+    def train(self, x_train_list, y_train_list):
+        for model in self.models:
+            model.train(x_train_list, y_train_list)
+
+    def evaluate(self, x_pred_list):
+        ensemble_y_pred = None
+        for i, (model, rate) in enumerate(zip(self.models, self.rates)):
+            y_pred = model.evaluate(x_pred_list)
+            if i == 0:
+                ensemble_y_pred = np.zeros_like(y_pred)
+            ensemble_y_pred += (y_pred * rate)
+        return ensemble_y_pred
+
+    def save(self, checkpoint_dir):
+        for model in self.models:
+            model.save(checkpoint_dir)
+
+    def load(self, checkpoint_dir):
+        for model in self.models:
+            model.load(checkpoint_dir)
\ No newline at end of file
diff --git a/torch/iidp/config/model/global_batch_size/exponential_smoothing.py b/torch/iidp/config/model/global_batch_size/exponential_smoothing.py
new file mode 100644
index 00000000000..dc779cef97d
--- /dev/null
+++ b/torch/iidp/config/model/global_batch_size/exponential_smoothing.py
@@ -0,0 +1,45 @@
+import os
+import numpy as np
+import pickle
+
+from statsmodels.tsa.holtwinters import ExponentialSmoothing as ETS
+from statsmodels.iolib.smpickle import load_pickle
+
+
+class ExponentialSmoothing(object):
+    def __init__(self):
+        self.ets_fit = None
+        self.total_y = []
+
+    def train(self, x_train_list, y_train_list):
+        self.total_y.extend(y_train_list)
+        y_train = np.array(self.total_y)
+        self.ets_fit = ETS(y_train, seasonal=None, seasonal_periods=None).fit()
+
+    def evaluate(self, x_pred_list):
+        assert self.ets_fit is not None
+        x_pred = (np.array(x_pred_list) / 100).astype(int)
+        start_x = int(x_pred[0])
+        end_x = int(x_pred[-1])
+        last_index = len(self.ets_fit.fittedvalues) - 1
+        start_x = start_x - last_index - 1
+        end_x = end_x - last_index
+        y_pred_mean = self.ets_fit.forecast(steps=end_x)[start_x:]
+        return y_pred_mean
+
+    def save(self, checkpoint_dir):
+        ckpt_file_path = os.path.join(checkpoint_dir, 'exp_smoothing_model.pkl')
+        total_y_ckpt_file_path = os.path.join(checkpoint_dir, 'exp_smoothing_total_y')
+        if self.ets_fit:
+            self.ets_fit.save(ckpt_file_path)
+        if self.total_y:
+            with open(total_y_ckpt_file_path, 'wb') as f:
+                pickle.dump(self.total_y, f)
+
+    def load(self, checkpoint_dir):
+        ckpt_file_path = os.path.join(checkpoint_dir, 'exp_smoothing_model.pkl')
+        total_y_ckpt_file_path = os.path.join(checkpoint_dir, 'exp_smoothing_total_y')
+        if os.path.isfile(ckpt_file_path):
+            self.ets_fit = load_pickle(ckpt_file_path)
+            with open(total_y_ckpt_file_path, 'rb') as f:
+                self.total_y = pickle.load(f)
\ No newline at end of file
diff --git a/torch/iidp/config/model/global_batch_size/gaussian_process.py b/torch/iidp/config/model/global_batch_size/gaussian_process.py
new file mode 100644
index 00000000000..73afb281d8f
--- /dev/null
+++ b/torch/iidp/config/model/global_batch_size/gaussian_process.py
@@ -0,0 +1,47 @@
+import os
+import numpy as np
+import pickle
+
+from sklearn.gaussian_process import GaussianProcessRegressor
+from sklearn.gaussian_process.kernels import *
+from sklearn.utils._testing import ignore_warnings
+from sklearn.exceptions import ConvergenceWarning
+
+
+class GaussianProcessRegressionModel(object):
+    def __init__(self):
+        self.kernel = ConstantKernel(constant_value=1,constant_value_bounds =(1,1e6)) * \
+                        ExpSineSquared(1.0, 3, periodicity_bounds=(1e-2, 10))
+        self.gpr = GaussianProcessRegressor(self.kernel, n_restarts_optimizer=9, normalize_y=True)
+
+    @ignore_warnings(category=ConvergenceWarning)
+    def train(self, x_train_list, y_train_list):
+        if x_train_list is None or y_train_list is None:
+            raise ValueError(f"Argument is None - x_train_list: {x_train_list} | y_train_list: {y_train_list}")
+        else:
+            if not isinstance(x_train_list, (list, tuple)):
+                raise ValueError(
+                    f"Argument x_train_list must be list or tuple type: {type(x_train_list)}")
+            if not isinstance(y_train_list, (list, tuple)):
+                raise ValueError(
+                    f"Argument y_train_list must be list or tuple type: {type(y_train_list)}")
+        if len(x_train_list) != len(y_train_list):
+            raise ValueError('Argument x_train_list and y_train_list must have equal length, '
+                             f'but {len(x_train_list)} and {len(y_train_list)}')
+
+        x_train = np.array(x_train_list).reshape(-1, 1)
+        y_train = np.array(y_train_list)
+        self.gpr.fit(x_train, y_train)
+
+    def evaluate(self, x_pred_list):
+        x_pred = np.array(x_pred_list).reshape(-1, 1)
+        y_pred_mean, y_pred_std = self.gpr.predict(x_pred, return_std=True)
+        return y_pred_mean
+
+    def save(self, checkpoint_dir):
+        ckpt_file_path = os.path.join(checkpoint_dir, 'gaussian_process_model.sav')
+        pickle.dump(self.gpr, open(ckpt_file_path, 'wb'))
+
+    def load(self, checkpoint_dir):
+        ckpt_file_path = os.path.join(checkpoint_dir, 'gaussian_process_model.sav')
+        self.gpr = pickle.load(open(ckpt_file_path, 'rb'))
\ No newline at end of file
diff --git a/torch/iidp/config/model/throughput/__init__.py b/torch/iidp/config/model/throughput/__init__.py
new file mode 100644
index 00000000000..0cf70e5b4da
--- /dev/null
+++ b/torch/iidp/config/model/throughput/__init__.py
@@ -0,0 +1 @@
+from . import throughput_model
\ No newline at end of file
diff --git a/torch/iidp/config/model/throughput/throughput_model.py b/torch/iidp/config/model/throughput/throughput_model.py
new file mode 100644
index 00000000000..4d76d9f7eeb
--- /dev/null
+++ b/torch/iidp/config/model/throughput/throughput_model.py
@@ -0,0 +1,353 @@
+import os
+import json
+import socket
+
+import matplotlib.pyplot as plt
+
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.utils.distributed import print_one_rank
+from torch.iidp.config.model.comm.allreduce_model import AllreduceModel
+from torch.iidp.config.model.comp.comp_model import StreamParallelThroughputModel, TrueThroughputModel
+from torch.iidp.cluster.resource import ResourceInfo, GlobalResourceInfo
+from torch.iidp.config.examples.config_utils import MODEL_TITLE_MAP
+
+
+class ThroughputModel(object):
+    def __init__(self, comp_profile_dir, comm_profile_dir, bucket_profile_dir,
+                 plot_dir=None, is_real_comp=False, verbose=False):
+        self.comp_profile_dir = comp_profile_dir
+        self.comm_profile_dir = comm_profile_dir
+        self.bucket_profile_dir = bucket_profile_dir
+        self.plot_dir = plot_dir
+        self.verbose = verbose
+        if self.plot_dir:
+            print(f'[INFO][{self.__class__.__name__}] Make plot directory: {self.plot_dir}')
+            os.makedirs(self.plot_dir, exist_ok=False)
+
+        self.all_comp_data = []
+
+        self._get_all_comp_profile_data()
+        self.get_constant()
+        if self.plot_dir:
+            self.plot_comp_profile_data()
+            self.plot_bucket_size_distribution()
+        self._build_allreduce_model()
+        if is_real_comp:
+            self._build_true_comp_model()
+        else:
+            self._build_comp_model()
+
+    def _get_all_comp_profile_data(self):
+        for comp_profile_file in os.listdir(self.comp_profile_dir):
+            comp_profile_file_path = os.path.join(self.comp_profile_dir, comp_profile_file)
+            #print_one_rank(f'Computation profile data file path: {comp_profile_file_path}', 'debug')
+            json_data = read_json(comp_profile_file_path)
+            self.all_comp_data.append(json_data)
+        # Sort data by number of models in increasing order
+        self.all_comp_data.sort(key= lambda x:x['num_models'])
+
+    def get_constant(self):
+        json_data = self.all_comp_data[-1] # Data with max number of models
+        self.max_num_models = json_data['num_models']
+        self.lbs = json_data['lbs']
+        self.gpu_type = json_data['gpu_type']
+        self.model = json_data['model']
+        try:
+            self.model_name = MODEL_TITLE_MAP[self.model]
+        except:
+            print_one_rank(f'[WARNING][{self.__class__.get_constant.__qualname__}] Model name is not registerd: {self.model}')
+            self.model_name = self.model
+        self.update_time = json_data['update_time'] / 1000 # ms -> s
+        if self.max_num_models > 1:
+            self.copy_time_per_model = (json_data['copy_time'] / (self.max_num_models-1)) / 1000 # ms -> s
+        else:
+            self.copy_time_per_model = 0
+        self.fwd_ratio = json_data['fwd_time'] / (json_data['fwd_time'] + json_data['bwd_time'])
+        self.bwd_ratio = 1 - self.fwd_ratio
+
+        bucket_profile_file_name = sorted(os.listdir(self.bucket_profile_dir))[-1]
+        comp_profile_file_path = os.path.join(self.bucket_profile_dir, bucket_profile_file_name)
+        json_data = read_json(comp_profile_file_path)
+        self.bucket_size_distribution = json_data['bucket_size_distribution']
+
+    def plot_comp_profile_data(self, file_path='comp_profile_data_breakdown.png'):
+        x_data = []
+        fwd_time = []
+        bwd_time = []
+        update_time = []
+        copy_time = []
+        for data in self.all_comp_data:
+            x_data.append(str(data['num_models']))
+            fwd_time.append(data['fwd_time']/data['total_time'])
+            bwd_time.append(data['bwd_time']/data['total_time'])
+            update_time.append(data['update_time']/data['total_time'])
+            copy_time.append(data['copy_time']/data['total_time'])
+        breakdown_data = [fwd_time, bwd_time, update_time, copy_time]
+        plt.clf()
+        stacked_data = [0 for _ in range(len(x_data))]
+        labels = ['Forward', 'Backward', 'Update', 'Copy']
+        for i, data in enumerate(breakdown_data):
+            plt.bar(x_data, data, bottom=stacked_data, label=labels[i])
+            stacked_data = [prev + data for prev, data in zip(stacked_data, data)]
+        plt.xlabel('Number of VSWs')
+        plt.ylabel('Normalized throughput breakdown')
+        plt.legend()
+        title = f'{self.model_name} ({self.lbs}) on {self.gpu_type}'
+        plt.title(title)
+        file_path = f'{socket.gethostname()}_{self.model}_{self.lbs}_{file_path}'
+        file_path = os.path.join(self.plot_dir, file_path)
+        plt.savefig(file_path)
+
+    def plot_bucket_size_distribution(self, file_path='bucket_size_distribution.png'):
+        plt.clf()
+        x = list(range(len(self.bucket_size_distribution)))
+        plt.bar(x, self.bucket_size_distribution)
+
+        plt.xlabel('Bucket order (backward)')
+        plt.ylabel('Bucket size (MB)')
+
+        title = f'{self.model_name}'
+        plt.title(title)
+
+        file_path = f'{self.model}_{file_path}'
+        file_path = os.path.join(self.plot_dir, file_path)
+        plt.savefig(file_path)
+
+    def _build_allreduce_model(self):
+        self.intra_allreduce_model, self.inter_allreduce_model = AllreduceModel(), AllreduceModel()
+        for comm_profile_file in os.listdir(self.comm_profile_dir):
+            comm_profile_file_path = os.path.join(self.comm_profile_dir, comm_profile_file)
+            #print_one_rank(f'All-reduce profile data file path: {comm_profile_file_path}')
+            x_data, y_data = [], []
+            with open(comm_profile_file_path, 'r') as f:
+                for line in f.readlines():
+                    x_data.append(float(line.split(',')[0]))
+                    y_data.append(float(line.split(',')[1]))
+            if 'intra' in comm_profile_file:
+                self.intra_allreduce_model.train(x_data, y_data)
+                if self.plot_dir:
+                    plot_file_path = os.path.join(self.plot_dir, 'intra_allreduce_model.png')
+                    self.intra_allreduce_model.plot(
+                            x_data, y_data, plot_file_path, 'Intra-node All-reduce Model')
+            elif 'inter' in comm_profile_file:
+                self.inter_allreduce_model.train(x_data, y_data)
+                if self.plot_dir:
+                    plot_file_path = os.path.join(self.plot_dir, 'inter_allreduce_model.png')
+                    self.inter_allreduce_model.plot(
+                            x_data, y_data, plot_file_path, 'Inter-node All-reduce Model')
+            else:
+                raise ValueError(
+                    f'allreduce profile filename must inclue inter or intra term: {comm_profile_file}')
+
+    def _build_comp_model(self):
+        if self.max_num_models < 3:
+            return self._build_true_comp_model()
+        self.stream_parallel_thp_model = StreamParallelThroughputModel()
+        x_data, y_data = [], []
+        self.init_thp = 0
+        for json_data in self.all_comp_data:
+            num_models = json_data['num_models']
+            fwd_time = json_data['fwd_time']
+            bwd_time = json_data['bwd_time']
+            fwd_bwd_time = (fwd_time + bwd_time) / 1000 # convert to ms
+            #print_one_rank(f'comp time on # of VSWs: {num_models} = {fwd_bwd_time}')
+            if num_models == 1:
+                self.init_thp = round((self.lbs * num_models) / fwd_bwd_time, 2)
+                #print_one_rank(f'initial throughput = {self.init_thp}')
+            else:
+                assert self.init_thp != 0, f"[ERROR] self.init_thp must be > 0 if num_models > 1 - file order maybe not sorted"
+            thp = (self.lbs * num_models) / fwd_bwd_time
+            #print_one_rank(f'throughput on # of VSWs: {num_models} = {thp}')
+            norm_thp = round(thp / self.init_thp, 3)
+            x_data.append(num_models)
+            y_data.append(norm_thp)
+        self.stream_parallel_thp_model.train(x_data, y_data)
+        if self.plot_dir:
+            self.stream_parallel_thp_model.plot(
+                    x_data, y_data,
+                    f"{self.plot_dir}/{socket.gethostname()}_{self.model}_{self.lbs}_comp_model.png",
+                    f"{self.model_name} ({self.lbs}) on {self.gpu_type}"
+            )
+
+    def _build_true_comp_model(self):
+        self.stream_parallel_thp_model = TrueThroughputModel()
+        x_data, y_data = [], []
+        self.init_thp = 0
+        for json_data in self.all_comp_data:
+            num_models = json_data['num_models']
+            fwd_time = json_data['fwd_time']
+            bwd_time = json_data['bwd_time']
+            fwd_bwd_time = (fwd_time + bwd_time) / 1000 # convert to ms
+            if num_models == 1:
+                self.init_thp = round((self.lbs * num_models) / fwd_bwd_time, 2)
+                #print_one_rank(f'initial throughput = {self.init_thp}')
+            thp = (self.lbs * num_models) / fwd_bwd_time
+            norm_thp = round(thp / self.init_thp, 3)
+            x_data.append(num_models)
+            y_data.append(norm_thp)
+        self.stream_parallel_thp_model.train(x_data, y_data)
+
+    def plot_evaluate_breakdown(self, comp_data, sync_data, file_path, title):
+        if self.plot_dir is None:
+            return
+        plt.clf()
+        plt.rcParams["figure.figsize"] = [8, 3]
+        plt.rcParams["figure.autolayout"] = True
+        plt.figure()
+        data_type = ['comp', 'sync']
+        comp_labels = {'fwd': 'Forward', 'bwd': 'Backward', 'GA': 'GA'}
+        sync_labels = {
+            'overlap': 'overlap all-reduce',
+            'last_bucket': 'Non-overlap all-reduce',
+            'update': 'Update',
+            'copy': 'Weight copy'
+        }
+
+        stacked_data = [0 for _ in range(len(data_type))] # [0] - comp, [1] - sync
+        for key, val in comp_data.items():
+            plt.barh(data_type, [val, 0], left=stacked_data, label=comp_labels[key], height=0.4)
+            stacked_data[0] += val
+
+        for key, val in sync_data.items():
+            if key == 'overlap':
+                if comp_data['bwd'] > val:
+                    stacked_data[1] = stacked_data[0] - val
+                else:
+                    stacked_data[1] = stacked_data[0] - comp_data['bwd']
+            plt.barh(data_type, [0, val], left=stacked_data, label=sync_labels[key], height=0.4)
+            stacked_data[1] += val
+
+        plt.xlabel('Iteration time (sec)')
+        plt.legend()
+        plt.title(title)
+        file_path = os.path.join(self.plot_dir, file_path)
+        plt.savefig(file_path)
+
+    def evaluate_fwd_bwd_time(self, num_models):
+        predicted_norm_thp = self.stream_parallel_thp_model.evaluate(num_models)
+        predicted_fwd_bwd_time = round((self.lbs * num_models) / (predicted_norm_thp * self.init_thp), 4)
+        return predicted_fwd_bwd_time
+
+    def calculate_ideal_allreduce_time(self, bucket_size_byte, total_num_gpus, bandwidth):
+        allreduce_step = 4*(total_num_gpus-1)
+        network_volume = bucket_size_byte/total_num_gpus * allreduce_step
+        return network_volume / bandwidth * 1000 # sec to ms
+
+    def evaluate(self, num_models, accum_step, weight_sync_method,
+                 resource_info: ResourceInfo, global_resource_info: GlobalResourceInfo):
+        """
+        Args:
+            num_models (int): Number of VSWs
+            accum_step (int): GA steps
+            weight_sync_method (str): 'overlap', 'sequential'
+            resource_info (ResourceInfo): Resource Information of intra-server aspect
+
+        Returns:
+            iter_time (float): predicted iteration time within server level
+            predicted_thp (float): predicted throughput within server level
+        """
+        predicted_thp = 0
+        fwd_time = self.evaluate_fwd_bwd_time(num_models) * self.fwd_ratio
+        bwd_time = self.evaluate_fwd_bwd_time(num_models) * self.bwd_ratio
+        if self.verbose:
+            print_one_rank(f'====== [{self.__class__.evaluate.__qualname__}] ======')
+            print_one_rank(f'Predicted fwd time: {fwd_time:.3f} | bwd time: {bwd_time:.3f}')
+        all_bucket_allreduce_time = []
+        if global_resource_info.total_num_servers > 1:
+            bandwidth = resource_info.inter_network_bandwidth
+            allreduce_model = self.inter_allreduce_model
+        else:
+            bandwidth = resource_info.intra_network_bandwidth
+            allreduce_model = self.intra_allreduce_model
+        for bucket_size in self.bucket_size_distribution:
+            bucket_size_byte = bucket_size * 1024 * 1024
+            ideal_allreduce_time = self.calculate_ideal_allreduce_time(
+                bucket_size_byte, global_resource_info.total_num_gpus, bandwidth)
+            predicted_allreduce_time = allreduce_model.evaluate(ideal_allreduce_time) / 1000 # ms to sec
+            all_bucket_allreduce_time.append(predicted_allreduce_time)
+        if weight_sync_method == 'sequential':
+            iter_time = (fwd_time + bwd_time) * (accum_step-1) + fwd_time + \
+                max(bwd_time, sum(all_bucket_allreduce_time[:-1])) + \
+                all_bucket_allreduce_time[-1] + \
+                self.update_time + (self.copy_time_per_model * (num_models-1))
+
+            gbs = self.lbs * num_models * accum_step * resource_info.num_gpus_in_server
+            predicted_thp = round(gbs / iter_time, 2)
+            if self.verbose:
+                print_one_rank(f'iter time = {iter_time:.4f}')
+                print_one_rank(f'GBS = {gbs}')
+                print_one_rank(f'Predicted throughput of GBS: {gbs} = {predicted_thp:.4f}')
+
+                # In-depth analysis
+                print_one_rank(f'========================== Breakdown ========================== \n' \
+                    f'1) GA + fwd: {(fwd_time + bwd_time) * (accum_step-1) + fwd_time:.4f} \n' \
+                    f'2) overlap bwd + allreduce: {max(bwd_time, sum(all_bucket_allreduce_time[:-1])):.4f} \n' \
+                    f'\t2-1) bwd: {bwd_time} 2-2) allreduce: {sum(all_bucket_allreduce_time[:-1]):.4f} \n' \
+                    f'3) allreduce of last bucket: {all_bucket_allreduce_time[-1]:.4f} \n' \
+                    f'4) update: {self.update_time:.4f} \n'
+                    f'5) copy: {self.copy_time_per_model * (num_models-1):.4f} \n' \
+                    f'===============================================================')
+            comp_data = {
+                'GA': (fwd_time + bwd_time) * (accum_step-1),
+                'fwd': fwd_time,
+                'bwd': bwd_time,
+            }
+            sync_data = {
+                'overlap': sum(all_bucket_allreduce_time[:-1]),
+                'last_bucket': all_bucket_allreduce_time[-1],
+                'update': self.update_time,
+                'copy': self.copy_time_per_model * (num_models-1)
+            }
+            if self.plot_dir:
+                self.plot_evaluate_breakdown(
+                    comp_data, sync_data,
+                    f'{socket.gethostname()}_{self.model}_{self.lbs}_{num_models}_{accum_step}_{weight_sync_method}.png',
+                    f'{self.model_name} ({self.lbs}) VSW: {num_models} GA: {accum_step-1} weight sync: {weight_sync_method} on {self.gpu_type}'
+                )
+
+        elif weight_sync_method == 'overlap':
+            last_bucket_size_ratio = self.bucket_size_distribution[-1] / sum(self.bucket_size_distribution)
+            if self.verbose:
+                print_one_rank(f'[INFO] last_bucket_size_ratio: {last_bucket_size_ratio}')
+            iter_time = (fwd_time + bwd_time) * (accum_step-1) + fwd_time + \
+                max(bwd_time, sum(all_bucket_allreduce_time[:-1])) + \
+                all_bucket_allreduce_time[-1] + \
+                (self.update_time + (self.copy_time_per_model * (num_models-1))) * last_bucket_size_ratio
+
+            gbs = self.lbs * num_models * accum_step * resource_info.num_gpus_in_server
+            predicted_thp = round(gbs / iter_time, 2)
+            if self.verbose:
+                print_one_rank(f'iter time = {iter_time:.4f}')
+                print_one_rank(f'GBS = {gbs}')
+                print_one_rank(f'Predicted throughput of GBS: {gbs} = {predicted_thp:.4f}')
+
+                # In-depth analysis
+                print_one_rank(f'========================== Breakdown ========================== \n' \
+                    f'1) GA + fwd: {(fwd_time + bwd_time) * (accum_step-1) + fwd_time:.4f} \n' \
+                    f'2) overlap bwd + allreduce: {max(bwd_time, sum(all_bucket_allreduce_time[:-1])):.4f} \n' \
+                    f'\t2-1) bwd: {bwd_time} 2-2) allreduce: {sum(all_bucket_allreduce_time[:-1]):.4f} \n' \
+                    f'3) allreduce of last bucket: {all_bucket_allreduce_time[-1]:.4f} \n' \
+                    f'4) update: {self.update_time*last_bucket_size_ratio:.4f} \n'
+                    f'5) copy: {(self.copy_time_per_model * (num_models-1))*last_bucket_size_ratio:.4f} \n' \
+                    f'===============================================================')
+            comp_data = {
+                'GA': (fwd_time + bwd_time) * (accum_step-1),
+                'fwd': fwd_time,
+                'bwd': bwd_time,
+            }
+            sync_data = {
+                'overlap': sum(all_bucket_allreduce_time[:-1]),
+                'last_bucket': all_bucket_allreduce_time[-1],
+                'update': self.update_time*last_bucket_size_ratio,
+                'copy': (self.copy_time_per_model * (num_models-1))*last_bucket_size_ratio
+            }
+            if self.plot_dir:
+                self.plot_evaluate_breakdown(
+                    comp_data, sync_data,
+                    f'{socket.gethostname()}_{self.model}_{self.lbs}_{num_models}_{accum_step}_{weight_sync_method}.png',
+                    f'{self.model_name} ({self.lbs}) VSW: {num_models} GA: {accum_step-1} weight sync: {weight_sync_method} on {self.gpu_type}'
+                )
+        else:
+            raise ValueError(f'Not support such weight sync method: {weight_sync_method}')
+
+        return iter_time, predicted_thp
diff --git a/torch/iidp/data/__init__.py b/torch/iidp/data/__init__.py
new file mode 100644
index 00000000000..c504f6cdc39
--- /dev/null
+++ b/torch/iidp/data/__init__.py
@@ -0,0 +1,4 @@
+from .sampler import ImbalancedSampler
+from .dataloader import DataLoader, AdaptiveDataLoader
+
+__all__ = ['ImbalancedSampler', 'DataLoader', 'AdaptiveDataLoader']
\ No newline at end of file
diff --git a/torch/iidp/data/dataloader.py b/torch/iidp/data/dataloader.py
new file mode 100644
index 00000000000..0151cfafdc8
--- /dev/null
+++ b/torch/iidp/data/dataloader.py
@@ -0,0 +1,251 @@
+import torch
+from torch.utils.data import DataLoader
+import torch.distributed as dist
+
+from torch.iidp.data.sampler import ImbalancedSampler
+from torch.iidp.trainer import GLOBAL_TRAINER_STATE, LOCAL_TRAINER_STATE
+
+
+class DataLoader(DataLoader):
+    def __init__(self, dataset, batch_size=1, batch_fn=None, loading_once=None, shuffle=False, **kwargs):
+        if kwargs.get("batch_sampler") is not None:
+            sampler = getattr(kwargs.get("batch_sampler"), 'sampler', None)
+        elif kwargs.get("sampler") is not None:
+            sampler = kwargs.get("sampler")
+        else:
+            sampler = None
+        if sampler is None or type(sampler) is torch.utils.data.distributed.DistributedSampler:
+            if dist.is_initialized():
+                imblanced_sampler = ImbalancedSampler(
+                    dataset, partition_size=GLOBAL_TRAINER_STATE.partition_size)
+                if kwargs.get("batch_sampler") is not None:
+                    kwargs.get("batch_sampler").sampler = imblanced_sampler
+                if kwargs.get("sampler") is not None:
+                    kwargs["sampler"] = imblanced_sampler
+        if (kwargs.get("num_workers") is not None and kwargs.get("num_workers") > 0) or \
+            (kwargs.get("persistent_workers") is not None and kwargs.get("persistent_workers") is True):
+            persistent_workers = True
+        else:
+            persistent_workers = False
+        super().__init__(dataset, batch_size, shuffle=shuffle, persistent_workers=persistent_workers, **kwargs)
+        self.initial_dataloader_length = super().__len__() # Equal dataloader length among all ranks
+        if batch_fn is None:
+            raise ValueError(f'Argument "batch_fn" must be configured by user, but: {batch_fn}')
+        if loading_once is None:
+            raise ValueError(f'Argument "loading_once" must be configured by user, but: {batch_fn}')
+        self.batch_fn = batch_fn
+        self.loading_once = loading_once
+        self.global_batch_size = GLOBAL_TRAINER_STATE.global_batch_size
+        self.total_local_num_models = LOCAL_TRAINER_STATE.num_models
+        self.accum_step = LOCAL_TRAINER_STATE.accum_step
+        self.data_index = -1
+        self.done = False
+
+    def __iter__(self):
+        self.data_index = 0
+        self.done = False
+        print(f'[INFO][torch/iidp/data/dataloader.py] Initial loading.. it might take time..')
+        if self.loading_once is True:
+            for idx, batch in enumerate(super().__iter__()):
+                chunked_batch = self.batch_fn(batch, self.total_local_num_models, self.loading_once)
+                yield chunked_batch
+            self.done = True
+        else:
+            # NOTE: Since self._index_sampler.batch_size is changed to local batch size,
+            # len(super().__iter__()) is also modified.
+            self._index_sampler.batch_size = LOCAL_TRAINER_STATE.local_batch_size
+            local_batch_data = []
+            padding_samples = []
+            iter_len = len(self.batch_sampler)
+            for idx, batch in enumerate(super().__iter__()):
+                last_batch = (idx == iter_len-1)
+                if idx < self.total_local_num_models:
+                    padding_samples.append(batch)
+                if len(local_batch_data) < self.total_local_num_models:
+                    local_batch_data.append(batch)
+                    # Handle the case that the last batch is reached, but not satisfied with total_local_num_models
+                    if last_batch and not (len(local_batch_data) == self.total_local_num_models):
+                        padding_num_local_models = self.total_local_num_models - len(local_batch_data)
+                        for i in range(padding_num_local_models):
+                            local_batch_data.append(padding_samples[i])
+                        padding_samples = []
+                if len(local_batch_data) == self.total_local_num_models:
+                    #assert len(local_batch_data) == total_local_num_models
+                    chunked_batch = self.batch_fn(local_batch_data, self.total_local_num_models, self.loading_once)
+                    yield chunked_batch
+                    local_batch_data = []
+                    if idx % self.accum_step == 0:
+                        self.data_index += self.global_batch_size
+                        #print(f'[DEBUG][torch/iidp/data/dataloader.py] data_index/len(dataset): {self.data_index}/{len(self.dataset)}')
+                if self.data_index >= len(self.dataset):
+                    self.done = True
+                    break
+            self.done = True
+
+        if self.done is False:
+            raise RuntimeError(f'[ERROR][torch/iidp/data/dataloader.py] Flag done is not True even iterator is finished')
+
+    def __len__(self):
+        return self.initial_dataloader_length
+
+
+class AdaptiveDataLoader(torch.utils.data.DataLoader):
+    def __init__(self, dataset, batch_size=1, batch_fn=None, size_fn=None, loading_once=None, shuffle=False, **kwargs):
+        if kwargs.get("batch_sampler") is not None:
+            sampler = getattr(kwargs.get("batch_sampler"), 'sampler', None)
+        elif kwargs.get("sampler") is not None:
+            sampler = kwargs.get("sampler")
+        else:
+            sampler = None
+        if sampler is None or type(sampler) is torch.utils.data.distributed.DistributedSampler:
+            if dist.is_initialized():
+                imblanced_sampler = ImbalancedSampler(
+                    dataset, partition_size=GLOBAL_TRAINER_STATE.partition_size)
+                if kwargs.get("batch_sampler") is not None:
+                    kwargs.get("batch_sampler").sampler = imblanced_sampler
+                if kwargs.get("sampler") is not None:
+                    kwargs["sampler"] = imblanced_sampler
+        if (kwargs.get("num_workers") is not None and kwargs.get("num_workers") > 0) or \
+            (kwargs.get("persistent_workers") is not None and kwargs.get("persistent_workers") is True):
+            persistent_workers = True
+        else:
+            persistent_workers = False
+        super().__init__(dataset, batch_size, shuffle=shuffle, persistent_workers=persistent_workers, **kwargs)
+        self.initial_dataloader_length = super().__len__() # Equal dataloader length among all ranks
+        if batch_fn is None:
+            raise ValueError(f'Argument "batch_fn" must be configured by user, but: {batch_fn}')
+        if size_fn is None:
+            raise ValueError(f'Argument "size_fn" must be configured by user, but: {size_fn}')
+        if loading_once is None:
+            raise ValueError(f'Argument "loading_once" must be configured by user, but: {batch_fn}')
+        if loading_once is True: # TODO
+            raise ValueError(f'Not support with Argument "loading_once" = True')
+        self.batch_fn = batch_fn
+        self.size_fn = size_fn
+        self.loading_once = loading_once
+        self.global_batch_size = GLOBAL_TRAINER_STATE.global_batch_size
+        self.total_local_num_models = LOCAL_TRAINER_STATE.num_models
+        self.accum_step = LOCAL_TRAINER_STATE.accum_step
+        self.data_index = -1
+        self.done = False
+        self.epoch = 0
+
+    def __iter__(self):
+        self.data_index = 0
+        self.done = False
+        iter_idx = 0
+        num_yielded = 0
+        if self.loading_once is True:
+            while not self.done:
+                print(f'[INFO][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | Initial loading.. it might take time..')
+                for idx, batch in enumerate(super().__iter__()):
+                    # NOTE: index drawn from super().__iter__() is initialized when it is over
+                    iter_idx += 1
+                    #print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | {batch[0].size()} | {self.total_local_num_models}')
+                    #assert batch[0].size()[0] == LOCAL_TRAINER_STATE.local_batch_size * self.total_local_num_models, \
+                    #    f'[DEBUG][torch/iidp/data/dataloader.py] __iter__ idx: {idx} | rank: {dist.get_rank()} | {batch[0].size()} | {self.total_local_num_models}'
+                    chunked_batch = self.batch_fn(batch, self.total_local_num_models, self.loading_once)
+                    if chunked_batch == []:
+                        print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | continue!')
+                        continue
+                    # NOTE: after yielding, self.global_batch_size and self.accum_step might be changed
+                    global_batch_size_progress = self.global_batch_size
+                    accum_progress = self.accum_step
+                    yield chunked_batch
+                    if iter_idx % accum_progress == 0:
+                        self.data_index += global_batch_size_progress
+                        #print(f'[DEBUG][torch/iidp/data/dataloader.py] data_index/len(dataset): {self.data_index}/{len(self.dataset)}')
+                    if self.data_index >= len(self.dataset):
+                        self.done = True
+                        break
+        else:
+            # NOTE: Since self._index_sampler.batch_size is changed to local batch size,
+            # len(super().__iter__()) is also modified.
+            self._index_sampler.batch_size = LOCAL_TRAINER_STATE.local_batch_size
+            local_batch_data = []
+            while not self.done:
+                print(f'[INFO][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | Initial loading.. it might take time..')
+                for idx, batch in enumerate(super().__iter__()):
+                    """
+                    print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | '
+                          f'idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | '
+                          f'self.size_fn(batch): {self.size_fn(batch)} | self.batch_sampler.batch_size: {self.batch_sampler.batch_size}')
+                    """
+                    if self.size_fn(batch) != self.batch_sampler.batch_size:
+                        """
+                        print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | '
+                              f'idx: {idx} | num_yielded: {num_yielded} | self.size_fn(batch): {self.size_fn(batch)} | '
+                              f'self.batch_sampler.batch_size: {self.batch_sampler.batch_size} | '
+                              f'skip!!')
+                        """
+                        continue
+                    # NOTE: index drawn from super().__iter__() is initialized when it is over
+                    iter_idx += 1
+                    if len(local_batch_data) < self.total_local_num_models:
+                        """
+                        print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | '
+                              f'idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | '
+                              f'self.total_local_num_models: {self.total_local_num_models} | '
+                              f'len(local_batch_data): {len(local_batch_data)} | '
+                              f'len(local_batch_data) < self.total_local_num_models condition enters!!!!!')
+                        """
+                        local_batch_data.append(batch)
+                    """
+                    print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | before if len(local_batch_data) == self.total_local_num_models!')
+                    print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | self.total_local_num_models: {self.total_local_num_models}')
+                    print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | local_batch_data[0][0].size()[0]: {local_batch_data[0][0].size()[0]}')
+                    """
+                    if len(local_batch_data) == self.total_local_num_models:
+                        # NOTE: after yielding, self.global_batch_size and self.accum_step might be changed
+                        global_batch_size_progress = self.global_batch_size
+                        accum_progress = self.accum_step
+                        chunked_batch = self.batch_fn(local_batch_data, self.total_local_num_models, self.loading_once)
+                        yield chunked_batch
+                        num_yielded += 1
+                        #print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | after yielding chunked_batch!')
+                        local_batch_data = []
+                        #print(f'[DEBUG][torch/iidp/data/dataloader.py] rank: {dist.get_rank()} | idx: {idx} | iter_idx: {iter_idx} | num_yielded: {num_yielded} | accum_progress: {accum_progress}')
+                        if num_yielded % accum_progress == 0:
+                            self.data_index += global_batch_size_progress
+                            num_yielded = 0
+                            #print(f'[DEBUG][torch/iidp/data/dataloader.py] ==================> data_index/len(dataset): {self.data_index}/{len(self.dataset)}')
+                    if self.data_index >= len(self.dataset):
+                        self.done = True
+                        break
+
+        if self.done is False:
+            raise RuntimeError(f'[ERROR][torch/iidp/data/dataloader.py] Flag done is not True even iterator is finished')
+        self.epoch += 1
+
+    def __len__(self):
+        return self.initial_dataloader_length
+
+    def get_progress(self):
+        return len(self.dataset) * self.epoch + self.data_index
+
+    def state_dict(self):
+        return {
+            'epoch': self.epoch,
+        }
+
+    def load_state_dict(self, state_dict):
+        self.epoch = state_dict['epoch']
+        if hasattr(self._index_sampler.sampler, 'epoch'):
+            self._index_sampler.sampler.epoch = self.epoch
+        #print(f'[DEBUG][AdaptiveDataLoader] load state dict: {state_dict}')
+
+    def update_local_state(self, batch_size, total_local_num_models, accum_step):
+        print(f'[DEBUG][torch/iidp/data/dataloader.py] update_local_state() - '
+              f'batch_size: {batch_size} | '
+              f'total_local_num_models: {total_local_num_models} | '
+              f'accum_step: {accum_step}')
+        if self.loading_once is True:
+            self.batch_sampler.batch_size = batch_size
+        else:
+            self.batch_sampler.batch_size = batch_size // total_local_num_models
+            self.curr_sampler_iter_len = len(self.batch_sampler)
+        self.total_local_num_models = total_local_num_models
+        self.accum_step = accum_step
+
+    def update_global_state(self, global_batch_size, partition_size):
+        self.global_batch_size = global_batch_size
\ No newline at end of file
diff --git a/torch/iidp/data/sampler.py b/torch/iidp/data/sampler.py
new file mode 100644
index 00000000000..78e13da5dbf
--- /dev/null
+++ b/torch/iidp/data/sampler.py
@@ -0,0 +1,99 @@
+import math
+from typing import TypeVar, Optional, Iterator, List
+
+import torch
+from torch.utils.data import Sampler, Dataset
+import torch.distributed as dist
+
+
+T_co = TypeVar('T_co', covariant=True)
+
+
+class ImbalancedSampler(Sampler[T_co]):
+    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,
+                 rank: Optional[int] = None, shuffle: bool = True,
+                 seed: int = 0, drop_last: bool = False, partition_size: List[float] = None) -> None:
+        if num_replicas is None:
+            if not dist.is_available():
+                raise RuntimeError("Requires distributed package to be available")
+            num_replicas = dist.get_world_size()
+        if rank is None:
+            if not dist.is_available():
+                raise RuntimeError("Requires distributed package to be available")
+            rank = dist.get_rank()
+        if rank >= num_replicas or rank < 0:
+            raise ValueError(
+                "Invalid rank {}, rank should be in the interval"
+                " [0, {}]".format(rank, num_replicas - 1))
+        if drop_last:
+            raise ValueError("Argument drop_last is not supported currently")
+        self.dataset = dataset
+        self.num_replicas = num_replicas
+        self.rank = rank
+        self.epoch = 0
+        self.drop_last = drop_last
+        self.partition_size = partition_size
+        self.all_num_samples_in_process_group = [
+            math.ceil(len(self.dataset) * partition_size) \
+            for partition_size in self.partition_size
+        ]
+        print('[INFO][torch/iidp/data/sampler.py] == ImbalancedSampler() == ')
+        print(f'[INFO][torch/iidp/data/sampler.py] rank: {rank}')
+        print(f'[INFO][torch/iidp/data/sampler.py] num_replicas: {num_replicas}')
+        print(f'[INFO][torch/iidp/data/sampler.py] dataset length: {len(self.dataset)}')
+        print(f'[INFO][torch/iidp/data/sampler.py] partition_size: {self.partition_size}')
+        #self.num_samples = math.ceil(len(self.dataset) * self.partition_size)
+        self.num_samples = self.all_num_samples_in_process_group[rank]
+        self.total_size = len(self.dataset)
+        print(f'[INFO][torch/iidp/data/sampler.py] self.num_samples: {self.num_samples}')
+        print(f'[INFO][torch/iidp/data/sampler.py] self.total_size: {self.total_size}')
+        self.shuffle = shuffle
+        self.seed = seed
+
+    def __iter__(self) -> Iterator[T_co]:
+        if self.shuffle:
+            # deterministically shuffle based on epoch and seed
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch)
+            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore
+        else:
+            indices = list(range(len(self.dataset)))  # type: ignore
+        #print('[DEBUG][torch/iidp/data/sampler.py] == ImbalancedSampler().__iter__ == ')
+        #print(f'[DEBUG][torch/iidp/data/sampler.py] len(indices): {len(indices)}')
+        # subsample
+        start_index = sum(self.all_num_samples_in_process_group[:self.rank])
+        end_index = start_index + self.num_samples
+        #print(f'[DEBUG][torch/iidp/data/sampler.py] subsampling index at rank: {self.rank} ==> {start_index} : {end_index}')
+        #indices = indices[self.rank*self.num_samples:(self.rank + 1)*self.num_samples]
+        indices = indices[start_index:end_index]
+        #print('[DEBUG][torch/iidp/data/sampler.py] == after sub-sampling ==')
+        #print(f'[DEBUG][torch/iidp/data/sampler.py] len(indices): {len(indices)}')
+        #print(f'[DEBUG][torch/iidp/data/sampler.py] self.num_samples: {self.num_samples}')
+        if len(indices) != self.num_samples:
+            # add extra samples to make it evenly divisible
+            padding_size = self.num_samples - len(indices)
+            #print(f'[DEBUG][torch/iidp/data/sampler.py] padding_size: {padding_size}')
+            if padding_size <= len(indices):
+                indices += indices[:padding_size]
+            else:
+                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]
+
+        assert len(indices) == self.num_samples
+
+        return iter(indices)
+
+    def __len__(self) -> int:
+        # print('[DEBUG][torch/iidp/data/sampler.py] == ImbalancedSampler() == ')
+        # print(f'[DEBUG][torch/iidp/data/sampler.py] self.num_samples: {self.num_samples}')
+        return self.num_samples
+
+    def set_epoch(self, epoch: int) -> None:
+        r"""
+        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
+        use a different random ordering for each epoch. Otherwise, the next iteration of this
+        sampler will yield the same ordering.
+
+        Args:
+            epoch (int): Epoch number.
+        """
+        self.epoch = epoch
\ No newline at end of file
diff --git a/torch/iidp/ddp_comm_hooks/__init__.py b/torch/iidp/ddp_comm_hooks/__init__.py
new file mode 100644
index 00000000000..17972df6a97
--- /dev/null
+++ b/torch/iidp/ddp_comm_hooks/__init__.py
@@ -0,0 +1,3 @@
+from .default_hooks import *
+from .iidp_allreduce import *
+from .simigrad_allreduce import *
\ No newline at end of file
diff --git a/torch/iidp/ddp_comm_hooks/default_hooks.py b/torch/iidp/ddp_comm_hooks/default_hooks.py
new file mode 100644
index 00000000000..7f9e6f94531
--- /dev/null
+++ b/torch/iidp/ddp_comm_hooks/default_hooks.py
@@ -0,0 +1,21 @@
+import torch
+import torch.distributed as dist
+
+
+def _allreduce_sum_fut(
+    process_group: dist.ProcessGroup, tensor: torch.Tensor
+) -> torch.futures.Future:
+    group_to_use = process_group if process_group is not None else dist.group.WORLD
+    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()
+
+    def get_value(fut):
+        # print(f'[DEBUG] _allreduce_sum_fut: {fut.value()[0]}')
+        return [fut.value()[0]]
+
+    return fut.then(get_value)
+
+
+def allreduce_sum_hook(
+    process_group: dist.ProcessGroup, bucket: dist._GradBucket
+) -> torch.futures.Future:
+    return _allreduce_sum_fut(process_group, bucket.get_tensors()[0])
\ No newline at end of file
diff --git a/torch/iidp/ddp_comm_hooks/iidp_allreduce.py b/torch/iidp/ddp_comm_hooks/iidp_allreduce.py
new file mode 100644
index 00000000000..e42055e8e1b
--- /dev/null
+++ b/torch/iidp/ddp_comm_hooks/iidp_allreduce.py
@@ -0,0 +1,30 @@
+import torch
+import torch.distributed as dist
+
+
+class IIDPState:
+    def __init__(self, process_group, total_num_models):
+        self.process_group = process_group
+        self.total_num_models = total_num_models
+
+
+def iidp_allreduce_hook(
+    state: IIDPState, bucket: dist._GradBucket
+) -> torch.futures.Future:
+    group_to_use = state.process_group if state.process_group is not None else dist.group.WORLD
+    tensor = bucket.get_tensors()[0]
+    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()
+
+    def get_average_value(fut):
+        #print(f'[DEBUG] get_average_value - {state.total_num_models}')
+        return [fut.value()[0].div_(state.total_num_models)]
+
+    return fut.then(get_average_value)
+
+
+def dummy_hook(state, bucket):
+    fut = torch.futures.Future()
+    #if dist.get_rank():
+    #    print(f'[DEBUG] [torch/iidp/ddp_comm_hooks/__init__.py] dummy_hook!')
+    fut.set_result(bucket.get_tensors())
+    return fut
\ No newline at end of file
diff --git a/torch/iidp/ddp_comm_hooks/simigrad_allreduce.py b/torch/iidp/ddp_comm_hooks/simigrad_allreduce.py
new file mode 100644
index 00000000000..613f201aa33
--- /dev/null
+++ b/torch/iidp/ddp_comm_hooks/simigrad_allreduce.py
@@ -0,0 +1,54 @@
+import os
+
+import torch
+import torch.distributed as dist
+from torch.iidp.ddp_comm_hooks.default_hooks import _allreduce_sum_fut
+from torch.iidp.ddp_comm_hooks.iidp_allreduce import iidp_allreduce_hook, IIDPState
+
+
+class SimiGradState(IIDPState):
+    def __init__(self, process_group, total_num_models, sub_group, grad_placeholder, interval):
+        super().__init__(process_group, total_num_models)
+        self.sub_group = sub_group
+        self.grad_placeholder = grad_placeholder
+        assert self.total_num_models % 2 == 0, \
+            f"self.total_num_models must be power of 2, but {self.total_num_models}"
+        self.subgroup_total_num_models = self.total_num_models / 2
+        self.interval = interval
+        self.step = 1 # To avoid first step because of rebuilding DDP bucket
+        if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+            self.interval = 0 # determined by remaining_epochs() in torch/iidp/trainer.py
+            self.done_epoch = False
+
+
+def subgroup_allreduce_hook(
+    state: SimiGradState, bucket: dist._GradBucket
+) -> torch.futures.Future:
+    if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+        if state.done_epoch is False:
+            fut = torch.futures.Future()
+            fut.set_result(bucket.get_tensors())
+            return fut
+    elif state.step % state.interval != 0:
+        fut = torch.futures.Future()
+        fut.set_result(bucket.get_tensors())
+        return fut
+    # Detach bucket's tensor not to be affected by all-reduce in all process group (allgroup_allreduce())
+    tensor = bucket.get_tensors()[0].detach().clone()
+    group_to_use = state.sub_group
+    future_work = _allreduce_sum_fut(group_to_use, tensor)
+    def append_to_grad_placeholder(fut):
+        state.grad_placeholder.append(fut.value()[0].div_(state.subgroup_total_num_models))
+        return [fut.value()[0]]
+
+    return future_work.then(append_to_grad_placeholder)
+
+
+def simigrad_allreduce_hook(hook):
+    def hook_with_allreduce(state, bucket):
+        future_work = hook(state, bucket)
+        def allgroup_allreduce(fut):
+            iidp_allreduce_hook(state, bucket).wait()
+            return bucket.get_tensors()
+        return future_work.then(allgroup_allreduce)
+    return hook_with_allreduce
diff --git a/torch/iidp/elastic/Makefile b/torch/iidp/elastic/Makefile
new file mode 100644
index 00000000000..c4292a7baa5
--- /dev/null
+++ b/torch/iidp/elastic/Makefile
@@ -0,0 +1,8 @@
+rpc_stubs:
+	python -m grpc_tools.protoc -Iruntime/protobuf --python_out=runtime/rpc_stubs --grpc_python_out=runtime/rpc_stubs runtime/protobuf/trainer_to_scheduler.proto
+	python -m grpc_tools.protoc -Iruntime/protobuf --python_out=runtime/rpc_stubs --grpc_python_out=runtime/rpc_stubs runtime/protobuf/scheduler_to_worker.proto
+	python -m grpc_tools.protoc -Iruntime/protobuf --python_out=runtime/rpc_stubs --grpc_python_out=runtime/rpc_stubs runtime/protobuf/worker_to_scheduler.proto
+	python -m grpc_tools.protoc -Iruntime/protobuf --python_out=runtime/rpc_stubs --grpc_python_out=runtime/rpc_stubs runtime/protobuf/common.proto
+
+clean:
+	rm -rf runtime/rpc_stubs/*_pb2.py runtime/rpc_stubs/*_pb2_grpc.py
diff --git a/torch/iidp/elastic/__init__.py b/torch/iidp/elastic/__init__.py
new file mode 100644
index 00000000000..999109d7766
--- /dev/null
+++ b/torch/iidp/elastic/__init__.py
@@ -0,0 +1,4 @@
+from . import scheduler
+from . import worker
+from . import _utils
+from . import runtime
diff --git a/torch/iidp/elastic/_utils.py b/torch/iidp/elastic/_utils.py
new file mode 100644
index 00000000000..130977bc3e5
--- /dev/null
+++ b/torch/iidp/elastic/_utils.py
@@ -0,0 +1,23 @@
+import psutil
+import subprocess
+
+
+def get_num_gpus():
+    command = 'nvidia-smi -L'
+    output = subprocess.run(command, stdout=subprocess.PIPE, check=True,
+                            shell=True).stdout.decode('utf-8').strip()
+    return len(output.split('\n'))
+
+
+def kill_pid_for_job(command):
+    cmd_name = command.split(' ')[1]
+    cmd_rank = command.split(' ')[-6]
+    for proc in psutil.process_iter(['pid', 'cmdline', 'status']):
+        proc_cmdline = proc.info['cmdline']
+        if cmd_name in proc_cmdline and cmd_rank in proc_cmdline and "/bin/sh" not in proc_cmdline:
+            try:
+                proc.terminate()
+            except psutil.NoSuchProcess:
+                print(f"Process with name {proc} not found.")
+            except psutil.AccessDenied:
+                print(f"Access denied to terminate process {proc} with PID: {proc.info['pid']}")
\ No newline at end of file
diff --git a/torch/iidp/elastic/example_init.json b/torch/iidp/elastic/example_init.json
new file mode 100644
index 00000000000..f6163bee1a6
--- /dev/null
+++ b/torch/iidp/elastic/example_init.json
@@ -0,0 +1,3 @@
+{
+  "0": 2
+}
diff --git a/torch/iidp/elastic/launch.py b/torch/iidp/elastic/launch.py
new file mode 100644
index 00000000000..a4b67dac61b
--- /dev/null
+++ b/torch/iidp/elastic/launch.py
@@ -0,0 +1,87 @@
+import argparse
+import os
+import subprocess
+import socket
+
+from torch.iidp.elastic.worker import Worker
+from torch.iidp.utils.json_utils import read_json
+
+
+def check_nfs(dir_path):
+    proc = subprocess.Popen(f'stat --file-system --format=%T {dir_path}', stdout=subprocess.PIPE, shell=True)
+    (out, err) = proc.communicate()
+    filesystem_type = out.decode('utf-8').replace('\n','')
+    if filesystem_type != 'nfs':
+        raise ValueError(
+            f'[torch/iidp/elastic/launch.py] checkpoint path: {dir_path} must be stored NFS, but : {filesystem_type}')
+
+
+def check_cluster_topology(config_params):
+    available_server_list = config_params['available_servers']
+    for rank, server_name in enumerate(available_server_list):
+        if socket.gethostname() == server_name:
+            if opt_dict['rank'] != rank:
+                raise AssertionError(
+                    "--rank must be equal to the order of available servers, but "
+                    f"--rank: {opt_dict['rank']} of server: {socket.gethostname()}"
+                    f" != rank: {rank} in available_servers: {available_server_list}")
+
+
+if __name__=='__main__':
+    parser = argparse.ArgumentParser(description='Elastic IIDP Launcher')
+    parser.add_argument('-r', '--rank', type=int, required=True,
+                        help='Unique Rank among Hetero Clusters')
+    parser.add_argument('-i', '--ip_addr', type=str, required=True,
+                        help='IP address for scheduler server')
+    parser.add_argument('--elastic_checkpoint_dir', type=str, required=True,
+                        help='Directory where checkpoints is stored')
+    parser.add_argument('--adaptive-config-file', type=str, required=True,
+                        help='Adaptive training configuration file path (json)')
+    parser.add_argument('--cmd', type=str, required=True,
+                        help='Job cmd to run')
+    parser.add_argument('--accum-step', type=int, required=True,
+                        help='Gradient accumulation step')
+    parser.add_argument('--num-models', type=int, required=True,
+                        help='Number of VSWs')
+    parser.add_argument('--local-batch-size', type=int, required=True,
+                        help='Local batch size')
+    parser.add_argument('--dist-url', type=str, required=True,
+                        help='URL used to set up distributed training')
+
+    parser.add_argument('--log_dir', type=str, default=None,
+                        help='Directory where log is stored')
+    parser.add_argument('-s', '--sched_port', type=int, default=50060,
+                        help='Port number for scheduler server')
+    parser.add_argument('-w', '--worker_port', type=int, default=50061,
+                        help='Port number for worker server')
+
+    args = parser.parse_args()
+    opt_dict = vars(args)
+
+    # [Assumption] Checkpoint and log path must be stored on NFS
+    ckpt_dir = opt_dict['elastic_checkpoint_dir']
+    os.makedirs(ckpt_dir, exist_ok=True)
+    #check_nfs(ckpt_dir)
+
+    log_dir = opt_dict['log_dir']
+    if log_dir is not None:
+        os.makedirs(log_dir, exist_ok=True)
+        #check_nfs(log_dir)
+
+    config_params = read_json(opt_dict['adaptive_config_file'])
+    gpu_cluster_info_file_path = config_params['gpu_cluster_info']
+
+    check_cluster_topology(config_params)
+
+    worker = Worker(opt_dict['rank'],
+                    opt_dict['ip_addr'],
+                    opt_dict['sched_port'],
+                    opt_dict['worker_port'],
+                    opt_dict['cmd'],
+                    opt_dict['dist_url'],
+                    (opt_dict['num_models'], opt_dict['accum_step']),
+                    opt_dict['local_batch_size'],
+                    ckpt_dir,
+                    gpu_cluster_info_file_path,
+                    log_dir)
+    worker.join()
diff --git a/torch/iidp/elastic/requirements.txt b/torch/iidp/elastic/requirements.txt
new file mode 100644
index 00000000000..79e4b9f1dca
--- /dev/null
+++ b/torch/iidp/elastic/requirements.txt
@@ -0,0 +1,16 @@
+annoy==1.17.3
+cvxpy==1.1.24
+dill==0.3.4
+filelock==3.4.1
+func_timeout==4.3.5
+glog==0.3.1
+grpcio==1.48.2
+grpcio-tools==1.48.2
+pandas==1.1.5
+preconditions==0.1
+psutil==5.9.5
+seaborn==0.11.2
+setproctitle==1.2.3
+sklearn==0.0
+scikit-learn==0.24.2
+statsmodels==0.12.2
diff --git a/torch/iidp/elastic/run_scheduler.py b/torch/iidp/elastic/run_scheduler.py
new file mode 100644
index 00000000000..84092408d47
--- /dev/null
+++ b/torch/iidp/elastic/run_scheduler.py
@@ -0,0 +1,30 @@
+import os, sys
+
+import argparse
+import time
+
+import scheduler
+
+SLEEP_TIME = 10
+
+
+def main(args):
+    # Instantiate scheduler.
+    sched = scheduler.Scheduler(args.port, args.config_file)
+
+    try:
+        while True:
+            time.sleep(SLEEP_TIME)
+    except KeyboardInterrupt:
+        pass
+    finally:
+        sched.shut_down()
+
+
+if __name__=='__main__':
+    parser = argparse.ArgumentParser(description='Elastic IIDP Scheduler Runner')
+    parser.add_argument('-p', '--port', type=int, default=40000,
+                        help="Scheduler's Port Number")
+    parser.add_argument('-c', '--config_file', type=str, required=True,
+                        help="Initial configuration JSON file path")
+    main(parser.parse_args())
\ No newline at end of file
diff --git a/torch/iidp/elastic/runtime/__init__.py b/torch/iidp/elastic/runtime/__init__.py
new file mode 100644
index 00000000000..0bf6a1ecf7d
--- /dev/null
+++ b/torch/iidp/elastic/runtime/__init__.py
@@ -0,0 +1,6 @@
+from .rpc import worker_client
+from .rpc import worker_server
+from .rpc import scheduler_client
+from .rpc import scheduler_server
+
+from .rpc_stubs import *
\ No newline at end of file
diff --git a/torch/iidp/elastic/runtime/protobuf/common.proto b/torch/iidp/elastic/runtime/protobuf/common.proto
new file mode 100644
index 00000000000..c48fcbd6730
--- /dev/null
+++ b/torch/iidp/elastic/runtime/protobuf/common.proto
@@ -0,0 +1,3 @@
+syntax = "proto3";
+
+message Empty {}
diff --git a/torch/iidp/elastic/runtime/protobuf/scheduler_to_worker.proto b/torch/iidp/elastic/runtime/protobuf/scheduler_to_worker.proto
new file mode 100644
index 00000000000..8860cf85aad
--- /dev/null
+++ b/torch/iidp/elastic/runtime/protobuf/scheduler_to_worker.proto
@@ -0,0 +1,20 @@
+syntax = "proto3";
+
+import "common.proto";
+
+service SchedulerToWorker {
+    // Start list of jobs on worker.
+    rpc RunJob (RunJobRequest) returns (Empty) {}
+    // Resets the worker.
+    rpc Reset (Empty) returns (Empty) {}
+    // Shuts down the worker.
+    rpc Shutdown (Empty) returns (Empty) {}
+}
+
+message RunJobRequest {
+    repeated uint64 trainer_ids = 1;
+    uint32 world_size = 2;
+    string master_addr = 3;
+    map<uint32, string> config = 4;
+    uint32 local_batch_size = 5;
+}
diff --git a/torch/iidp/elastic/runtime/protobuf/trainer_to_scheduler.proto b/torch/iidp/elastic/runtime/protobuf/trainer_to_scheduler.proto
new file mode 100644
index 00000000000..f004f1928a0
--- /dev/null
+++ b/torch/iidp/elastic/runtime/protobuf/trainer_to_scheduler.proto
@@ -0,0 +1,22 @@
+syntax = "proto3";
+
+import "common.proto";
+
+service TrainerToScheduler {
+    // Initializes the job.
+    rpc InitJob (InitJobRequest) returns (Empty);
+    // Update Configuraton
+    rpc UpdateConfig (UpdateConfigRequest) returns (Empty);
+
+    rpc ShutDown (Empty) returns (Empty) {}
+}
+
+message InitJobRequest {
+    uint64 trainer_id = 1;
+}
+
+message UpdateConfigRequest {
+    map<uint32, string> config = 1;
+    uint32 local_batch_size = 2;
+}
+
diff --git a/torch/iidp/elastic/runtime/protobuf/worker_to_scheduler.proto b/torch/iidp/elastic/runtime/protobuf/worker_to_scheduler.proto
new file mode 100644
index 00000000000..9e83d375881
--- /dev/null
+++ b/torch/iidp/elastic/runtime/protobuf/worker_to_scheduler.proto
@@ -0,0 +1,36 @@
+syntax = "proto3";
+
+import "common.proto";
+
+service WorkerToScheduler {
+    // Registers the worker with the scheduler
+    rpc RegisterWorker (RegisterWorkerRequest) returns (RegisterWorkerResponse);
+
+    // Indicates to the scheduler that a job has completed
+    rpc Done (DoneRequest) returns (Empty);
+
+    // Indicates to the scheduler that worker was killed
+    rpc Killed (KilledRequest) returns (Empty);
+}
+
+message RegisterWorkerRequest {
+    string device_id = 1;
+    uint64 worker_id = 2;
+    uint32 num_gpus = 3;
+    string ip_addr = 4;
+    uint32 port = 5;
+}
+
+message RegisterWorkerResponse {
+    bool success = 1;
+    uint64 worker_id = 2;
+    string error_message = 3;
+}
+
+message DoneRequest {
+    uint64 worker_id = 1;
+}
+
+message KilledRequest {
+    uint64 worker_id = 1;
+}
\ No newline at end of file
diff --git a/torch/iidp/elastic/runtime/rpc/__init__.py b/torch/iidp/elastic/runtime/rpc/__init__.py
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/torch/iidp/elastic/runtime/rpc/dispatcher.py b/torch/iidp/elastic/runtime/rpc/dispatcher.py
new file mode 100644
index 00000000000..540da447cf3
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/dispatcher.py
@@ -0,0 +1,242 @@
+import copy
+import json
+from multiprocessing.pool import ThreadPool
+import math
+import logging
+import os
+import queue
+import re
+from shutil import which
+import signal
+import subprocess
+import sys
+import threading
+import datetime
+import traceback
+
+sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
+
+import _utils
+
+MAX_CPUS_PER_GPU = 8
+LOG_FORMAT = '{name}:{levelname} [{asctime}] {message}'
+ITERATOR_LOG_FORMAT = '[{asctime}] [{event}] [{status}] {message}'
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
+
+
+class Dispatcher:
+    def __init__(self, worker_rpc_client, sched_addr, sched_port,
+                 cmd, initial_url, initial_config, initial_local_batch_size,
+                 checkpoint_dir, worker_id, worker_info, gpu_cluster_info, log_dir=None):
+        logger = logging.getLogger(__name__)
+        logger.setLevel(logging.DEBUG)
+        ch = logging.StreamHandler()
+        ch.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT,
+                                          style='{'))
+        logger.addHandler(ch)
+        self._logger = logger
+        self._thread_pool = ThreadPool()
+        self._worker_rpc_client = worker_rpc_client
+        self._sched_addr = sched_addr
+        self._sched_port = sched_port
+        self._cmd = cmd
+        self._checkpoint_dir = checkpoint_dir
+        self._resetted = False
+        self._killed = False
+        # Job management
+        self._job_assignments = dict()
+        self._commands = dict()
+        # Lock
+        self._lock = threading.Lock()
+        # Worker data
+        self._worker_id = worker_id
+        self.initial_master_url = initial_url
+        self.master_port = initial_url.split(':')[-1]
+        self.worker_info = worker_info
+        self.gpu_cluster_info = gpu_cluster_info
+        self.num_gpus_in_server = worker_info['number']
+        self._gpu_ids = list(range(self.num_gpus_in_server))
+        self.initial_iidp_config = initial_config # (num_models, accum_step)
+        self.initial_local_batch_size = initial_local_batch_size
+
+        self._log_dir = log_dir
+
+    def _construct_command(self, gpu_id, trainer_id, world_size, master_addr="", config="", local_batch_size=0):
+        master_url = f'tcp://{master_addr}:{self.master_port}' if master_addr else self.initial_master_url
+        num_models, accum_step = map(int, config.split(",")) if config else self.initial_iidp_config
+        local_batch_size = local_batch_size if local_batch_size > 0 else self.initial_local_batch_size
+        checkpoint_dir = os.path.join(self._checkpoint_dir)
+
+        command = '%s --gpu %d' % (self._cmd, gpu_id)
+        command = '%s --elastic-checkpoint-dir %s' % (command, checkpoint_dir)
+        command = '%s --rank %d' % (command, trainer_id)
+        command = '%s --world-size %d' % (command, world_size)
+        command = '%s --is-elastic-training' % (command)
+        command = '%s --dist-url %s' % (command, master_url)
+        command = '%s --num-models %d' % (command, num_models)
+        command = '%s --accum-step %d' % (command, accum_step)
+        command = '%s --local-batch-size %d' % (command, local_batch_size)
+
+        self._logger.debug("command: {}".format(command))
+
+        return command
+
+    def _kill_jobs(self, job_id=None):
+        with self._lock:
+            if job_id is not None:
+                self._logger.debug('Killing job {0}...'.format(job_id))
+            else:
+                self._logger.debug('Killing all jobs!')
+            if job_id is not None:
+                if job_id in self._job_assignments:
+                    del self._job_assignments[job_id]
+                if job_id not in self._commands:
+                    return
+                command = self._commands[job_id]
+                del self._commands[job_id]
+                _utils.kill_pid_for_job(command)
+            else:
+                for job_id in self._job_assignments.keys():
+                    if job_id not in self._commands:
+                        continue
+                    command = self._commands[job_id]
+                    _utils.kill_pid_for_job(command)
+            self._logger.debug('Finished killing job(s)')
+
+    def launch_job(self, job_id, local_rank, command):
+        self._logger.debug('Job {} launched...'.format(job_id))
+
+        with self._lock:
+            self._commands[job_id] = command
+
+        # Try to dispatch job.
+        job_succeeded = False
+        try:
+            env = copy.deepcopy(os.environ)
+            env['IIDP_JOB_ID'] = str(job_id)
+            env['IIDP_LOCAL_RANK'] = str(local_rank)
+            env['IIDP_WORKER_ID'] = str(self._worker_id)
+            env['IIDP_SCHED_ADDR'] = self._sched_addr
+            env['IIDP_SCHED_PORT'] = str(self._sched_port)
+
+            proc = subprocess.Popen(command,
+                        stdout=subprocess.PIPE,
+                        stderr=subprocess.STDOUT,
+                        env=env,
+                        shell=True)
+            if job_id == 0 and self._log_dir is not None:
+                log_file = os.path.join(self._log_dir, 'convergence_log.txt')
+                with open(log_file, 'a') as f:
+                    for line in proc.stdout:
+                        print(line.decode('utf-8').replace('\n',''))
+                        f.write(line.decode('utf-8'))
+            else:
+                for line in proc.stdout:
+                    print(line.decode('utf-8').replace('\n',''))
+            job_succeeded = True
+        except subprocess.CalledProcessError as e:
+            error_message = ('Job {job_id} (worker {worker_id} failed!').format(
+                                     job_id=job_id, worker_id=self._worker_id)
+            self._logger.error(error_message)
+            traceback.print_exc()
+            if e.stdout is not None:
+                self._logger.debug('Job {job_id} (worker {worker_id}, '
+                                   'stdout:\n{output}'.format(
+                                       job_id=job_id, worker_id=self._worker_id,
+                                       output=e.stdout))
+            if e.stderr is not None:
+                self._logger.debug('Job {job_id} (worker {worker_id}, '
+                                   'stderr:\n{output}'.format(
+                                       job_id=job_id, worker_id=self._worker_id,
+                                       output=e.stderr))
+            self._kill_jobs(job_id=job_id)
+        except Exception as e:
+            self._logger.error('Dispatcher failed to launch job {job_id} '
+                               '(worker {worker_id})!'.format(
+                                   job_id=job_id, worker_id=self._worker_id))
+            traceback.print_exc()
+            self._kill_jobs(job_id=job_id)
+
+        with self._lock:
+            if not self._resetted:
+                if job_id in self._commands:
+                    del self._commands[job_id]
+                if job_id in self._job_assignments:
+                    del self._job_assignments[job_id]
+
+        if not self._resetted:
+            self._logger.debug('Job {} finished launch...'.format(job_id))
+        return 0
+
+    def _dispatch_jobs_helper(self, trainer_ids, world_size, master_addr, config, local_batch_size):
+        if len(trainer_ids) == 0 or len(trainer_ids) > self.num_gpus_in_server:
+            return
+        with self._lock:
+            for i, job_id in enumerate(trainer_ids):
+                self._job_assignments[job_id] = (self._gpu_ids[i], i)
+                self._logger.debug('Job {} dispatched...'.format(job_id))
+
+        success = True
+        commands = []
+        for job_id in trainer_ids:
+            try:
+                gpu_id, _ = self._job_assignments[job_id]
+                if config:
+                    trainer_config = config[job_id]
+                else:
+                    trainer_config = ""
+                command = \
+                    self._construct_command(
+                        gpu_id, job_id, world_size, master_addr,
+                        trainer_config, local_batch_size)
+                commands.append(command)
+            except Exception as e:
+                self._logger.error('Failed to construct command '
+                                   'for job {0}!'.format(job_id))
+                traceback.print_exc()
+                success = False
+                break
+
+        sync_results = []
+        if success:
+            # Launch the jobs.
+            for job_id, command in zip(trainer_ids, commands):
+                _, local_rank = self._job_assignments[job_id]
+
+                sync_results.append(self._thread_pool.apply_async(
+                        self.launch_job,
+                        (job_id, local_rank, command)))
+
+        for sync_ in sync_results:
+            sync_.get()
+        # Cleanup and notify the scheduler.
+        if not self._resetted:
+            self._worker_rpc_client.notify_scheduler(self._worker_id)
+        else:
+            self._resetted = False
+        return
+
+    def dispatch_jobs(self, trainer_ids, world_size, master_addr, config, local_batch_size):
+        self._thread_pool.apply_async(self._dispatch_jobs_helper,
+                                      (trainer_ids, world_size, master_addr, config, local_batch_size ))
+
+    def reset(self):
+        self._resetted = True
+        self._logger.debug('Resetting dispatcher...')
+        self._kill_jobs()
+        self._job_assignments.clear()
+        self._commands.clear()
+        self._thread_pool = ThreadPool()
+        self._logger.debug('Finished resetting dispatcher')
+
+    def shutdown(self, killed=False):
+        if self._killed:
+            return
+        self._logger.debug('Shutting down dispatcher...')
+        self._kill_jobs()
+        self._thread_pool.terminate()
+        self._logger.debug('Finished shutting down dispatcher')
+        if killed:
+            self._worker_rpc_client.notify_scheduler(self._worker_id,
+                                                     killed=killed)
+        self._killed = True
diff --git a/torch/iidp/elastic/runtime/rpc/scheduler_client.py b/torch/iidp/elastic/runtime/rpc/scheduler_client.py
new file mode 100644
index 00000000000..13d55dffd9b
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/scheduler_client.py
@@ -0,0 +1,47 @@
+import grpc
+import os
+import sys
+sys.path.append(os.path.join(os.path.dirname(__file__), '../rpc_stubs'))
+
+import scheduler_to_worker_pb2 as s2w_pb2
+import scheduler_to_worker_pb2_grpc as s2w_pb2_grpc
+import common_pb2
+
+
+class SchedulerRpcClient:
+    """Scheduler client for sending RPC requests to a worker server."""
+
+    def __init__(self, server_ip_addr, port):
+        self._addr = server_ip_addr
+        self._port = port
+        self._server_loc = '%s:%d' % (server_ip_addr, port)
+
+    @property
+    def addr(self):
+        return self._addr
+
+    @property
+    def port(self):
+        return self._port
+
+    def run_job(self, trainer_ids, world_size, master_addr="", config={}, local_batch_size=0):
+        print('[DEBUG][torch/iidp/elastic/runtime/rpc/scheduler_client.py] run_job()')
+        with grpc.insecure_channel(self._server_loc) as channel:
+            stub = s2w_pb2_grpc.SchedulerToWorkerStub(channel)
+            request = s2w_pb2.RunJobRequest(trainer_ids=trainer_ids,
+                                            world_size=world_size,
+                                            master_addr=master_addr,
+                                            config=config,
+                                            local_batch_size=local_batch_size)
+            response = stub.RunJob(request)
+
+    def reset(self):
+        print(f'[DEBUG][torch/iidp/elastic/runtime/rpc/scheduler_client.py] reset()')
+        with grpc.insecure_channel(self._server_loc) as channel:
+            stub = s2w_pb2_grpc.SchedulerToWorkerStub(channel)
+            response = stub.Reset(common_pb2.Empty())
+
+    def shutdown(self):
+        with grpc.insecure_channel(self._server_loc) as channel:
+            stub = s2w_pb2_grpc.SchedulerToWorkerStub(channel)
+            stub.Shutdown(common_pb2.Empty())
diff --git a/torch/iidp/elastic/runtime/rpc/scheduler_server.py b/torch/iidp/elastic/runtime/rpc/scheduler_server.py
new file mode 100644
index 00000000000..edb08e5873c
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/scheduler_server.py
@@ -0,0 +1,146 @@
+from concurrent import futures
+import time
+
+import grpc
+import logging
+import os
+import sys
+import socket
+import traceback
+sys.path.append(os.path.join(os.path.dirname(__file__), '../rpc_stubs'))
+sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
+
+import worker_to_scheduler_pb2 as w2s_pb2
+import worker_to_scheduler_pb2_grpc as w2s_pb2_grpc
+import trainer_to_scheduler_pb2 as t2s_pb2
+import trainer_to_scheduler_pb2_grpc as t2s_pb2_grpc
+import common_pb2
+
+_ONE_DAY_IN_SECONDS = 60 * 60 * 24
+LOG_FORMAT = '{name}:{levelname} [{asctime}] {message}'
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
+
+
+class SchedulerRpcServer(w2s_pb2_grpc.WorkerToSchedulerServicer):
+    def __init__(self, callbacks, logger):
+        self._callbacks = callbacks
+        self._logger = logger
+
+    def RegisterWorker(self, request, context):
+        register_worker_callback = self._callbacks['RegisterWorker']
+        try:
+            succeed, worker_id, err = register_worker_callback(
+                                        device_id=request.device_id,
+                                        worker_id=request.worker_id,
+                                        num_gpus=request.num_gpus,
+                                        ip_addr=request.ip_addr,
+                                        port=request.port)
+            if succeed:
+                self._logger.info(
+                    'Successfully registered GPU: {device_id} in worker: '
+                    '{worker_id}'.format(
+                        device_id=request.device_id,
+                        worker_id=worker_id))
+                return w2s_pb2.RegisterWorkerResponse(success=succeed,
+                                                      worker_id=worker_id)
+            else:
+                self._logger.error('Could not register worker: {0}'.format(err))
+                return w2s_pb2.RegisterWorkerResponse(success=succeed,
+                                                      error_message=err)
+        except Exception as e:
+            self._logger.error('Could not register worker: {0}'.format(e))
+            return w2s_pb2.RegisterWorkerResponse(success=False,
+                                                  error_message=e)
+
+    def Done(self, request, context):
+        done_callback = self._callbacks['Done']
+        try:
+            trainer_ids, succeed = done_callback(request.worker_id)
+            if succeed:
+                self._logger.info(
+                    'Received completion notification: '
+                    'Trainer IDs: {trainer_ids}, Worker ID: {worker_id}'.format(
+                        trainer_ids=str(trainer_ids), worker_id=request.worker_id))
+            else:
+                self._logger.error('Could not process completion '
+                                'notification for worker {}'.format(request.worker_id))
+        except Exception as e:
+            self._logger.error('Could not process completion '
+                               'notification for worker {}'.format(request.worker_id))
+            traceback.print_exc()
+
+        return common_pb2.Empty()
+
+    def Killed(self, request, context):
+        killed_callback = self._callbacks['Killed']
+        try:
+            succeed = killed_callback(request.worker_id)
+            if succeed:
+                self._logger.info(
+                    'Received killed notification: '
+                    'Worker ID: {worker_id}'.format(worker_id=request.worker_id))
+            else:
+                self._logger.error('Could not process killed '
+                                'notification for worker {}'.format(request.worker_id))
+        except Exception as e:
+            self._logger.error('Could not process completion '
+                               'notification for worker {}'.format(request.worker_id))
+            traceback.print_exc()
+
+        return common_pb2.Empty()
+
+
+class SchedulerTrainerRpcServer(t2s_pb2_grpc.TrainerToSchedulerServicer):
+    def __init__(self, callbacks, logger):
+        self._callbacks = callbacks
+        self._logger = logger
+
+    def InitJob(self, request, context):
+        trainer_id = request.trainer_id
+        init_job_callback = self._callbacks['InitJob']
+        err = init_job_callback(trainer_id)
+        if err is not None:
+            self._logger.error(err)
+        else:
+            self._logger.info('Received job initialization request '
+                              'from trainer {0}'.format(trainer_id))
+        return common_pb2.Empty()
+
+    def UpdateConfig(self, request, context):
+        update_job_callback = self._callbacks['UpdateConfig']
+        succeed = update_job_callback(request.config, request.local_batch_size)
+        if succeed:
+            self._logger.info("Updated Configuration...")
+        return common_pb2.Empty()
+
+    def ShutDown(self, request, context):
+        shutdown_job_callback = self._callbacks['ShutDown']
+        err = shutdown_job_callback()
+        if err is not None:
+            self._logger.error(err)
+        else:
+            self._logger.info('Received job shutdown request ')
+        return common_pb2.Empty()
+
+
+def serve(port, callbacks):
+    logger = logging.getLogger(__name__)
+    logger.setLevel(logging.DEBUG)
+    ch = logging.StreamHandler()
+    ch.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT,
+                                      style='{'))
+    logger.addHandler(ch)
+    server = grpc.server(futures.ThreadPoolExecutor())
+    w2s_pb2_grpc.add_WorkerToSchedulerServicer_to_server(
+            SchedulerRpcServer(callbacks, logger), server)
+    t2s_pb2_grpc.add_TrainerToSchedulerServicer_to_server(
+            SchedulerTrainerRpcServer(callbacks, logger), server)
+    ip_address = socket.gethostbyname(socket.gethostname())
+    server.add_insecure_port('%s:%d' % (ip_address, port))
+    logger.info('Starting server at {0}:{1}'.format(ip_address, port))
+    server.start()
+    try:
+        while True:
+            time.sleep(_ONE_DAY_IN_SECONDS)
+    except KeyboardInterrupt:
+        server.stop(0)
diff --git a/torch/iidp/elastic/runtime/rpc/trainer_client.py b/torch/iidp/elastic/runtime/rpc/trainer_client.py
new file mode 100644
index 00000000000..867e92e7dc0
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/trainer_client.py
@@ -0,0 +1,31 @@
+import grpc
+import sys
+
+import trainer_to_scheduler_pb2 as t2s_pb2
+import trainer_to_scheduler_pb2_grpc as t2s_pb2_grpc
+import common_pb2
+
+
+class TrainerRpcClient:
+
+    def __init__(self, trainer_id, worker_id, sched_ip_addr, sched_port):
+        self._trainer_id = trainer_id
+        self._worker_id = worker_id
+        self._sched_loc = '%s:%d' % (sched_ip_addr, sched_port)
+
+    def init(self):
+        request = t2s_pb2.InitJobRequest(trainer_id=self._trainer_id)
+        with grpc.insecure_channel(self._sched_loc) as channel:
+            stub = t2s_pb2_grpc.TrainerToSchedulerStub(channel)
+            stub.InitJob(request)
+
+    def update_config(self, config, local_batch_size):
+        request = t2s_pb2.UpdateConfigRequest(config=config, local_batch_size=local_batch_size)
+        with grpc.insecure_channel(self._sched_loc) as channel:
+            stub = t2s_pb2_grpc.TrainerToSchedulerStub(channel)
+            stub.UpdateConfig(request)
+
+    def shutdown(self):
+        with grpc.insecure_channel(self._sched_loc) as channel:
+            stub = t2s_pb2_grpc.TrainerToSchedulerStub(channel)
+            stub.ShutDown(common_pb2.Empty())
diff --git a/torch/iidp/elastic/runtime/rpc/worker_client.py b/torch/iidp/elastic/runtime/rpc/worker_client.py
new file mode 100644
index 00000000000..9d4983acf19
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/worker_client.py
@@ -0,0 +1,70 @@
+import grpc
+import logging
+import os
+import sys
+import time
+import socket
+sys.path.append(os.path.join(os.path.dirname(__file__), '../rpc_stubs'))
+
+import worker_to_scheduler_pb2 as w2s_pb2
+import worker_to_scheduler_pb2_grpc as w2s_pb2_grpc
+
+LOG_FORMAT = '{name}:{levelname} [{asctime}] {message}'
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
+
+
+class WorkerRpcClient:
+    """Worker client for sending RPC requests to a scheduler server."""
+
+    def __init__(self, device_id, worker_id, worker_ip_addr, worker_port,
+                 sched_ip_addr, sched_port):
+        logger = logging.getLogger(__name__)
+        logger.setLevel(logging.DEBUG)
+        ch = logging.StreamHandler()
+        ch.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT,
+                                          style='{'))
+        logger.addHandler(ch)
+        self._logger = logger
+        self._device_id = device_id
+        self._worker_id = worker_id
+        self._worker_ip_addr = worker_ip_addr
+        self._worker_port = worker_port
+        self._sched_ip_addr = sched_ip_addr
+        self._sched_port = sched_port
+        # TODO: Remove self._sched_ip_addr and self._sched_port?
+        self._sched_loc = '%s:%d' % (sched_ip_addr, sched_port)
+
+    def register_worker(self, num_gpus):
+        request = w2s_pb2.RegisterWorkerRequest(
+            device_id=self._device_id,
+            worker_id=self._worker_id,
+            num_gpus=num_gpus,
+            ip_addr=self._worker_ip_addr,
+            port=self._worker_port)
+        with grpc.insecure_channel(self._sched_loc) as channel:
+            self._logger.debug('Trying to register worker...')
+            stub = w2s_pb2_grpc.WorkerToSchedulerStub(channel)
+            response = stub.RegisterWorker(request)
+            if response.success:
+                self._logger.info(
+                    f'Succesfully registered worker ID (=rank): {self._worker_id} '
+                    f'| server hostname: {socket.gethostname()}'
+                )
+                return None
+            else:
+                assert(response.HasField('error_message'))
+                self._logger.error(f'Failed to register worker | server hostname: {socket.gethostname()}')
+                return response.error_message
+
+    def notify_scheduler(self, worker_id, killed=False):
+        # Send a Done message.
+        if killed:
+            request = w2s_pb2.KilledRequest(worker_id=worker_id)
+        else:
+            request = w2s_pb2.DoneRequest(worker_id=worker_id)
+        with grpc.insecure_channel(self._sched_loc) as channel:
+            stub = w2s_pb2_grpc.WorkerToSchedulerStub(channel)
+            if killed:
+                stub.Killed(request)
+            else:
+                stub.Done(request)
diff --git a/torch/iidp/elastic/runtime/rpc/worker_server.py b/torch/iidp/elastic/runtime/rpc/worker_server.py
new file mode 100644
index 00000000000..ae4df38454d
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc/worker_server.py
@@ -0,0 +1,82 @@
+from concurrent import futures
+import grpc
+import logging
+import os
+import sys
+import threading
+import time
+
+sys.path.append(os.path.join(os.path.dirname(__file__), '../rpc_stubs'))
+
+import scheduler_to_worker_pb2 as s2w_pb2
+import scheduler_to_worker_pb2_grpc as s2w_pb2_grpc
+import common_pb2
+
+
+_ONE_DAY_IN_SECONDS = 60 * 60 * 24
+LOG_FORMAT = '{name}:{levelname} [{asctime}] {message}'
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
+
+
+class WorkerServer(s2w_pb2_grpc.SchedulerToWorkerServicer):
+    def __init__(self, callbacks, condition, logger):
+        self._callbacks = callbacks
+        self._condition = condition
+        self._logger = logger
+
+    def RunJob(self, request, context):
+        self._logger.debug('Received run job request from server')
+        #self._logger.debug(f'request: {request}')
+        run_job_callback = self._callbacks['RunJob']
+        run_job_callback(
+            request.trainer_ids, request.world_size, request.master_addr,
+            request.config, request.local_batch_size)
+        return common_pb2.Empty()
+
+    def KillJob(self, request, context):
+        self._logger.debug('Received kill job request from server')
+        kill_job_callback = self._callbacks['KillJob']
+        kill_job_callback(request.job_ids)
+        return common_pb2.Empty()
+
+    def Reset(self, request, context):
+        self._logger.debug('Received to reset jobs from server')
+        reset_callback = self._callbacks['Reset']
+        reset_callback()
+        return common_pb2.Empty()
+
+    def Shutdown(self, request, context):
+        # Handle any custom cleanup in the scheduler.
+        shutdown_callback = self._callbacks['Shutdown']
+        shutdown_callback()
+
+        # Indicate to the worker server that a shutdown RPC has been received.
+        self._condition.acquire()
+        self._condition.notify()
+        self._condition.release()
+
+        return common_pb2.Empty()
+
+
+def serve(port, callbacks):
+    logger = logging.getLogger(__name__)
+    logger.setLevel(logging.DEBUG)
+    ch = logging.StreamHandler()
+    ch.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT,
+                                      style='{'))
+    logger.addHandler(ch)
+    condition = threading.Condition()
+    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
+    s2w_pb2_grpc.add_SchedulerToWorkerServicer_to_server(
+            WorkerServer(callbacks, condition, logger), server)
+
+    logger.info('Starting server at port {0}'.format(port))
+    server.add_insecure_port('[::]:%d' % (port))
+    server.start()
+
+    # Wait for worker server to receive a shutdown RPC from scheduler.
+    with condition:
+        condition.wait()
+
+    # Wait for shutdown message to be sent to scheduler.
+    time.sleep(5)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/__init__.py b/torch/iidp/elastic/runtime/rpc_stubs/__init__.py
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/common_pb2.py b/torch/iidp/elastic/runtime/rpc_stubs/common_pb2.py
new file mode 100644
index 00000000000..5b60ce31775
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/common_pb2.py
@@ -0,0 +1,34 @@
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: common.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0c\x63ommon.proto\"\x07\n\x05\x45mptyb\x06proto3')
+
+
+
+_EMPTY = DESCRIPTOR.message_types_by_name['Empty']
+Empty = _reflection.GeneratedProtocolMessageType('Empty', (_message.Message,), {
+  'DESCRIPTOR' : _EMPTY,
+  '__module__' : 'common_pb2'
+  # @@protoc_insertion_point(class_scope:Empty)
+  })
+_sym_db.RegisterMessage(Empty)
+
+if _descriptor._USE_C_DESCRIPTORS == False:
+
+  DESCRIPTOR._options = None
+  _EMPTY._serialized_start=16
+  _EMPTY._serialized_end=23
+# @@protoc_insertion_point(module_scope)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/common_pb2_grpc.py b/torch/iidp/elastic/runtime/rpc_stubs/common_pb2_grpc.py
new file mode 100644
index 00000000000..2daafffebfc
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/common_pb2_grpc.py
@@ -0,0 +1,4 @@
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2.py b/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2.py
new file mode 100644
index 00000000000..d41232c8c8c
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2.py
@@ -0,0 +1,51 @@
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: scheduler_to_worker.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+import common_pb2 as common__pb2
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x19scheduler_to_worker.proto\x1a\x0c\x63ommon.proto\"\xc2\x01\n\rRunJobRequest\x12\x13\n\x0btrainer_ids\x18\x01 \x03(\x04\x12\x12\n\nworld_size\x18\x02 \x01(\r\x12\x13\n\x0bmaster_addr\x18\x03 \x01(\t\x12*\n\x06\x63onfig\x18\x04 \x03(\x0b\x32\x1a.RunJobRequest.ConfigEntry\x12\x18\n\x10local_batch_size\x18\x05 \x01(\r\x1a-\n\x0b\x43onfigEntry\x12\x0b\n\x03key\x18\x01 \x01(\r\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x32p\n\x11SchedulerToWorker\x12\"\n\x06RunJob\x12\x0e.RunJobRequest\x1a\x06.Empty\"\x00\x12\x19\n\x05Reset\x12\x06.Empty\x1a\x06.Empty\"\x00\x12\x1c\n\x08Shutdown\x12\x06.Empty\x1a\x06.Empty\"\x00\x62\x06proto3')
+
+
+
+_RUNJOBREQUEST = DESCRIPTOR.message_types_by_name['RunJobRequest']
+_RUNJOBREQUEST_CONFIGENTRY = _RUNJOBREQUEST.nested_types_by_name['ConfigEntry']
+RunJobRequest = _reflection.GeneratedProtocolMessageType('RunJobRequest', (_message.Message,), {
+
+  'ConfigEntry' : _reflection.GeneratedProtocolMessageType('ConfigEntry', (_message.Message,), {
+    'DESCRIPTOR' : _RUNJOBREQUEST_CONFIGENTRY,
+    '__module__' : 'scheduler_to_worker_pb2'
+    # @@protoc_insertion_point(class_scope:RunJobRequest.ConfigEntry)
+    })
+  ,
+  'DESCRIPTOR' : _RUNJOBREQUEST,
+  '__module__' : 'scheduler_to_worker_pb2'
+  # @@protoc_insertion_point(class_scope:RunJobRequest)
+  })
+_sym_db.RegisterMessage(RunJobRequest)
+_sym_db.RegisterMessage(RunJobRequest.ConfigEntry)
+
+_SCHEDULERTOWORKER = DESCRIPTOR.services_by_name['SchedulerToWorker']
+if _descriptor._USE_C_DESCRIPTORS == False:
+
+  DESCRIPTOR._options = None
+  _RUNJOBREQUEST_CONFIGENTRY._options = None
+  _RUNJOBREQUEST_CONFIGENTRY._serialized_options = b'8\001'
+  _RUNJOBREQUEST._serialized_start=44
+  _RUNJOBREQUEST._serialized_end=238
+  _RUNJOBREQUEST_CONFIGENTRY._serialized_start=193
+  _RUNJOBREQUEST_CONFIGENTRY._serialized_end=238
+  _SCHEDULERTOWORKER._serialized_start=240
+  _SCHEDULERTOWORKER._serialized_end=352
+# @@protoc_insertion_point(module_scope)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2_grpc.py b/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2_grpc.py
new file mode 100644
index 00000000000..fa04f6b2ee6
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/scheduler_to_worker_pb2_grpc.py
@@ -0,0 +1,136 @@
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+import common_pb2 as common__pb2
+import scheduler_to_worker_pb2 as scheduler__to__worker__pb2
+
+
+class SchedulerToWorkerStub(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.RunJob = channel.unary_unary(
+                '/SchedulerToWorker/RunJob',
+                request_serializer=scheduler__to__worker__pb2.RunJobRequest.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+        self.Reset = channel.unary_unary(
+                '/SchedulerToWorker/Reset',
+                request_serializer=common__pb2.Empty.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+        self.Shutdown = channel.unary_unary(
+                '/SchedulerToWorker/Shutdown',
+                request_serializer=common__pb2.Empty.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+
+
+class SchedulerToWorkerServicer(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def RunJob(self, request, context):
+        """Start list of jobs on worker.
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def Reset(self, request, context):
+        """Resets the worker.
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def Shutdown(self, request, context):
+        """Shuts down the worker.
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+
+def add_SchedulerToWorkerServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+            'RunJob': grpc.unary_unary_rpc_method_handler(
+                    servicer.RunJob,
+                    request_deserializer=scheduler__to__worker__pb2.RunJobRequest.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+            'Reset': grpc.unary_unary_rpc_method_handler(
+                    servicer.Reset,
+                    request_deserializer=common__pb2.Empty.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+            'Shutdown': grpc.unary_unary_rpc_method_handler(
+                    servicer.Shutdown,
+                    request_deserializer=common__pb2.Empty.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+            'SchedulerToWorker', rpc_method_handlers)
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+ # This class is part of an EXPERIMENTAL API.
+class SchedulerToWorker(object):
+    """Missing associated documentation comment in .proto file."""
+
+    @staticmethod
+    def RunJob(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/SchedulerToWorker/RunJob',
+            scheduler__to__worker__pb2.RunJobRequest.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def Reset(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/SchedulerToWorker/Reset',
+            common__pb2.Empty.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def Shutdown(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/SchedulerToWorker/Shutdown',
+            common__pb2.Empty.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2.py b/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2.py
new file mode 100644
index 00000000000..e76a98c52ea
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2.py
@@ -0,0 +1,61 @@
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: trainer_to_scheduler.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+import common_pb2 as common__pb2
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x1atrainer_to_scheduler.proto\x1a\x0c\x63ommon.proto\"$\n\x0eInitJobRequest\x12\x12\n\ntrainer_id\x18\x01 \x01(\x04\"\x90\x01\n\x13UpdateConfigRequest\x12\x30\n\x06\x63onfig\x18\x01 \x03(\x0b\x32 .UpdateConfigRequest.ConfigEntry\x12\x18\n\x10local_batch_size\x18\x02 \x01(\r\x1a-\n\x0b\x43onfigEntry\x12\x0b\n\x03key\x18\x01 \x01(\r\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x32\x84\x01\n\x12TrainerToScheduler\x12\"\n\x07InitJob\x12\x0f.InitJobRequest\x1a\x06.Empty\x12,\n\x0cUpdateConfig\x12\x14.UpdateConfigRequest\x1a\x06.Empty\x12\x1c\n\x08ShutDown\x12\x06.Empty\x1a\x06.Empty\"\x00\x62\x06proto3')
+
+
+
+_INITJOBREQUEST = DESCRIPTOR.message_types_by_name['InitJobRequest']
+_UPDATECONFIGREQUEST = DESCRIPTOR.message_types_by_name['UpdateConfigRequest']
+_UPDATECONFIGREQUEST_CONFIGENTRY = _UPDATECONFIGREQUEST.nested_types_by_name['ConfigEntry']
+InitJobRequest = _reflection.GeneratedProtocolMessageType('InitJobRequest', (_message.Message,), {
+  'DESCRIPTOR' : _INITJOBREQUEST,
+  '__module__' : 'trainer_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:InitJobRequest)
+  })
+_sym_db.RegisterMessage(InitJobRequest)
+
+UpdateConfigRequest = _reflection.GeneratedProtocolMessageType('UpdateConfigRequest', (_message.Message,), {
+
+  'ConfigEntry' : _reflection.GeneratedProtocolMessageType('ConfigEntry', (_message.Message,), {
+    'DESCRIPTOR' : _UPDATECONFIGREQUEST_CONFIGENTRY,
+    '__module__' : 'trainer_to_scheduler_pb2'
+    # @@protoc_insertion_point(class_scope:UpdateConfigRequest.ConfigEntry)
+    })
+  ,
+  'DESCRIPTOR' : _UPDATECONFIGREQUEST,
+  '__module__' : 'trainer_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:UpdateConfigRequest)
+  })
+_sym_db.RegisterMessage(UpdateConfigRequest)
+_sym_db.RegisterMessage(UpdateConfigRequest.ConfigEntry)
+
+_TRAINERTOSCHEDULER = DESCRIPTOR.services_by_name['TrainerToScheduler']
+if _descriptor._USE_C_DESCRIPTORS == False:
+
+  DESCRIPTOR._options = None
+  _UPDATECONFIGREQUEST_CONFIGENTRY._options = None
+  _UPDATECONFIGREQUEST_CONFIGENTRY._serialized_options = b'8\001'
+  _INITJOBREQUEST._serialized_start=44
+  _INITJOBREQUEST._serialized_end=80
+  _UPDATECONFIGREQUEST._serialized_start=83
+  _UPDATECONFIGREQUEST._serialized_end=227
+  _UPDATECONFIGREQUEST_CONFIGENTRY._serialized_start=182
+  _UPDATECONFIGREQUEST_CONFIGENTRY._serialized_end=227
+  _TRAINERTOSCHEDULER._serialized_start=230
+  _TRAINERTOSCHEDULER._serialized_end=362
+# @@protoc_insertion_point(module_scope)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2_grpc.py b/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2_grpc.py
new file mode 100644
index 00000000000..e058622741c
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/trainer_to_scheduler_pb2_grpc.py
@@ -0,0 +1,135 @@
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+import common_pb2 as common__pb2
+import trainer_to_scheduler_pb2 as trainer__to__scheduler__pb2
+
+
+class TrainerToSchedulerStub(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.InitJob = channel.unary_unary(
+                '/TrainerToScheduler/InitJob',
+                request_serializer=trainer__to__scheduler__pb2.InitJobRequest.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+        self.UpdateConfig = channel.unary_unary(
+                '/TrainerToScheduler/UpdateConfig',
+                request_serializer=trainer__to__scheduler__pb2.UpdateConfigRequest.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+        self.ShutDown = channel.unary_unary(
+                '/TrainerToScheduler/ShutDown',
+                request_serializer=common__pb2.Empty.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+
+
+class TrainerToSchedulerServicer(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def InitJob(self, request, context):
+        """Initializes the job.
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def UpdateConfig(self, request, context):
+        """Update Configuraton
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def ShutDown(self, request, context):
+        """Missing associated documentation comment in .proto file."""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+
+def add_TrainerToSchedulerServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+            'InitJob': grpc.unary_unary_rpc_method_handler(
+                    servicer.InitJob,
+                    request_deserializer=trainer__to__scheduler__pb2.InitJobRequest.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+            'UpdateConfig': grpc.unary_unary_rpc_method_handler(
+                    servicer.UpdateConfig,
+                    request_deserializer=trainer__to__scheduler__pb2.UpdateConfigRequest.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+            'ShutDown': grpc.unary_unary_rpc_method_handler(
+                    servicer.ShutDown,
+                    request_deserializer=common__pb2.Empty.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+            'TrainerToScheduler', rpc_method_handlers)
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+ # This class is part of an EXPERIMENTAL API.
+class TrainerToScheduler(object):
+    """Missing associated documentation comment in .proto file."""
+
+    @staticmethod
+    def InitJob(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/TrainerToScheduler/InitJob',
+            trainer__to__scheduler__pb2.InitJobRequest.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def UpdateConfig(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/TrainerToScheduler/UpdateConfig',
+            trainer__to__scheduler__pb2.UpdateConfigRequest.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def ShutDown(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/TrainerToScheduler/ShutDown',
+            common__pb2.Empty.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2.py b/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2.py
new file mode 100644
index 00000000000..a667356db1c
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2.py
@@ -0,0 +1,68 @@
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: worker_to_scheduler.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+import common_pb2 as common__pb2
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x19worker_to_scheduler.proto\x1a\x0c\x63ommon.proto\"n\n\x15RegisterWorkerRequest\x12\x11\n\tdevice_id\x18\x01 \x01(\t\x12\x11\n\tworker_id\x18\x02 \x01(\x04\x12\x10\n\x08num_gpus\x18\x03 \x01(\r\x12\x0f\n\x07ip_addr\x18\x04 \x01(\t\x12\x0c\n\x04port\x18\x05 \x01(\r\"S\n\x16RegisterWorkerResponse\x12\x0f\n\x07success\x18\x01 \x01(\x08\x12\x11\n\tworker_id\x18\x02 \x01(\x04\x12\x15\n\rerror_message\x18\x03 \x01(\t\" \n\x0b\x44oneRequest\x12\x11\n\tworker_id\x18\x01 \x01(\x04\"\"\n\rKilledRequest\x12\x11\n\tworker_id\x18\x01 \x01(\x04\x32\x96\x01\n\x11WorkerToScheduler\x12\x41\n\x0eRegisterWorker\x12\x16.RegisterWorkerRequest\x1a\x17.RegisterWorkerResponse\x12\x1c\n\x04\x44one\x12\x0c.DoneRequest\x1a\x06.Empty\x12 \n\x06Killed\x12\x0e.KilledRequest\x1a\x06.Emptyb\x06proto3')
+
+
+
+_REGISTERWORKERREQUEST = DESCRIPTOR.message_types_by_name['RegisterWorkerRequest']
+_REGISTERWORKERRESPONSE = DESCRIPTOR.message_types_by_name['RegisterWorkerResponse']
+_DONEREQUEST = DESCRIPTOR.message_types_by_name['DoneRequest']
+_KILLEDREQUEST = DESCRIPTOR.message_types_by_name['KilledRequest']
+RegisterWorkerRequest = _reflection.GeneratedProtocolMessageType('RegisterWorkerRequest', (_message.Message,), {
+  'DESCRIPTOR' : _REGISTERWORKERREQUEST,
+  '__module__' : 'worker_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:RegisterWorkerRequest)
+  })
+_sym_db.RegisterMessage(RegisterWorkerRequest)
+
+RegisterWorkerResponse = _reflection.GeneratedProtocolMessageType('RegisterWorkerResponse', (_message.Message,), {
+  'DESCRIPTOR' : _REGISTERWORKERRESPONSE,
+  '__module__' : 'worker_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:RegisterWorkerResponse)
+  })
+_sym_db.RegisterMessage(RegisterWorkerResponse)
+
+DoneRequest = _reflection.GeneratedProtocolMessageType('DoneRequest', (_message.Message,), {
+  'DESCRIPTOR' : _DONEREQUEST,
+  '__module__' : 'worker_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:DoneRequest)
+  })
+_sym_db.RegisterMessage(DoneRequest)
+
+KilledRequest = _reflection.GeneratedProtocolMessageType('KilledRequest', (_message.Message,), {
+  'DESCRIPTOR' : _KILLEDREQUEST,
+  '__module__' : 'worker_to_scheduler_pb2'
+  # @@protoc_insertion_point(class_scope:KilledRequest)
+  })
+_sym_db.RegisterMessage(KilledRequest)
+
+_WORKERTOSCHEDULER = DESCRIPTOR.services_by_name['WorkerToScheduler']
+if _descriptor._USE_C_DESCRIPTORS == False:
+
+  DESCRIPTOR._options = None
+  _REGISTERWORKERREQUEST._serialized_start=43
+  _REGISTERWORKERREQUEST._serialized_end=153
+  _REGISTERWORKERRESPONSE._serialized_start=155
+  _REGISTERWORKERRESPONSE._serialized_end=238
+  _DONEREQUEST._serialized_start=240
+  _DONEREQUEST._serialized_end=272
+  _KILLEDREQUEST._serialized_start=274
+  _KILLEDREQUEST._serialized_end=308
+  _WORKERTOSCHEDULER._serialized_start=311
+  _WORKERTOSCHEDULER._serialized_end=461
+# @@protoc_insertion_point(module_scope)
diff --git a/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2_grpc.py b/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2_grpc.py
new file mode 100644
index 00000000000..049254599ea
--- /dev/null
+++ b/torch/iidp/elastic/runtime/rpc_stubs/worker_to_scheduler_pb2_grpc.py
@@ -0,0 +1,136 @@
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+import common_pb2 as common__pb2
+import worker_to_scheduler_pb2 as worker__to__scheduler__pb2
+
+
+class WorkerToSchedulerStub(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.RegisterWorker = channel.unary_unary(
+                '/WorkerToScheduler/RegisterWorker',
+                request_serializer=worker__to__scheduler__pb2.RegisterWorkerRequest.SerializeToString,
+                response_deserializer=worker__to__scheduler__pb2.RegisterWorkerResponse.FromString,
+                )
+        self.Done = channel.unary_unary(
+                '/WorkerToScheduler/Done',
+                request_serializer=worker__to__scheduler__pb2.DoneRequest.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+        self.Killed = channel.unary_unary(
+                '/WorkerToScheduler/Killed',
+                request_serializer=worker__to__scheduler__pb2.KilledRequest.SerializeToString,
+                response_deserializer=common__pb2.Empty.FromString,
+                )
+
+
+class WorkerToSchedulerServicer(object):
+    """Missing associated documentation comment in .proto file."""
+
+    def RegisterWorker(self, request, context):
+        """Registers the worker with the scheduler
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def Done(self, request, context):
+        """Indicates to the scheduler that a job has completed
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def Killed(self, request, context):
+        """Indicates to the scheduler that worker was killed
+        """
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+
+def add_WorkerToSchedulerServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+            'RegisterWorker': grpc.unary_unary_rpc_method_handler(
+                    servicer.RegisterWorker,
+                    request_deserializer=worker__to__scheduler__pb2.RegisterWorkerRequest.FromString,
+                    response_serializer=worker__to__scheduler__pb2.RegisterWorkerResponse.SerializeToString,
+            ),
+            'Done': grpc.unary_unary_rpc_method_handler(
+                    servicer.Done,
+                    request_deserializer=worker__to__scheduler__pb2.DoneRequest.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+            'Killed': grpc.unary_unary_rpc_method_handler(
+                    servicer.Killed,
+                    request_deserializer=worker__to__scheduler__pb2.KilledRequest.FromString,
+                    response_serializer=common__pb2.Empty.SerializeToString,
+            ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+            'WorkerToScheduler', rpc_method_handlers)
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+ # This class is part of an EXPERIMENTAL API.
+class WorkerToScheduler(object):
+    """Missing associated documentation comment in .proto file."""
+
+    @staticmethod
+    def RegisterWorker(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/WorkerToScheduler/RegisterWorker',
+            worker__to__scheduler__pb2.RegisterWorkerRequest.SerializeToString,
+            worker__to__scheduler__pb2.RegisterWorkerResponse.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def Done(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/WorkerToScheduler/Done',
+            worker__to__scheduler__pb2.DoneRequest.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def Killed(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/WorkerToScheduler/Killed',
+            worker__to__scheduler__pb2.KilledRequest.SerializeToString,
+            common__pb2.Empty.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
diff --git a/torch/iidp/elastic/scheduler.py b/torch/iidp/elastic/scheduler.py
new file mode 100644
index 00000000000..79340fb1d6e
--- /dev/null
+++ b/torch/iidp/elastic/scheduler.py
@@ -0,0 +1,269 @@
+from torch.iidp.elastic.runtime.rpc import scheduler_server, scheduler_client
+import threading
+import time
+import json
+import copy
+
+
+class WorkerInfo:
+    def __init__(self, worker_id, device_name, max_num_gpus, sched_rpc_client,
+                 trainer_ids=None):
+        self.worker_id = worker_id
+        self.device_name = device_name
+        self.max_num_gpus = max_num_gpus
+        self.rpc_client = sched_rpc_client
+        self.trainer_ids = trainer_ids if trainer_ids is not None else []
+
+        start_rank = worker_id * max_num_gpus
+        self.ranks = list(range(start_rank, start_rank+max_num_gpus))
+
+
+class Timer:
+    def __init__(self, start_time):
+        self.start_time = start_time
+        self.elapsed_time = 0
+
+    def update(self, measured_time):
+        self.elapsed_time = measured_time - self.start_time
+
+
+class Scheduler:
+    def __init__(self, scheduler_port, init_file):
+        # Synchronization primitives to ensure thread-safe updates of
+        # scheduler metadata.
+        self._scheduler_lock = threading.Lock()
+        self._scheduler_cv = threading.Condition(self._scheduler_lock)
+        # List to Maintain Global Information
+        """{ Worker ID : WorkerInfo} """
+        self._worker_list = {}
+        self._rank_to_worker_id_map = {}
+        self._worker_changed = False
+        self._need_to_reschedule = False
+        self._init = True
+        self._initialized = False
+        self._updated = False
+        self._finished = False
+        with open(init_file, "r") as json_file:
+            self._init_workers = json.load(json_file, object_pairs_hook=lambda pairs: {int(k): v for k, v in pairs})
+        self._updated_config = {}
+        self._updated_local_batch_size = 0
+        self.initialize_overhead_timer = None
+        self.initialize_overhead = -1
+
+        callbacks = {
+            'RegisterWorker': self._register_worker_callback,
+            'InitJob': self._init_job_callback,
+            'UpdateConfig': self._update_config_callback,
+            'Done': self._done_callback,
+            'Killed': self._killed_callback,
+            'ShutDown': self.shut_down
+        }
+
+        self._reschedule_thread = \
+            threading.Thread(target=self._reschedule_thread)
+        self._reschedule_thread.daemon = True
+        self._reschedule_thread.start()
+
+        self._server_thread = threading.Thread(
+            target=scheduler_server.serve,
+            args=(scheduler_port, callbacks))
+        self._server_thread.daemon = True
+        self._server_thread.start()
+
+    """
+    ======================================================================
+       Callback methods called by workers.
+    ======================================================================
+    """
+
+    def _register_worker_callback(self, device_id: str, worker_id: int,
+                                  num_gpus: int, ip_addr: str, port: int):
+        if worker_id in self._worker_list:
+            return (False, -1, "Agent already running.")
+        if num_gpus <= 0:
+            return (False, -1, "No Device to work with.")
+        # Share a single RPC client for each GPU on the worker.
+        rpc_client = scheduler_client.SchedulerRpcClient(ip_addr, port)
+
+        with self._scheduler_lock:
+            self._worker_changed = True
+            self._worker_list[worker_id] = WorkerInfo(worker_id, device_id, num_gpus, rpc_client)
+            for trainer_id in self._worker_list[worker_id].ranks:
+                self._rank_to_worker_id_map[trainer_id] = worker_id
+
+            if worker_id in self._init_workers:
+                if not (num_gpus >= self._init_workers[worker_id] and self._init_workers[worker_id] >= 0):
+                    self._shut_down()
+                if self.initialize_overhead_timer is None:
+                    self.initialize_overhead_timer = Timer(time.time())
+
+            if not self._initialized:
+                initialize_finished = all(id in self._worker_list for id in self._init_workers)
+                if initialize_finished:
+                    self._init = True
+                    self._initialized = True
+                    self._need_to_reschedule = True
+                    self._scheduler_cv.notifyAll()
+        return (True, worker_id, "")
+
+    def _init_job_callback(self, trainer_id: int):
+        # TODO: Implement the initial callback for IIDP trainer
+        with self._scheduler_lock:
+            if trainer_id not in self._rank_to_worker_id_map:
+                return "Unknown Trainer ID : {}".format(trainer_id)
+            if self.initialize_overhead < 0:
+                self.initialize_overhead_timer.update(time.time())
+                self.initialize_overhead = self.initialize_overhead_timer.elapsed_time
+                print(f'[INFO][torch/iidp/elastic/scheduler.py] Initialize overhead = {self.initialize_overhead:.2f} sec')
+            self._updated = False
+        return None
+
+    def _update_config_callback(self, config, local_batch_size):
+        with self._scheduler_lock:
+            #print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _update_config_callback() starts!')
+            if len(config) == 0 and not self._worker_changed:
+                return False
+            if self._updated:
+                return False
+            self._worker_changed = False
+            sorted_config = dict(sorted(config.items()))
+            self._updated_config.update(sorted_config)
+            self._updated_local_batch_size = local_batch_size
+            self._need_to_reschedule = True
+            self._init = False
+            self._updated = True
+            self._scheduler_cv.notifyAll()
+        return True
+
+    def _done_callback(self, worker_id: int):
+        # Every job in worker is done
+        with self._scheduler_lock:
+            if worker_id not in self._worker_list:
+                return None, False
+            trainer_ids = copy.deepcopy(self._worker_list[worker_id].trainer_ids)
+            self._worker_list[worker_id].trainer_ids.clear()
+            return trainer_ids, True
+
+    def _killed_callback(self, worker_id: int):
+        # Worker is killed
+        with self._scheduler_lock:
+            if worker_id not in self._worker_list:
+                return False
+            self._worker_changed = True
+
+            del self._worker_list[worker_id]
+            if worker_id in self._updated_config:
+                del self._updated_config[worker_id]
+            if len(self._worker_list) == 0:
+                self._initialized = False
+            return True
+
+    """
+    ======================================================================
+       Public-facing scheduler methods.
+    ======================================================================
+    """
+
+    def _run_trainer(self, worker_id, trainer_ids, world_size, master_addr="", trainer_config_map={}):
+        rpc_client = self._worker_list[worker_id].rpc_client
+        local_batch_size = self._updated_local_batch_size
+        rpc_client.run_job(trainer_ids, world_size, master_addr, trainer_config_map, local_batch_size)
+
+    def _reset_workers(self):
+        #print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reset_workers() starts! => rpc_client.reset()')
+        # rpc_client and trainer IDs are reset in self._worker_list
+        for worker_id in self._worker_list.keys():
+            self._worker_list[worker_id].rpc_client.reset()
+            self._worker_list[worker_id].trainer_ids.clear()
+
+    def shut_down(self):
+        """Sends a shutdown signal to every worker and ends the scheduler."""
+        with self._scheduler_lock:
+            if self._finished:
+                return
+            for worker in self._worker_list.values():
+                worker.rpc_client.shutdown()
+            self._finished = True
+
+    def _shut_down(self):
+        if self._finished:
+            return
+        for worker in self._worker_list.values():
+            worker.rpc_client.shutdown()
+        self._finished = True
+        exit(-1)
+
+    """
+    ======================================================================
+       Helper methods to get and mutate state needed for scheduling.
+    ======================================================================
+    """
+
+    def _reschedule_thread(self):
+        """Computes the VSWs/Resource Rescheduling asynchronously."""
+        while True:
+            # Check whether rescheduling needs to be re-computed.
+            self._scheduler_cv.acquire()
+            while not self._need_to_reschedule:
+                self._scheduler_cv.wait()
+            self._scheduler_cv.release()
+            # Reschedule
+            with self._scheduler_lock:
+                self._reschedule()
+                self._updated_config.clear()
+                self._need_to_reschedule = False
+                self._init = False
+
+    def _reschedule(self):
+        print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() starts with self._init: {self._init}!')
+        new_worker_list = {}
+        if self._init:
+            new_worker_list.update(self._init_workers)
+            print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() - new_worker_list: {new_worker_list}!')
+            rank = 0
+            world_size = sum(new_worker_list.values())
+            for worker_id in sorted(self._worker_list.keys()):
+                if worker_id in new_worker_list:
+                    new_trainer_ids = list(set(range(rank, rank + new_worker_list[worker_id])))
+                    #for trainer_id in new_trainer_set:
+                    #    self._rank_to_worker_id_map[trainer_id] = worker_id
+                    print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() => worker_id: {worker_id}! | new_trainer_ids: {new_trainer_ids}')
+                    self._worker_list[worker_id].trainer_ids = new_trainer_ids
+                    self._run_trainer(worker_id, new_trainer_ids, world_size)
+                    rank += new_worker_list[worker_id]
+                    #print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() => after self._run_trainer | rank: {rank}')
+        else:
+            print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() check _updated_config: {self._updated_config}!')
+            print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() check _worker_list: {self._worker_list}!')
+            print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() check _rank_to_worker_id_map: {self._rank_to_worker_id_map}!')
+            for rank, config_str in self._updated_config.items():
+                worker_id = self._rank_to_worker_id_map[rank]
+                if worker_id not in new_worker_list.keys():
+                    new_worker_list[worker_id] = {}
+                new_worker_list[worker_id].update({rank: config_str})
+            print(f'[DEBUG][torch/iidp/elastic/scheduler.py] _reschedule() new_worker_list: {new_worker_list}!')
+            self._reset_workers()
+            rank = 0
+            world_size = len(self._updated_config.keys())
+            master_addr = ""
+            for worker_id in sorted(self._worker_list.keys()):
+                if worker_id in new_worker_list:
+                    trainer_ids = list(set(range(rank, rank+len(new_worker_list[worker_id]))))
+                    print(f'[DEBUG][torch/iidp/elastic/scheduler.py] trainer_ids: {trainer_ids}!')
+                    new_trainer_config_map = {}
+                    for idx, (_, config_str) in enumerate(new_worker_list[worker_id].items()):
+                        new_rank = trainer_ids[idx]
+                        new_trainer_config_map[new_rank] = config_str
+                    print(f'[DEBUG][torch/iidp/elastic/scheduler.py] new_trainer_config_map: {new_trainer_config_map}!')
+                    self._worker_list[worker_id].trainer_ids = trainer_ids
+                    if 0 in trainer_ids: # If rank 0 is included, it is master worker (server)
+                        master_addr = self._worker_list[worker_id].rpc_client.addr
+                        print(f'[DEBUG][torch/iidp/elastic/scheduler.py] master_addr: {master_addr}!')
+                    if not master_addr:
+                        raise ValueError(f"[ERROR][torch/iidp/elastic/scheduler.py] master_addr must be configured, but {master_addr}")
+                    self._run_trainer(worker_id, trainer_ids, world_size, master_addr, new_trainer_config_map)
+                    rank += len(new_worker_list[worker_id])
+
+        if not (rank == world_size):
+            print(f'[ERROR][torch/iidp/elastic/scheduler.py] _reschedule() - rank: {rank} != world_size: {world_size}!')
+            self._shut_down()
\ No newline at end of file
diff --git a/torch/iidp/elastic/worker.py b/torch/iidp/elastic/worker.py
new file mode 100644
index 00000000000..362228dfbb3
--- /dev/null
+++ b/torch/iidp/elastic/worker.py
@@ -0,0 +1,119 @@
+import argparse
+import datetime
+import logging
+import os
+import queue
+import shutil
+import socket
+import signal
+import sys
+import threading
+
+from runtime.rpc import dispatcher
+from runtime.rpc import worker_client
+from runtime.rpc import worker_server
+
+from torch.iidp.utils.json_utils import read_json
+
+import _utils
+
+LOG_FORMAT = '{name}:{levelname} [{asctime}] {message}'
+DATE_FORMAT = '%Y-%m-%d %H:%M:%S'
+
+
+class Worker:
+    def __init__(self, worker_id, sched_addr, sched_port, worker_port,
+                 cmd, initial_url, initial_config, initial_local_batch_size,
+                 checkpoint_dir, gpu_cluster_info_file, log_dir=None):
+        logger = logging.getLogger('worker')
+        logger.setLevel(logging.DEBUG)
+        ch = logging.StreamHandler()
+        ch.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT,
+                                          style='{'))
+        logger.addHandler(ch)
+        self._logger = logger
+        self._logging_handler = ch
+
+        signal.signal(signal.SIGINT, self._signal_handler)
+
+        self.gpu_cluster_info = read_json(gpu_cluster_info_file)
+        self.server_info = self.gpu_cluster_info[socket.gethostname()]
+        num_gpus = self.server_info['number']
+        self._device_id = self.server_info['type']
+        self._worker_id = worker_id
+        self._worker_addr = socket.gethostbyname(socket.gethostname())
+        self._worker_port = worker_port
+        self._worker_rpc_client = worker_client.WorkerRpcClient(
+                self._device_id, self._worker_id, self._worker_addr,
+                self._worker_port, sched_addr, sched_port)
+
+        callbacks = {
+            'RunJob': self._run_job_callback,
+            'Reset': self._reset_callback,
+            'Shutdown': self._shutdown_callback,
+        }
+
+        self._server_thread = threading.Thread(
+            target=worker_server.serve,
+            args=(worker_port, callbacks,))
+        self._server_thread.daemon = True
+        self._server_thread.start()
+
+        error = self._worker_rpc_client.register_worker(num_gpus)
+        if error:
+            raise RuntimeError(error)
+
+        if not os.path.isdir(checkpoint_dir):
+            # Set up a new checkpoint directory if does not already exist.
+            os.mkdir(checkpoint_dir)
+
+        if not initial_url:
+            raise ValueError(f'Worker must have initial URL for distributed training, but: {initial_url}')
+
+        if not initial_config:
+            raise ValueError(f'Worker must have initial IIDP configuration (VSW, GA), but: {initial_config}')
+
+        if not initial_local_batch_size:
+            raise ValueError(f'Worker must have initial local batch size, but: {initial_local_batch_size}')
+
+        self._dispatcher = dispatcher.Dispatcher(self._worker_rpc_client,
+                                                 sched_addr,
+                                                 sched_port,
+                                                 cmd,
+                                                 initial_url,
+                                                 initial_config,
+                                                 initial_local_batch_size,
+                                                 checkpoint_dir,
+                                                 worker_id,
+                                                 self.server_info,
+                                                 self.gpu_cluster_info,
+                                                 log_dir)
+
+    def _run_job_callback(self, trainer_ids, world_size: int, master_addr="", config={}, local_batch_size=0):
+        # hack to prevent a job being dispatched before the dispatcher is set up
+        # TODO: fix this by sending a "I'm ready" message to scheduler
+        while True:
+            try:
+                self._dispatcher
+                break
+            except Exception as e:
+              continue
+        self._logger.debug(f'Dispatching run request from scheduler client gRPC')
+        self._dispatcher.dispatch_jobs(trainer_ids, world_size, master_addr, config, local_batch_size)
+
+    def _signal_handler(self, sig, frame):
+        self._dispatcher.shutdown(killed=True)
+        self._logger.removeHandler(self._logging_handler)
+        self._logging_handler.close()
+        sys.exit(0)
+
+    def _reset_callback(self):
+        self._dispatcher.reset()
+
+    def _shutdown_callback(self):
+        self._dispatcher.shutdown()
+        self._logger.removeHandler(self._logging_handler)
+        self._logging_handler.close()
+
+    def join(self):
+        self._server_thread.join()
\ No newline at end of file
diff --git a/torch/iidp/optim/__init__.py b/torch/iidp/optim/__init__.py
new file mode 100644
index 00000000000..b4111744a1a
--- /dev/null
+++ b/torch/iidp/optim/__init__.py
@@ -0,0 +1,3 @@
+from .shard_optimizer import *
+
+__all__ = ['ShardSGD', 'ShardAdam', 'ShardAdamW']
\ No newline at end of file
diff --git a/torch/iidp/optim/shard_optimizer.py b/torch/iidp/optim/shard_optimizer.py
new file mode 100644
index 00000000000..4bea88da7da
--- /dev/null
+++ b/torch/iidp/optim/shard_optimizer.py
@@ -0,0 +1,250 @@
+import torch
+from torch.optim.optimizer import Optimizer, required
+import torch.optim._functional as F
+
+
+class ShardSGD(Optimizer):
+    def __init__(self, params, lr=required, momentum=0, dampening=0,
+                 weight_decay=0, nesterov=False):
+        if lr is not required and lr < 0.0:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if momentum < 0.0:
+            raise ValueError("Invalid momentum value: {}".format(momentum))
+        if weight_decay < 0.0:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+
+        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,
+                        weight_decay=weight_decay, nesterov=nesterov)
+        if nesterov and (momentum <= 0 or dampening != 0):
+            raise ValueError("Nesterov momentum requires a momentum and zero dampening")
+        super(ShardSGD, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(ShardSGD, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('nesterov', False)
+
+    @torch.no_grad()
+    def step(self, gradients):
+        loss = None
+        if gradients is None or len(gradients) == 0:
+            raise RuntimeError("Arguments gradients must not be empty or None")
+        for group in self.param_groups:
+            params_with_grad = []
+            d_p_list = []
+            momentum_buffer_list = []
+            weight_decay = group['weight_decay']
+            momentum = group['momentum']
+            dampening = group['dampening']
+            nesterov = group['nesterov']
+            lr = group['lr']
+            params = group['params']
+
+            for p in params:
+                grad = None
+                for g in gradients:
+                    if p.index == g.index:
+                        grad = g
+                if grad is not None:
+                    params_with_grad.append(p)
+                    d_p_list.append(grad)
+
+                    state = self.state[p]
+                    if 'momentum_buffer' not in state:
+                        momentum_buffer_list.append(None)
+                    else:
+                        momentum_buffer_list.append(state['momentum_buffer'])
+
+            F.sgd(params_with_grad,
+                  d_p_list,
+                  momentum_buffer_list,
+                  weight_decay=weight_decay,
+                  momentum=momentum,
+                  lr=lr,
+                  dampening=dampening,
+                  nesterov=nesterov)
+
+            # update momentum_buffers in state
+            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):
+                state = self.state[p]
+                state['momentum_buffer'] = momentum_buffer
+
+        return loss
+
+
+class ShardAdam(Optimizer):
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
+                 weight_decay=0, amsgrad=False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        if not 0.0 <= weight_decay:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay, amsgrad=amsgrad)
+        super(ShardAdam, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(ShardAdam, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('amsgrad', False)
+
+    @torch.no_grad()
+    def step(self, gradients):
+        loss = None
+        if gradients is None or len(gradients) == 0:
+            raise RuntimeError("Arguments gradients must not be empty or None")
+        for group in self.param_groups:
+            params_with_grad = []
+            grads = []
+            exp_avgs = []
+            exp_avg_sqs = []
+            state_sums = []
+            max_exp_avg_sqs = []
+            state_steps = []
+            params = group['params']
+
+            for p in params:
+                grad = None
+                for g in gradients:
+                    if p.index == g.index:
+                        grad = g
+                if grad is not None:
+                    params_with_grad.append(p)
+                    if grad.is_sparse:
+                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
+                    grads.append(grad)
+
+                    state = self.state[p]
+                    # Lazy state initialization
+                    if len(state) == 0:
+                        state['step'] = 0
+                        # Exponential moving average of gradient values
+                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        # Exponential moving average of squared gradient values
+                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        if group['amsgrad']:
+                            # Maintains max of all exp. moving avg. of sq. grad. values
+                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+
+                    exp_avgs.append(state['exp_avg'])
+                    exp_avg_sqs.append(state['exp_avg_sq'])
+
+                    if group['amsgrad']:
+                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])
+
+                    # update the steps for each param group update
+                    state['step'] += 1
+                    # record the step after step update
+                    state_steps.append(state['step'])
+
+            beta1, beta2 = group['betas']
+            F.adam(params_with_grad,
+                   grads,
+                   exp_avgs,
+                   exp_avg_sqs,
+                   max_exp_avg_sqs,
+                   state_steps,
+                   group['amsgrad'],
+                   beta1,
+                   beta2,
+                   group['lr'],
+                   group['weight_decay'],
+                   group['eps'])
+        return loss
+
+
+class ShardAdamW(Optimizer):
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
+                 weight_decay=1e-2, amsgrad=False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        if not 0.0 <= weight_decay:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay, amsgrad=amsgrad)
+        super(ShardAdamW, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(ShardAdamW, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('amsgrad', False)
+
+    @torch.no_grad()
+    def step(self, gradients):
+        loss = None
+        if gradients is None or len(gradients) == 0:
+            raise RuntimeError("Arguments gradients must not be empty or None")
+        for group in self.param_groups:
+            params_with_grad = []
+            grads = []
+            exp_avgs = []
+            exp_avg_sqs = []
+            state_sums = []
+            max_exp_avg_sqs = []
+            state_steps = []
+            amsgrad = group['amsgrad']
+
+            for p in group['params']:
+                if p.grad is None:
+                    continue
+                grad = None
+                for g in gradients:
+                    if p.index == g.index:
+                        grad = g
+                if grad is not None:
+                    params_with_grad.append(p)
+                    if p.grad.is_sparse:
+                        raise RuntimeError('AdamW does not support sparse gradients')
+                    grads.append(grad)
+
+                    state = self.state[p]
+
+                    # State initialization
+                    if len(state) == 0:
+                        state['step'] = 0
+                        # Exponential moving average of gradient values
+                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        # Exponential moving average of squared gradient values
+                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        if amsgrad:
+                            # Maintains max of all exp. moving avg. of sq. grad. values
+                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+
+                    exp_avgs.append(state['exp_avg'])
+                    exp_avg_sqs.append(state['exp_avg_sq'])
+
+                    if amsgrad:
+                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])
+
+                    beta1, beta2 = group['betas']
+                    # update the steps for each param group update
+                    state['step'] += 1
+                    # record the step after step update
+                    state_steps.append(state['step'])
+
+            F.adamw(params_with_grad,
+                    grads,
+                    exp_avgs,
+                    exp_avg_sqs,
+                    max_exp_avg_sqs,
+                    state_steps,
+                    amsgrad,
+                    beta1,
+                    beta2,
+                    group['lr'],
+                    group['weight_decay'],
+                    group['eps'])
+
+        return loss
\ No newline at end of file
diff --git a/torch/iidp/profiler/__init__.py b/torch/iidp/profiler/__init__.py
new file mode 100644
index 00000000000..e9500df5113
--- /dev/null
+++ b/torch/iidp/profiler/__init__.py
@@ -0,0 +1,5 @@
+from .profiler import *
+from .profiler_utils import *
+from .api import *
+
+__all__ = ['ComputationProfiler', 'DDPBucketProfiler', 'IIDPTrainerHelper', 'DDPHelper']
\ No newline at end of file
diff --git a/torch/iidp/profiler/api/__init__.py b/torch/iidp/profiler/api/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/profiler/api/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/profiler/api/memory_profile_data_summary.py b/torch/iidp/profiler/api/memory_profile_data_summary.py
new file mode 100644
index 00000000000..c4c2bc32d7d
--- /dev/null
+++ b/torch/iidp/profiler/api/memory_profile_data_summary.py
@@ -0,0 +1,64 @@
+import argparse
+import os
+
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.profiler import MAX_MEM_PROFILE_FILE_NAME
+
+
+parser = argparse.ArgumentParser(description='Memory Profile Data Summary API')
+parser.add_argument('--profile-dir', '-d', type=str, default=None,
+                    help='Directory of profile data file.')
+parser.add_argument('--config-file', '-c', type=str, default=None,
+                    help='Configuration file path (json) - Deprecated')
+
+
+def main():
+    args = parser.parse_args()
+
+    if args.config_file is not None and args.profile_dir is not None:
+        raise ValueError(f'Not support both setup of --config-file (-c) and --profile-dir (-d)')
+    if args.config_file is None and args.profile_dir is None:
+        raise ValueError(f'One of --config-file (-c) and --profile-dir (-d) must be setup')
+
+    summary_str = ''
+    col_str = '---------------------------------------------------'
+
+    if args.config_file is not None:
+        print(f'[WARNING][Memory profile data summary] Argument --config-file is deprecated.')
+        config_params = read_json(args.config_file)
+        gpu_cluster_info = read_json(config_params["gpu_cluster_info"])
+        memory_profile_dir = config_params['memory_profile_dir']
+
+        for lbs in sorted(os.listdir(memory_profile_dir), key=lambda x: int(x)):
+            static_lbs_profile_dir = os.path.join(memory_profile_dir, lbs)
+            for server_name in os.listdir(static_lbs_profile_dir):
+                gpu_type = gpu_cluster_info[server_name]['type']
+                max_memory_profile_file = os.path.join(
+                    static_lbs_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+                memory_profile_json_data = read_json(max_memory_profile_file)
+                max_num_models = memory_profile_json_data['max_num_models']
+                #summary_str += f'LBS: {lbs} | GPU: {gpu_type} | Max number of VSWs: {max_num_models} \n'
+                summary_str += f'   {lbs}\t|\t{gpu_type}\t|    {max_num_models} \n'
+            summary_str += col_str+'\n'
+    else:
+        memory_profile_dir = args.profile_dir
+        for lbs in sorted(os.listdir(memory_profile_dir), key=lambda x: int(x)):
+            static_lbs_profile_dir = os.path.join(memory_profile_dir, lbs)
+            for server_name in os.listdir(static_lbs_profile_dir):
+                max_memory_profile_file = os.path.join(
+                    static_lbs_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+                memory_profile_json_data = read_json(max_memory_profile_file)
+                max_num_models = memory_profile_json_data['max_num_models']
+                gpu_type = memory_profile_json_data['gpu_type']
+                #summary_str += f'LBS: {lbs} | GPU: {gpu_type} | Max number of VSWs: {max_num_models} \n'
+                summary_str += f'   {lbs}\t|\t{gpu_type}\t|    {max_num_models} \n'
+            summary_str += col_str+'\n'
+
+    print('======== Memory Profile Data Summary ========')
+    print('   LBS\t|\tGPU\t|\tMax number of VSWs')
+    print(col_str)
+    print(summary_str)
+
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
diff --git a/torch/iidp/profiler/profiler.py b/torch/iidp/profiler/profiler.py
new file mode 100644
index 00000000000..66d168d86a3
--- /dev/null
+++ b/torch/iidp/profiler/profiler.py
@@ -0,0 +1,827 @@
+import os
+import json
+import socket
+import gc
+import time
+import asyncio
+from contextlib import contextmanager
+
+import matplotlib.pyplot as plt
+
+import torch
+import torch.distributed as dist
+
+from torch.iidp.utils.json_utils import read_json, write_json
+from torch.iidp.config.examples.config_utils import MODEL_TITLE_MAP
+
+MAX_MEM_PROFILE_FILE_NAME = 'max_memory_profile_info.json'
+
+# NOTE: Not change import order to avoid import module error
+from torch.iidp.profiler.profiler_utils import nvidia_smi_memory_monitoring, async_run_command
+
+
+class AverageMeter(object):
+    """Computes and stores the average and current value"""
+    def __init__(self, name, fmt=':f'):
+        self.name = name
+        self.fmt = fmt
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
+
+    def __str__(self):
+        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
+        return fmtstr.format(**self.__dict__)
+
+
+class CompProfileData(object):
+    def __init__(self):
+        # NOTE: If new member variable is added, ProfileJSONData().update() should be updated
+        self.avg_total_time = AverageMeter('Total', ':6.3f')
+        self.avg_fwd_time = AverageMeter('Fwd', ':6.3f')
+        self.avg_bwd_time = AverageMeter('Bwd', ':6.3f')
+        self.avg_update_time = AverageMeter('Update', ':6.3f')
+        self.avg_copy_time = AverageMeter('Copy', ':6.3f')
+
+    def __str__(self):
+        return f'[Profile time (ms)] {self.avg_fwd_time} | {self.avg_bwd_time} | {self.avg_update_time} | {self.avg_copy_time} | {self.avg_total_time}'
+
+    def update(self, fwd_time, bwd_time, update_time, copy_time, total_time):
+        self.avg_fwd_time.update(fwd_time)
+        self.avg_bwd_time.update(bwd_time)
+        self.avg_update_time.update(update_time)
+        self.avg_copy_time.update(copy_time)
+        self.avg_total_time.update(total_time)
+
+
+class IIDPTrainerHelper(object):
+    def __init__(self, lbs, num_models):
+        """
+        The below member variables are required for ComputationProfiler
+            ```model_name```
+            ```lbs```
+            ```num_models```
+            ```profile_data```
+        """
+        self.gpu = 0
+        self.model_name = None
+        self.lbs = lbs
+        self.num_models = num_models
+        self.accum_step = 1
+        self.weight_sync_method = 'sequential'
+
+        self.trainer = None
+        self.model = None
+        self.criterion = None
+        self.optimizer = None
+        self.param_groups_func = None
+        self.warmup_step = 10
+        self.num_minibatches = 90
+
+        # Profile Data
+        self.profile_data = CompProfileData()
+
+    def set_optimizer(self):
+        raise NotImplementedError
+
+    def prepare(self):
+        torch.manual_seed(31415)
+        self.trainer = torch.iidp.IIDPTrainer(
+            self.gpu, self.lbs, self.num_models, self.accum_step, self.weight_sync_method)
+        self.trainer.prepare_stream_parallel(self.model, self.criterion, no_local_aggregation=True)
+        self.set_optimizer()
+        self.trainer.prepare_weight_sync_method(self.optimizer, None, self.param_groups_func)
+
+    def run(self):
+        raise NotImplementedError
+
+
+class ProfileJSONData(object):
+    def __init__(self, model_name, gpu_type, lbs, num_models):
+        self.dict = {
+            'model': model_name,
+            'gpu_type': gpu_type,
+            'lbs': lbs,
+            'num_models': num_models
+        }
+
+    def update(self, runtime_profile_data):
+        self.dict.update({
+            'total_time': runtime_profile_data.avg_total_time.avg,
+            'fwd_time': runtime_profile_data.avg_fwd_time.avg,
+            'bwd_time': runtime_profile_data.avg_bwd_time.avg,
+            'update_time': runtime_profile_data.avg_update_time.avg,
+            'copy_time': runtime_profile_data.avg_copy_time.avg,
+        })
+
+
+class ComputationProfiler(object):
+    def __init__(self, profiler_instance, profile_dir=None, plot_dir=None):
+        torch.cuda.set_device(0)
+        torch.cuda.empty_cache()
+
+        if profiler_instance is None:
+            raise ValueError('Argument profiler_instance must be configured.')
+        self.profiler_instance = profiler_instance
+        if not hasattr(self.profiler_instance, 'model_name') or \
+            not hasattr(self.profiler_instance, 'lbs') or \
+                not hasattr(self.profiler_instance, 'num_models') or \
+                    not hasattr(self.profiler_instance, 'profile_data'):
+            raise AttributeError(f'[{self.__class__.__name__}] {self.profiler_instance.__dict__}')
+        if not isinstance(self.profiler_instance.profile_data, CompProfileData):
+            raise TypeError(f'[{self.__class__.__name__}] '
+                            f'Type of self.profiler_instance.profile_data must be CompProfileData, '
+                            'but {type(self.profiler_instance.profile_data)}')
+        if self.profiler_instance.model_name is None or self.profiler_instance.lbs is None or \
+            self.profiler_instance.num_models is None:
+                raise ValueError(f'[{self.__class__.__name__}] '
+                                 f'model_name: {self.profiler_instance.model_name} | '
+                                 f'lbs: {self.profiler_instance.lbs} | '
+                                 f'num_models: {self.profiler_instance.num_models}')
+        self.model_name = self.profiler_instance.model_name
+        self.lbs = self.profiler_instance.lbs
+        self.num_models = self.profiler_instance.num_models
+        self.accum_step = 1
+        self.weight_sync_method = 'sequential'
+
+        self.hostname = socket.gethostname()
+        self.gpu_type = torch.cuda.get_device_name()
+        self.profile_dir = profile_dir
+        self.plot_dir = plot_dir
+
+        self.profile_json_data = ProfileJSONData(
+            self.model_name, self.gpu_type, self.lbs, self.num_models)
+
+    def record_profile_data(self):
+        self.profile_dir = os.path.join(
+                self.profile_dir, self.model_name, str(self.lbs), self.hostname)
+        os.makedirs(self.profile_dir, exist_ok=True)
+        json_file = os.path.join(
+            self.profile_dir,
+            f'{self.hostname}_{self.model_name}_{self.lbs}_{self.num_models}_comp_profile.json'
+        )
+        try:
+            with open(json_file, 'w') as jf:
+                json_str = json.dumps(self.profile_json_data.dict)
+                jf.write(json_str)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+
+        # Test to confirm write json object to file
+        json_data = read_json(json_file)
+        print(json_data)
+
+    def run(self):
+        self.profiler_instance.run()
+        self.profile_json_data.update(self.profiler_instance.profile_data)
+        if self.profile_dir:
+            self.record_profile_data()
+        if self.plot_dir:
+            self.plot_comp_profile_data()
+
+    def plot_comp_profile_data(self, file_path='comp_profile_data_breakdown.png'):
+        try:
+            model_name_for_plot = MODEL_TITLE_MAP[self.model_name]
+        except:
+            print(f'[WARNING] Model name is not registerd: {self.model}')
+            model_name_for_plot = self.model_name
+        all_data = [self.profile_json_data.dict]
+        x_data = []
+        fwd_time = []
+        bwd_time = []
+        update_time = []
+        copy_time = []
+        for data in all_data:
+            x_data.append(str(data['num_models']))
+            fwd_time.append(data['fwd_time']/data['total_time'])
+            bwd_time.append(data['bwd_time']/data['total_time'])
+            update_time.append(data['update_time']/data['total_time'])
+            copy_time.append(data['copy_time']/data['total_time'])
+        breakdown_data = [fwd_time, bwd_time, update_time, copy_time]
+        plt.clf()
+        stacked_data = [0 for _ in range(len(x_data))]
+        labels = ['Forward', 'Backward', 'Update', 'Copy']
+        for i, data in enumerate(breakdown_data):
+            plt.bar(x_data, data, bottom=stacked_data, label=labels[i], width=0.5)
+            stacked_data = [prev + data for prev, data in zip(stacked_data, data)]
+        plt.xlabel('Number of VSWs')
+        plt.ylabel('Normalized throughput breakdown')
+        plt.legend()
+        title = f'{model_name_for_plot} ({self.lbs}) on {self.gpu_type}'
+        plt.title(title)
+        file_path = f'{socket.gethostname()}_{self.model_name}_{self.lbs}_{self.num_models}_{file_path}'
+        os.makedirs(self.plot_dir, exist_ok=True)
+        file_path = os.path.join(self.plot_dir, file_path)
+        plt.savefig(file_path)
+
+
+class DDPHelper(object):
+    def __init__(self):
+        """
+        The below member variables are required for DDPBucketProfiler
+            ```model_name```
+        """
+        self.gpu = 0
+        self.model_name = None
+        self.ddp_module = None
+        self.lbs = 1
+        self.model = None
+        self.criterion = None
+        self.step = 2
+        self.bucket_size_distribution = []
+
+    def _get_ddp_bucket_indices(self):
+        raise NotImplementedError
+
+    def get_bucket_size_distribution(self):
+        self._get_ddp_bucket_indices()
+        if self.ddp_module is None:
+            raise TypeError(
+                f'[ERROR][{self.__class__.__name__}] Member variable ddp_module is None')
+        print(self.ddp_module.bucket_indices)
+        bucket_size_distribution = []
+        parameter_size_distribution = []
+        for _, param in enumerate(self.ddp_module.ddp_register_params):
+            if hasattr(param, 'index'):
+                param_mem_value = round(param.nelement() * param.element_size() / (1024 ** 2), 2)
+                parameter_size_distribution.append(param_mem_value)
+
+        for bucket in self.ddp_module.bucket_indices:
+            bucket_size = 0
+            for param_index in bucket:
+                param_size = parameter_size_distribution[param_index]
+                bucket_size += param_size
+            bucket_size_distribution.append(round(bucket_size, 2))
+        print(f'[Profile info] bucket_size_distribution (backward order): {bucket_size_distribution}')
+        self.bucket_size_distribution = bucket_size_distribution
+
+    def run(self):
+        raise NotImplementedError
+
+
+class DDPBucketProfiler(object):
+    def __init__(self, profiler_instance, profile_dir=None, plot_dir=None):
+        torch.cuda.empty_cache()
+        if not dist.is_initialized():
+            torch.cuda.set_device(0)
+            dist.init_process_group(
+                backend='nccl', init_method='tcp://127.0.0.1:22222', world_size=1, rank=0)
+
+        if profiler_instance is None:
+            raise ValueError('Argument profiler_instance must be configured.')
+        self.profiler_instance = profiler_instance
+        self.model_name = self.profiler_instance.model_name
+
+        self.profile_dir = profile_dir
+        self.plot_dir = plot_dir
+
+        self.profile_data = {
+            'model': self.model_name,
+            'bucket_size_distribution': []
+        }
+
+    def run(self):
+        self.profiler_instance.run()
+        self.profile_data['bucket_size_distribution'] = self.profiler_instance.bucket_size_distribution
+        if dist.get_rank() == 0:
+            if self.profile_dir:
+                self.record_profile_data()
+            if self.plot_dir:
+                self.plot_profile_data()
+
+    def record_profile_data(self):
+        os.makedirs(self.profile_dir, exist_ok=True)
+        json_file = os.path.join(
+            self.profile_dir,
+            f'{self.model_name}_bucket_size_profile.json'
+        )
+        try:
+            with open(json_file, 'w') as jf:
+                json_str = json.dumps(self.profile_data)
+                jf.write(json_str)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+
+        # Test to confirm write json object to file
+        json_data = read_json(json_file)
+        print(json_data)
+
+    def plot_profile_data(self, file_path='bucket_size_distribution.png'):
+        try:
+            model_title = MODEL_TITLE_MAP[self.model_name]
+        except:
+            print(f'[WARNING] Model name is not registerd: {self.model_name}')
+            model_title = self.model_name
+        plt.clf()
+        x = list(range(len(self.profile_data['bucket_size_distribution'])))
+        plt.bar(x, self.profile_data['bucket_size_distribution'])
+
+        plt.xlabel('Bucket order (backward)')
+        plt.ylabel('Bucket size (MB)')
+
+        title = f'{model_title}'
+        plt.title(title)
+
+        os.makedirs(self.plot_dir, exist_ok=True)
+        file_path = f'{self.model_name}_{file_path}'
+        file_path = os.path.join(self.plot_dir, file_path)
+        plt.savefig(file_path)
+
+
+class MemoryProfileJSONData(object):
+    def __init__(self, gpu_type, total_memory):
+        self.dict = {
+            'gpu_type': gpu_type,
+            'total_memory': total_memory
+        }
+
+    def update(self, runtime_profile_data):
+        self.dict.update(runtime_profile_data)
+
+
+class IIDPMemoryProfilerHelper(object):
+    def __init__(self, lbs, num_models):
+        self.gpu = 0
+        self.lbs = lbs
+        self.num_models = num_models
+        self.accum_step = 1
+        self.weight_sync_method = 'sequential'
+
+        self.trainer = None
+        self.model = None
+        self.criterion = None
+        self.optimizer = None
+        self.param_groups_func = None
+        self.warmup_step = 10
+        self.num_minibatches = 90
+        # For debugging
+        #self.warmup_step = 5
+        #self.num_minibatches = 10
+
+        # Profile Data
+        # TODO: Update strong data format (potential to have different format defined by users)
+        self.profile_data = {}
+
+    def set_optimizer(self):
+        raise NotImplementedError
+
+    def prepare(self):
+
+        torch.manual_seed(31415)
+        self.trainer = torch.iidp.IIDPTrainer(
+            self.gpu, self.lbs, self.num_models, self.accum_step, self.weight_sync_method)
+        self.trainer.prepare_stream_parallel(self.model, self.criterion, no_local_aggregation=True)
+        self.set_optimizer()
+        self.trainer.prepare_weight_sync_method(self.optimizer, None, self.param_groups_func)
+
+    def run(self):
+        raise NotImplementedError
+
+
+class BaseSingleGPUMemoryProfiler(object):
+    def __init__(self, profiler_class, profile_dir=None):
+        if profiler_class is None:
+            raise ValueError('Argument profiler_class must be configured.')
+        torch.cuda.set_device(0)
+        torch.cuda.empty_cache()
+        self.profiler_class = profiler_class
+        # NOTE: If profile_dir is None, not record profile data to JSON file
+        self.profile_dir = profile_dir
+
+        self.model_name = '' # defined in run() method
+        self.accum_step = 1
+        self.weight_sync_method = 'sequential'
+
+        self.hostname = socket.gethostname()
+        self.gpu_type = torch.cuda.get_device_name()
+        self.total_gpu_memory = torch.cuda.get_device_properties(0).total_memory
+
+        self.profile_json_data = MemoryProfileJSONData(
+            self.gpu_type, self.total_gpu_memory
+        )
+        self.max_mem_profile_json_data = MemoryProfileJSONData(
+            self.gpu_type, self.total_gpu_memory
+        )
+
+    def log(self, message, status='info'):
+        print_msg = f'[{status.upper()}][{self.__class__.__name__}] {message}'
+        print(print_msg)
+
+    def record_max_mem_profile_data(self, lbs):
+        lbs = str(lbs)
+        profile_dir = os.path.join(
+                self.profile_dir, self.model_name, lbs, self.hostname)
+        os.makedirs(profile_dir, exist_ok=True)
+        json_file = os.path.join(
+            profile_dir,
+            MAX_MEM_PROFILE_FILE_NAME
+        )
+        try:
+            with open(json_file, 'w') as jf:
+                json_str = json.dumps(self.max_mem_profile_json_data.dict)
+                jf.write(json_str)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+
+        # Test to confirm write json object to file
+        json_data = read_json(json_file)
+        print(json_data)
+
+    def record_profile_data(self, lbs, num_models):
+        lbs = str(lbs)
+        profile_dir = os.path.join(
+                self.profile_dir, self.model_name, lbs, self.hostname)
+        os.makedirs(profile_dir, exist_ok=True)
+        json_file = os.path.join(
+            profile_dir,
+            f'{self.hostname}_{self.model_name}_{lbs}_{num_models}_mem_profile.json'
+        )
+        try:
+            with open(json_file, 'w') as jf:
+                json_str = json.dumps(self.profile_json_data.dict)
+                jf.write(json_str)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+
+        # Test to confirm write json object to file
+        json_data = read_json(json_file)
+        print(json_data)
+
+    def run(self):
+        raise NotImplementedError
+
+
+class StaticLocalBatchSizMemoryProfiler(BaseSingleGPUMemoryProfiler):
+    def __init__(self, profiler_class, local_batch_size, profile_dir=None):
+        super().__init__(profiler_class, profile_dir)
+        if local_batch_size is None:
+            raise ValueError('Argument local_batch_size must be configured.')
+        if not isinstance(local_batch_size, int):
+            raise ValueError(
+                f'Argument local_batch_size must be integer type, '
+                f'but {type(local_batch_size)}')
+        self.lbs = local_batch_size
+
+    def run(self):
+        max_num_models_on_hardware = min(os.cpu_count() // torch.cuda.device_count(), 10)
+        self.log(f'Max number of models that this GPU server can run: '
+                 f'{max_num_models_on_hardware} | '
+                 f'CPU count: {os.cpu_count()} | GPU count: {torch.cuda.device_count()}')
+
+        is_oom_by_num_models = False
+        is_oom_by_lbs = False
+        while True:
+            for num_models in range(1, max_num_models_on_hardware+1):
+                try:
+                    gc.collect()
+                    with torch.no_grad():
+                        torch.cuda.empty_cache()
+                    self.log(f'Profiling with LBS: {self.lbs} | num models: {num_models} .. ')
+                    profiler_instance = self.profiler_class(self.lbs, num_models)
+                    profiler_instance.run()
+                    self.model_name = profiler_instance.model_name
+                    self.profile_json_data.update(profiler_instance.profile_data)
+                    del profiler_instance
+                    if self.profile_dir:
+                        self.record_profile_data(self.lbs, num_models)
+                except RuntimeError as e:
+                    del profiler_instance
+                    self.log(f'OOM error by LBS: {self.lbs} | num models: {num_models}')
+                    max_num_models = num_models - 1
+                    if max_num_models == 0:
+                        is_oom_by_lbs = True
+                        break
+                    is_oom_by_num_models = True
+                    self.max_mem_profile_json_data.update({'lbs': self.lbs, 'max_num_models': max_num_models})
+                    if self.profile_dir:
+                        self.record_max_mem_profile_data(self.lbs)
+                    break
+            if is_oom_by_lbs is True: # Reach max local batch size
+                break
+            if is_oom_by_num_models is True:
+                break
+            if is_oom_by_num_models is False: # With LBS, it can run with max_num_models_on_hardware
+                self.max_mem_profile_json_data.update({'lbs': self.lbs, 'max_num_models': max_num_models_on_hardware})
+                if self.profile_dir:
+                    self.record_max_mem_profile_data(self.lbs)
+                break
+
+
+class DynamicLocalBatchSizeMemoryProfiler(BaseSingleGPUMemoryProfiler):
+    def __init__(self, profiler_class, min_lbs, max_lbs=None,
+                 profile_dir=None, search_lbs_fn=None):
+        super().__init__(profiler_class, profile_dir)
+        if min_lbs is None:
+            raise ValueError('Argument min_lbs must be configured.')
+        if not isinstance(min_lbs, int):
+            raise ValueError(f'Argument min_lbs must be integer type, but {type(min_lbs)}')
+        if search_lbs_fn is None:
+            raise ValueError(
+                f'Argumnet search_lbs_fn must be configured')
+
+        self.min_batch_size = min_lbs
+        self.max_batch_size = max_lbs
+        self.search_lbs_fn = search_lbs_fn
+
+    def run(self):
+        max_num_models_on_hardware = min(os.cpu_count() // torch.cuda.device_count(), 10)
+        self.log(f'Max number of models that this GPU server can run: '
+                 f'{max_num_models_on_hardware} | '
+                 f'CPU count: {os.cpu_count()} | GPU count: {torch.cuda.device_count()}')
+
+        local_batch_size = self.min_batch_size
+        is_oom_by_num_models = False
+        is_oom_by_lbs = False
+        while True:
+            if self.max_batch_size is not None and local_batch_size > self.max_batch_size:
+                break
+
+            for num_models in range(1, max_num_models_on_hardware+1):
+                profiler_instance = None
+                try:
+                    gc.collect()
+                    with torch.no_grad():
+                        torch.cuda.empty_cache()
+                    self.log(f'Profiling with LBS: {local_batch_size} | num models: {num_models} .. ')
+                    self.log(f'cuda memory allocated: {torch.cuda.memory_allocated()}')
+                    assert torch.cuda.memory_allocated() == 0, \
+                        f"Before runnig profiler, CUDA allocated memory must be 0, " \
+                        f"but {round(torch.cuda.memory_allocated() / (1024*1024))} MB"
+                    profiler_instance = self.profiler_class(local_batch_size, num_models)
+                    profiler_instance.run()
+                    self.model_name = profiler_instance.model_name
+                    self.profile_json_data.update(profiler_instance.profile_data)
+                    del profiler_instance
+                    if self.profile_dir:
+                        self.record_profile_data(local_batch_size, num_models)
+                except RuntimeError as e:
+                    self.log(f'OOM error by LBS: {local_batch_size} | num models: {num_models}')
+                    torch.cuda.synchronize()
+                    # TODO: Find the way to guarantee all theads in trainer.profile_parallel_compute()
+                    # finish join() here.
+                    time.sleep(10)
+                    del profiler_instance
+                    gc.collect()
+                    with torch.no_grad():
+                        torch.cuda.empty_cache()
+                    max_num_models = num_models - 1
+                    if max_num_models == 0:
+                        is_oom_by_lbs = True
+                        break
+                    is_oom_by_num_models = True
+                    self.max_mem_profile_json_data.update({'lbs': local_batch_size, 'max_num_models': max_num_models})
+                    if self.profile_dir:
+                        self.record_max_mem_profile_data(local_batch_size)
+                    break
+            if is_oom_by_lbs is True: # Reach max local batch size
+                break
+            if is_oom_by_num_models is False: # With LBS, it can run with max_num_models_on_hardware
+                self.max_mem_profile_json_data.update({'lbs': local_batch_size, 'max_num_models': max_num_models_on_hardware})
+                if self.profile_dir:
+                    self.record_max_mem_profile_data(local_batch_size)
+
+            local_batch_size = self.search_lbs_fn(local_batch_size)
+
+
+class BaseMultiGPUMemoryProfiler(object):
+    def __init__(self, profile_dir, model_name, training_scheme, shell_script,
+                 timeout=60, mem_util_threshold=100, user_defined_cmd_fn=None):
+        if training_scheme not in ['static', 'adaptive']:
+            raise ValueError(
+                f'Argument training_schemes must be configured among [static, adaptive]')
+        if not os.path.exists(shell_script):
+            raise ValueError(f'Argument shell_script must exist')
+
+        self.profile_dir = profile_dir
+        self.model_name = model_name
+
+        self.hostname = socket.gethostname()
+        self.gpu_type = torch.cuda.get_device_name()
+        self.total_gpu_memory = torch.cuda.get_device_properties(0).total_memory
+
+        self.max_mem_profile_json_data = MemoryProfileJSONData(
+            self.gpu_type, self.total_gpu_memory
+        )
+
+        self.training_scheme = training_scheme
+        self.shell_script = shell_script
+        self.timeout = timeout
+        self.user_defined_cmd_fn = user_defined_cmd_fn
+        self.mem_util_threshold = mem_util_threshold
+
+        self.max_num_models_on_hardware = min(os.cpu_count() // torch.cuda.device_count(), 10)
+        self.log(f'Max number of models that this GPU server can run: ' \
+                 f'{self.max_num_models_on_hardware} | ' \
+                 f'CPU count: {os.cpu_count()} | GPU count: {torch.cuda.device_count()}')
+        self.max_num_models = 1
+
+        self.loop = None
+
+    def log(self, message, status='info'):
+        print_msg = f'[{status.upper()}][{self.__class__.__name__}] {message}'
+        print(print_msg)
+
+    def record_max_mem_profile_data(self, lbs):
+        lbs = str(lbs)
+        profile_dir = os.path.join(
+                self.profile_dir, self.model_name, lbs, self.hostname)
+        os.makedirs(profile_dir, exist_ok=True)
+        json_file = os.path.join(
+            profile_dir,
+            MAX_MEM_PROFILE_FILE_NAME
+        )
+        try:
+            with open(json_file, 'w') as jf:
+                json_str = json.dumps(self.max_mem_profile_json_data.dict)
+                jf.write(json_str)
+        except IOError as e:
+            print("I/O error({0}): {1}".format(e.errno, e.strerror))
+            exit(1)
+
+        # Test to confirm write json object to file
+        json_data = read_json(json_file)
+        self.log(f'Record data: {json_data} to {json_file}')
+
+    def _prepare_for_training_scheme(self):
+        if self.training_scheme == 'static':
+            pass
+        elif self.training_scheme == 'adaptive':
+            # generate mock config json file - enable_adjust = False (defined in torch/iidp/trainer.py)
+            mock_adaptive_config_data = {
+                "metric": "similarity",
+                "enable_adjust": "False",
+                "batch_size_adjust_interval": 10
+            }
+            mock_config_dir = 'adaptive_config'
+            os.makedirs(mock_config_dir, exist_ok=True)
+            self.mock_adaptive_config_file = os.path.join(
+                mock_config_dir, 'adaptive_config_for_mem_profile_validation.json')
+            write_json(self.mock_adaptive_config_file, mock_adaptive_config_data)
+        else:
+            raise ValueError(f'Not support such trainin scheme: {self.training_scheme}')
+
+    @contextmanager
+    def execute_handler(self):
+        try:
+            yield
+        finally:
+            if self.training_scheme == 'adaptive':
+                self.log(f'Remove mock config file for adaptive training: {self.mock_adaptive_config_file}')
+                os.system(f'rm -rf {self.mock_adaptive_config_file}')
+
+    def _execute(self):
+        lbs_str = str(self.local_batch_size)
+        while True:
+            try:
+                rank = 0
+                world_size = 1
+                weight_sync_method = 'recommend'
+                if self.training_scheme == 'static':
+                    accum_step = 1
+                    if self.user_defined_cmd_fn is not None:
+                        command = self.user_defined_cmd_fn(
+                            self.shell_script, rank, world_size, lbs_str,
+                            self.max_num_models, accum_step, weight_sync_method
+                        )
+                    else:
+                        command = [
+                            self.shell_script, rank, world_size, lbs_str,
+                            self.max_num_models, accum_step, weight_sync_method
+                        ]
+                elif self.training_scheme == 'adaptive':
+                    accum_step = 2
+                    if self.user_defined_cmd_fn is not None:
+                        command = self.user_defined_cmd_fn(
+                            self.shell_script, rank, world_size, lbs_str,
+                            self.max_num_models, accum_step, weight_sync_method,
+                            self.mock_adaptive_config_file
+                        )
+                    else:
+                        command = [
+                            self.shell_script, rank, world_size, lbs_str,
+                            self.max_num_models, accum_step, weight_sync_method,
+                            self.mock_adaptive_config_file
+                        ]
+                log_str = f'Start to profile number of VSWs ({lbs_str}): {self.max_num_models}'
+                col_str = '=' * (len(log_str) + 1)
+                self.log(col_str)
+                self.log(log_str)
+                self.log(col_str)
+                with nvidia_smi_memory_monitoring(self.max_num_models, self.mem_util_threshold):
+                    self.loop = asyncio.get_event_loop()
+                    self.loop.run_until_complete(async_run_command(command, self.timeout))
+                self.log('Success to execute command')
+                self.log('Sleep 30 sec ..')
+                time.sleep(30)
+                self.max_num_models += 1
+            except RuntimeError as e: # OOM happen
+                self.log(e)
+                kill_python_cmd = "kill -9 `ps | grep python | grep -v {0} | grep -v defunct | awk -F' ' '{{print $1}}'`".format(os.getpid())
+                os.system(kill_python_cmd)
+                kill_nvidiasmi_query_cmd = \
+                    f"kill -9 `ps -ef | grep -v grep | grep \"nvidia-smi --query\" | awk '{{print $2}}' `"
+                os.system(kill_nvidiasmi_query_cmd)
+                self.log('Sleep 30 sec ..')
+                time.sleep(30)
+                self.max_num_models -= 1
+                break
+
+            if self.max_num_models >= self.max_num_models_on_hardware:
+                self.log('Terminate to reach max number of VSWs to max constaint on hardware')
+                break
+
+    def run(self):
+        raise NotImplementedError
+
+
+class StaticLocalBatchSizeMultiGPUMemoryProfiler(BaseMultiGPUMemoryProfiler):
+    def __init__(self, profile_dir, model_name, training_scheme, shell_script, local_batch_size,
+                 timeout=60, mem_util_threshold=100, user_defined_cmd_fn=None):
+        super().__init__(profile_dir, model_name, training_scheme, shell_script,
+                         timeout, mem_util_threshold, user_defined_cmd_fn)
+        if local_batch_size is None:
+            raise ValueError('Argument local_batch_size must be configured.')
+        if not isinstance(local_batch_size, int):
+            raise ValueError(
+                f'Argument local_batch_size must be integer type, '
+                f'but {type(local_batch_size)}')
+
+        self.local_batch_size = str(local_batch_size)
+
+        self._prepare_for_training_scheme()
+
+    def run(self):
+        with self.execute_handler():
+            self._execute()
+            # Get max num models
+            self.log('========================================================')
+            self.log(f'Profiled max number of VSWs ({self.local_batch_size}): {self.max_num_models}')
+            self.log('========================================================')
+            if self.max_num_models >= 1:
+                self.max_mem_profile_json_data.update(
+                    {'lbs': self.local_batch_size, 'max_num_models': self.max_num_models}
+                )
+                self.record_max_mem_profile_data(self.local_batch_size)
+            self.log('Sleep 30 sec ..')
+            time.sleep(30)
+
+        self.loop.close()
+
+
+class DynamicLocalBatchSizeMultiGPUMemoryProfiler(BaseMultiGPUMemoryProfiler):
+    def __init__(self, profile_dir, model_name, training_scheme, shell_script,
+                 min_lbs, search_lbs_fn, max_lbs=None,
+                 timeout=60, mem_util_threshold=100, user_defined_cmd_fn=None):
+        super().__init__(profile_dir, model_name, training_scheme, shell_script,
+                         timeout, mem_util_threshold, user_defined_cmd_fn)
+        if min_lbs is None:
+            raise ValueError('Argument min_lbs must be configured.')
+        if not isinstance(min_lbs, int):
+            raise ValueError(f'Argument min_lbs must be integer type, but {type(min_lbs)}')
+        if search_lbs_fn is None:
+            raise ValueError(
+                f'Argumnet search_lbs_fn must be configured')
+
+        self.min_batch_size = min_lbs
+        self.max_batch_size = max_lbs
+        self.search_lbs_fn = search_lbs_fn
+
+        self.local_batch_size = self.min_batch_size
+
+        self._prepare_for_training_scheme()
+
+    def run(self):
+        with self.execute_handler():
+            while True:
+                if self.max_batch_size is not None and self.local_batch_size > self.max_batch_size:
+                    break
+                self._execute()
+                # Get max num models
+                self.log('========================================================')
+                self.log(f'Profiled max number of VSWs ({self.local_batch_size}): {self.max_num_models}')
+                self.log('========================================================')
+                if self.max_num_models >= 1:
+                    self.max_mem_profile_json_data.update(
+                        {'lbs': self.local_batch_size, 'max_num_models': self.max_num_models}
+                    )
+                    self.record_max_mem_profile_data(self.local_batch_size)
+                else:
+                    break
+                self.log('Sleep 30 sec ..')
+                time.sleep(30)
+                self.local_batch_size = self.search_lbs_fn(self.local_batch_size)
+                self.max_num_models = 1
\ No newline at end of file
diff --git a/torch/iidp/profiler/profiler_utils.py b/torch/iidp/profiler/profiler_utils.py
new file mode 100644
index 00000000000..9dc5e7950ab
--- /dev/null
+++ b/torch/iidp/profiler/profiler_utils.py
@@ -0,0 +1,443 @@
+import os
+import socket
+import time
+import subprocess
+import sys
+sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1)
+
+import asyncio
+from asyncio.subprocess import PIPE, STDOUT
+
+from contextlib import contextmanager
+
+from torch.iidp.utils.json_utils import read_json, write_json
+from torch.iidp.profiler.profiler import MAX_MEM_PROFILE_FILE_NAME
+
+
+def get_mem_profile_data_summary(profile_dir):
+    summary_str = ''
+    col_str = '---------------------------------------------------'
+    for lbs in sorted(os.listdir(profile_dir), key=lambda x: int(x)):
+        static_lbs_profile_dir = os.path.join(profile_dir, lbs)
+        for server_name in os.listdir(static_lbs_profile_dir):
+            max_memory_profile_file = os.path.join(
+                static_lbs_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+            memory_profile_json_data = read_json(max_memory_profile_file)
+            max_num_models = memory_profile_json_data['max_num_models']
+            gpu_type = memory_profile_json_data['gpu_type']
+            #summary_str += f'LBS: {lbs} | GPU: {gpu_type} | Max number of VSWs: {max_num_models} \n'
+            summary_str += f'   {lbs}\t|\t{gpu_type}\t|    {max_num_models} \n'
+        summary_str += col_str+'\n'
+    return summary_str
+
+
+def get_max_profile_json_data(profile_dir, lbs):
+    memory_profile_json_data, max_memory_profile_file = None, None
+    static_lbs_profile_dir = os.path.join(profile_dir, lbs)
+    for server_name in os.listdir(static_lbs_profile_dir):
+        if socket.gethostname() != server_name:
+            continue
+        max_memory_profile_file = os.path.join(
+            static_lbs_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+        memory_profile_json_data = read_json(max_memory_profile_file)
+    if max_memory_profile_file is None:
+        current_server_max_memory_profile_file = os.path.join(
+            static_lbs_profile_dir, socket.gethostname(), MAX_MEM_PROFILE_FILE_NAME)
+        raise ValueError(f'No such memory profile dir: {current_server_max_memory_profile_file}')
+    if memory_profile_json_data is None:
+        raise ValueError('return value memory_profile_json_data is None')
+    return memory_profile_json_data
+
+
+def set_max_profile_json_data(profile_dir, lbs, data):
+    static_lbs_profile_dir = os.path.join(profile_dir, lbs)
+    max_memory_profile_file = None
+    for server_name in os.listdir(static_lbs_profile_dir):
+        if socket.gethostname() != server_name:
+            continue
+        max_memory_profile_file = os.path.join(
+            static_lbs_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+    if max_memory_profile_file is None:
+        raise ValueError('max_memory_profile_file is None')
+    write_json(max_memory_profile_file, data)
+
+
+def get_max_num_models_for_static_lbs(profile_dir, lbs):
+    memory_profile_json_data = get_max_profile_json_data(profile_dir, lbs)
+    profiled_max_num_models = memory_profile_json_data['max_num_models']
+    return profiled_max_num_models
+
+
+# reference: Second solution in https://stackoverflow.com/questions/10756383/timeout-on-subprocess-readline-in-python
+async def async_run_command(command, timeout=60):
+    command = [str(arg) for arg in command]
+    program = command[0]
+    args = command[1:]
+    print(f'[INFO] command: {" ".join(command)}')
+    print(f'[INFO] It might take time .. please wait ..')
+    proc = await asyncio.create_subprocess_exec(program, *args, stdout=PIPE, stderr=STDOUT)
+    while True:
+        try:
+            line = await asyncio.wait_for(proc.stdout.readline(), timeout)
+        except asyncio.TimeoutError:
+            pass
+        else:
+            if not line: # EOF
+                break
+            else:
+                print(line.decode('utf-8').replace('\n',''))
+                log_str = line.decode('utf-8')
+                if 'out of memory' in log_str or 'RuntimeError' in log_str:
+                    proc.kill()
+                    raise RuntimeError('CUDA out of memory error')
+                continue
+        proc.kill() # Timeout or some criterion is not satisfied
+        raise RuntimeError('TimeoutExpired - CUDA out of memory error')
+    return await proc.wait() # Wait for the child process to exit
+
+
+class nvidia_smi_memory_monitoring(object):
+    def __init__(self, num_models, mem_util_threshold):
+        self.num_models = num_models
+        self.mem_util_threshold = mem_util_threshold
+        self.proc = None
+        if self.num_models > 1:
+            nvidiasmi_query_cmd = "nvidia-smi --query-gpu=memory.total,memory.used --format=csv -lms 100 &"
+            # NOTE: shell=True for background command
+            self.proc = subprocess.Popen(nvidiasmi_query_cmd,
+                                         stdout=subprocess.PIPE,
+                                         stderr=subprocess.STDOUT,
+                                         shell=True)
+
+    def __enter__(self):
+        pass
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        #print(f'[DEBUG][{self.__class__.__name__}] __exit__ => exc_type: {exc_type} | exc_val: {exc_val} | exc_tb: {exc_tb}')
+        is_timeout_error, is_mem_util_over = False, False
+        if self.num_models > 1:
+            kill_nvidiasmi_query_cmd = \
+                f"kill -9 `ps -ef | grep -v grep | grep \"nvidia-smi --query\" | awk '{{print $2}}' `"
+            os.system(kill_nvidiasmi_query_cmd)
+            # NOTE: Safe to check max mem usage by TimeoutExpired error
+            if exc_type is RuntimeError:
+                if 'TimeoutExpired' in str(exc_val):
+                    is_timeout_error = True
+                    print(f'[INFO][{self.__class__.__name__}] '
+                          f'Safe to check max mem usage by TimeoutExpired error: {exc_val}')
+                else:
+                    return
+            memory_value_parsing_count = 0
+            max_mem_util, max_mem_used = 0, 0
+            for stdout in self.proc.stdout.readlines():
+                log_str = stdout.decode('utf-8').replace('\n','')
+                #print(f'[DEBUG] nvidia-smi memory query: {log_str}')
+                if 'memory' not in log_str and 'MiB' in log_str:
+                    if len(log_str.split()) != 4: # NOTE: To avoid some stdout that has only total memory size
+                        continue
+                    try:
+                        mem_total, mem_used = float(log_str.split()[0]), float(log_str.split()[2])
+                    except:
+                        print(f'[ERROR][{self.__class__.__name__}] '
+                              f'log_str: {log_str}\n '
+                              f'log_str.split(): {log_str.split()} | '
+                              f'log_str.split(): {log_str.split()}')
+                        exit(1)
+                    if memory_value_parsing_count == 0 and mem_used != 0:
+                        #assert mem_used == 0, f"The first parsed memory used must be zero, but {mem_used}"
+                        print(f'[WARNING][{self.__class__.__name__}] '
+                              f'The first parsed memory used must be zero, but {mem_used} | '
+                              f'Log: {log_str}')
+                    memory_value_parsing_count+=1
+                    mem_util = (mem_used / mem_total) * 100
+                    if mem_util >= self.mem_util_threshold:
+                        is_mem_util_over = True
+                        max_mem_util = mem_util
+                        max_mem_used = mem_used
+                        self.proc.kill()
+                        self.proc = None
+                        raise RuntimeError(
+                            f'[{self.__class__.__name__}] CUDA out of memory error - '
+                            f'Memory util: {mem_util:.2f}% > threshold: 99% | '
+                            f'{mem_used}MiB / {mem_total}MiB')
+                    else:
+                        if mem_util > max_mem_util:
+                            max_mem_util = mem_util
+                            max_mem_used = mem_used
+
+            if is_timeout_error is True and is_mem_util_over is False:
+                print(f'[WARNING][{self.__class__.__name__}] '
+                      f'TimeoutExpired error might not caused by Out of Memory error | '
+                      f'Max memory usage: {max_mem_used}MiB / {mem_total}MiB')
+
+            print(f'[INFO][{self.__class__.__name__}] '
+                  f'Max memory usage util: {max_mem_util:.2f}% | '
+                  f'{max_mem_used}MiB / {mem_total}MiB')
+
+
+class BaseMultiGPUMemoryProfileValidator(object):
+    def __init__(self, profile_dir, dest_profile_dir, training_scheme, shell_script,
+                 timeout=60, user_defined_cmd_fn=None, mem_util_threshold=100):
+        if not os.path.exists(profile_dir):
+            raise ValueError(f'Argument profile_dir must exist')
+        if training_scheme not in ['static', 'adaptive']:
+            raise ValueError(
+                f'Argument training_schemes must be configured among [static, adaptive]')
+        if not os.path.exists(shell_script):
+            raise ValueError(f'Argument shell_script must exist')
+
+        self.profile_dir = profile_dir
+        self.dest_profile_dir = dest_profile_dir
+        self.training_scheme = training_scheme
+        self.shell_script = shell_script
+        self.timeout = timeout
+        self.user_defined_cmd_fn = user_defined_cmd_fn
+        self.mem_util_threshold = mem_util_threshold
+
+    def log(self, message, status='info'):
+        print_msg = f'[{status.upper()}][{self.__class__.__name__}] {message}'
+        print(print_msg)
+
+    def _prepare_for_training_scheme(self):
+        if self.training_scheme == 'static':
+            pass
+        elif self.training_scheme == 'adaptive':
+            # generate mock config json file - enable_adjust = False (defined in torch/iidp/trainer.py)
+            mock_adaptive_config_data = {
+                "metric": "similarity",
+                "enable_adjust": "False",
+                "batch_size_adjust_interval": 10
+            }
+            self.mock_adaptive_config_file = 'adaptive_config/adaptive_config_for_mem_profile_validation.json'
+            write_json(self.mock_adaptive_config_file, mock_adaptive_config_data)
+        else:
+            raise ValueError(f'Not support such trainin scheme: {self.training_scheme}')
+
+    @contextmanager
+    def evaluate_handler(self):
+        try:
+            yield
+        finally:
+            if self.training_scheme == 'adaptive':
+                self.log(f'Remove mock config file for adaptive training: {self.mock_adaptive_config_file}')
+                os.system(f'rm -rf {self.mock_adaptive_config_file}')
+
+    def evaluate(self):
+        raise NotImplementedError
+
+
+class MemoryProfileValidator(BaseMultiGPUMemoryProfileValidator):
+    def __init__(self, profile_dir, dest_profile_dir, training_scheme, shell_script,
+                 timeout=60, user_defined_cmd_fn=None, mem_util_threshold=100):
+        super().__init__(profile_dir, dest_profile_dir, training_scheme, shell_script,
+                         timeout, user_defined_cmd_fn, mem_util_threshold)
+        try:
+            if len(os.listdir(self.profile_dir)) >= 1:
+                int(os.listdir(self.profile_dir)[0])
+        except ValueError:
+            raise ValueError(f"profile dir must contain local batch size sub-directory, "
+                             f"but: {os.listdir(self.profile_dir)}")
+
+        os.makedirs(dest_profile_dir)
+        os.system(f'cp -r {self.profile_dir} {dest_profile_dir}')
+        self.log('========================================================')
+        self.log(f'[Initialize] Copy {self.profile_dir} to {dest_profile_dir}')
+        self.log('========================================================')
+        # NOTE: profile dir must contain one sub-directory of model name
+        assert len(os.listdir(dest_profile_dir)) == 1
+        model_name = os.listdir(dest_profile_dir)[0]
+        self.dest_profile_dir = os.path.join(dest_profile_dir, model_name)
+
+        self._prepare_for_training_scheme()
+
+    def evaluate(self):
+        with self.evaluate_handler():
+            max_num_models_with_prev_lbs = -1
+            for lbs in sorted(os.listdir(self.profile_dir), key=lambda x: int(x)):
+                profiled_max_num_models = get_max_num_models_for_static_lbs(self.profile_dir, lbs)
+                if max_num_models_with_prev_lbs < 0:
+                    max_num_models = profiled_max_num_models
+                else:
+                    max_num_models = max_num_models_with_prev_lbs
+                while True:
+                    if max_num_models < 1:
+                        break
+                    try:
+                        rank = 0
+                        world_size = 1
+                        local_batch_size = lbs
+                        weight_sync_method = 'recommend'
+                        if self.training_scheme == 'static':
+                            accum_step = 1
+                            if self.user_defined_cmd_fn is not None:
+                                command = self.user_defined_cmd_fn(
+                                    self.shell_script, rank, world_size, local_batch_size,
+                                    max_num_models, accum_step, weight_sync_method
+                                )
+                            else:
+                                command = [
+                                    self.shell_script, rank, world_size, local_batch_size,
+                                    max_num_models, accum_step, weight_sync_method
+                                ]
+                        elif self.training_scheme == 'adaptive':
+                            accum_step = 2
+                            if self.user_defined_cmd_fn is not None:
+                                command = self.user_defined_cmd_fn(
+                                    self.shell_script, rank, world_size, local_batch_size,
+                                    max_num_models, accum_step, weight_sync_method,
+                                    self.mock_adaptive_config_file
+                                )
+                            else:
+                                command = [
+                                    self.shell_script, rank, world_size, local_batch_size,
+                                    max_num_models, accum_step, weight_sync_method,
+                                    self.mock_adaptive_config_file
+                                ]
+                        with nvidia_smi_memory_monitoring(max_num_models, self.mem_util_threshold):
+                            loop = asyncio.get_event_loop()
+                            loop.run_until_complete(async_run_command(command, self.timeout))
+                        self.log('Success to execute command')
+                        self.log('Sleep 30 sec ..')
+                        time.sleep(30)
+                        break
+                    except RuntimeError as e: # OOM happen
+                        self.log(e)
+                        kill_python_cmd = "kill -9 `ps | grep python | grep -v {0} | grep -v defunct | awk -F' ' '{{print $1}}'`".format(os.getpid())
+                        os.system(kill_python_cmd)
+                        kill_nvidiasmi_query_cmd = \
+                            f"kill -9 `ps -ef | grep -v grep | grep \"nvidia-smi --query\" | awk '{{print $2}}' `"
+                        os.system(kill_nvidiasmi_query_cmd)
+                        self.log('Sleep 30 sec ..')
+                        time.sleep(30)
+                        max_num_models -= 1
+                # Get real max num models
+                self.log('========================================================')
+                self.log(f'Profiled max number of VSWs ({lbs}): {profiled_max_num_models}')
+                self.log(f'Real max number of VSWs ({lbs}): {max_num_models}')
+                self.log('========================================================')
+                # Change max num models if it is different
+                if profiled_max_num_models != max_num_models:
+                    if max_num_models == 0:
+                        self.log(f'Remove profile data in {os.path.join(self.dest_profile_dir, lbs)}')
+                        os.system(f'rm -rf {os.path.join(self.dest_profile_dir, lbs)}')
+                    else:
+                        self.log(f'Update profile data in {os.path.join(self.dest_profile_dir, lbs)}')
+                        memory_profile_json_data = get_max_profile_json_data(self.dest_profile_dir, lbs)
+                        memory_profile_json_data['max_num_models'] = max_num_models
+                        set_max_profile_json_data(self.dest_profile_dir, lbs, memory_profile_json_data)
+                # Record current profiled max number of VSWs for the next local batch size
+                max_num_models_with_prev_lbs = max_num_models
+                self.log('Sleep 30 sec ..')
+                time.sleep(30)
+
+        loop.close()
+
+
+class StaticLocalBatchSizeMemoryProfileValidator(BaseMultiGPUMemoryProfileValidator):
+    def __init__(self, profile_dir, dest_profile_dir, training_scheme, shell_script,
+                 local_batch_size, timeout=60, user_defined_cmd_fn=None, mem_util_threshold=100):
+        super().__init__(profile_dir, dest_profile_dir, training_scheme, shell_script,
+                         timeout, user_defined_cmd_fn, mem_util_threshold)
+        # Handle both case of '{profile dir}/{model}' and '{profile dir}/{model}/'
+        if self.profile_dir[-1] == '/':
+            self.profile_dir = self.profile_dir[:-1]
+        # Check structure of profile dir is {profile dir}/{model}
+        assert len(self.profile_dir.split('/')) == 2, \
+            "[ERROR] Argument profile_dir must have {profile dir}/{model}, " \
+            f"but profile_dir: {self.profile_dir} | " \
+            f"len(profile_dir.split('/')): {len(self.profile_dir.split('/'))}"
+        model_name = self.profile_dir.split('/')[-1]
+
+        self.local_batch_size = str(local_batch_size)
+        self.static_lbs_profile_dir = os.path.join(self.profile_dir, self.local_batch_size)
+        # Check structure of self.static_lbs_profile_dir is {profile dir}/{model}/{lbs}
+        if not (len(os.listdir(self.static_lbs_profile_dir)) == 1 and \
+                os.listdir(self.static_lbs_profile_dir)[0] == socket.gethostname()):
+            raise ValueError(f"profile dir must contain local batch size sub-directory, "
+                             f"but: {os.listdir(self.static_lbs_profile_dir)}")
+
+        # NOTE: profile dir must contain one sub-directory of model name: {dest_profile_dir}/{model}
+        self.dest_profile_dir = os.path.join(dest_profile_dir, model_name)
+        os.makedirs(self.dest_profile_dir, exist_ok=True)
+        if os.path.exists(os.path.join(self.dest_profile_dir, self.local_batch_size)):
+            raise ValueError(f'{os.path.join(self.dest_profile_dir, self.local_batch_size)} already exist')
+        self.log('========================================================')
+        self.log(f'[Initialize] Copy {self.static_lbs_profile_dir} to {self.dest_profile_dir}')
+        self.log('========================================================')
+        os.system(f'cp -r {self.static_lbs_profile_dir} {self.dest_profile_dir}')
+
+        self._prepare_for_training_scheme()
+
+    def evaluate(self):
+        with self.evaluate_handler():
+            profiled_max_num_models = get_max_num_models_for_static_lbs(self.profile_dir, self.local_batch_size)
+            max_num_models = profiled_max_num_models
+            while True:
+                if max_num_models < 1:
+                    break
+                try:
+                    rank = 0
+                    world_size = 1
+                    weight_sync_method = 'recommend'
+                    if self.training_scheme == 'static':
+                        accum_step = 1
+                        if self.user_defined_cmd_fn is not None:
+                            command = self.user_defined_cmd_fn(
+                                self.shell_script, rank, world_size, self.local_batch_size,
+                                max_num_models, accum_step, weight_sync_method
+                            )
+                        else:
+                            command = [
+                                self.shell_script, rank, world_size, self.local_batch_size,
+                                max_num_models, accum_step, weight_sync_method
+                            ]
+                    elif self.training_scheme == 'adaptive':
+                        accum_step = 2
+                        if self.user_defined_cmd_fn is not None:
+                            command = self.user_defined_cmd_fn(
+                                self.shell_script, rank, world_size, self.local_batch_size,
+                                max_num_models, accum_step, weight_sync_method,
+                                self.mock_adaptive_config_file
+                            )
+                        else:
+                            command = [
+                                self.shell_script, rank, world_size, self.local_batch_size,
+                                max_num_models, accum_step, weight_sync_method,
+                                self.mock_adaptive_config_file
+                            ]
+                    with nvidia_smi_memory_monitoring(max_num_models, self.mem_util_threshold):
+                        loop = asyncio.get_event_loop()
+                        loop.run_until_complete(async_run_command(command, self.timeout))
+                    self.log('Success to execute command')
+                    self.log('Sleep 30 sec ..')
+                    time.sleep(30)
+                    break
+                except RuntimeError as e: # OOM happen
+                    self.log(e)
+                    kill_python_cmd = "kill -9 `ps | grep python | grep -v {0} | grep -v defunct | awk -F' ' '{{print $1}}'`".format(os.getpid())
+                    os.system(kill_python_cmd)
+                    kill_nvidiasmi_query_cmd = \
+                        f"kill -9 `ps -ef | grep -v grep | grep \"nvidia-smi --query\" | awk '{{print $2}}' `"
+                    os.system(kill_nvidiasmi_query_cmd)
+                    self.log('Sleep 30 sec ..')
+                    time.sleep(30)
+                    max_num_models -= 1
+            # Get real max num models
+            self.log('========================================================')
+            self.log(f'Profiled max number of VSWs ({self.local_batch_size}): {profiled_max_num_models}')
+            self.log(f'Real max number of VSWs ({self.local_batch_size}): {max_num_models}')
+            self.log('========================================================')
+            # change max num models if it is different
+            if profiled_max_num_models != max_num_models:
+                if max_num_models == 0:
+                    self.log(f'Remove profile data in {os.path.join(self.dest_profile_dir, self.local_batch_size)}')
+                    os.system(f'rm -rf {os.path.join(self.dest_profile_dir, self.local_batch_size)}')
+                else:
+                    self.log(f'Update profile data in {os.path.join(self.dest_profile_dir, self.local_batch_size)}')
+                    memory_profile_json_data = get_max_profile_json_data(self.dest_profile_dir, self.local_batch_size)
+                    memory_profile_json_data['max_num_models'] = max_num_models
+                    set_max_profile_json_data(self.dest_profile_dir, self.local_batch_size, memory_profile_json_data)
+            self.log('Sleep 30 sec ..')
+            time.sleep(30)
+
+        loop.close()
\ No newline at end of file
diff --git a/torch/iidp/test/__init__.py b/torch/iidp/test/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/test/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/test/utils/__init__.py b/torch/iidp/test/utils/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/test/utils/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/test/utils/api/__init__.py b/torch/iidp/test/utils/api/__init__.py
new file mode 100644
index 00000000000..b9742821a6f
--- /dev/null
+++ b/torch/iidp/test/utils/api/__init__.py
@@ -0,0 +1 @@
+from . import *
\ No newline at end of file
diff --git a/torch/iidp/test/utils/api/cost_estimator.py b/torch/iidp/test/utils/api/cost_estimator.py
new file mode 100644
index 00000000000..7daccff1eb7
--- /dev/null
+++ b/torch/iidp/test/utils/api/cost_estimator.py
@@ -0,0 +1,151 @@
+import os
+
+from torch.iidp.test.utils.server import build_mock_server_info, build_server_from_resource_info
+from torch.iidp.utils.json_utils import read_json
+
+from torch.iidp.utils.cost_utils import estimate_cost
+
+import argparse
+
+parser = argparse.ArgumentParser(description='Cost Estimator')
+parser.add_argument('-i', '--input-file', default=None, type=str, required=True,
+                    help='path to input file (convergence log)')
+parser.add_argument('-c', '--config-file', type=str, required=True,
+                    help='Training configuration file path (json)')
+
+
+def parse_time_per_epoch(args):
+    all_time_per_epoch = []
+    epoch_identifier = 'Epoch time:'
+    with open(args.input_file, 'r') as f:
+        for i, line in enumerate(f.readlines()):
+            if epoch_identifier in line and 'min' not in line and 'total' not in line:
+                try:
+                    data = int(float(line.split(' ')[3])) # -1 doesn't work due to reallocation time string
+                except:
+                    print('[ERROR]' + line)
+                    print(line.split(' '))
+                    exit(1)
+                #print(data)
+                all_time_per_epoch.append(data)
+
+    return all_time_per_epoch
+
+
+def parse_as_time_per_epoch(args):
+    all_time_per_epoch = []
+    epoch_identifier = 'Forecasting overhead takes'
+    with open(args.input_file, 'r') as f:
+        for i, line in enumerate(f.readlines()):
+            if epoch_identifier in line:
+                try:
+                    data = int(float(line.split(' ')[-2])) # -1 is "sec"
+                except:
+                    print('[ERROR]' + line)
+                    print(line.split(' '))
+                    exit(1)
+                #print(data)
+                all_time_per_epoch.append(data)
+
+    return all_time_per_epoch
+
+
+def parse_resource_per_epoch(args):
+    all_data = []
+    identifier = 'current resource info:'
+    with open(args.input_file, 'r') as f:
+        for i, line in enumerate(f.readlines()):
+            if identifier in line:
+                try:
+                    data = eval(line.split(identifier)[-1])
+                except:
+                    print('[ERROR]' + line)
+                    print(line.split(' '))
+                    exit(1)
+                #print(data)
+                all_data.append(data)
+
+    num_gpus_data = []
+    all_gpus_info = []
+    for data in all_data:
+        gpus_info = {}
+        for key, val in data.items():
+            if key == 'total_num_gpus':
+                num_gpus_data.append(int(val))
+            else:
+                gpus_info[key] = int(val)
+        all_gpus_info.append(gpus_info)
+
+    return all_gpus_info
+
+
+def measure_epoch_cost(epoch, epoch_duration, global_server_info, resource_info):
+    assert epoch_duration > 0, \
+        f"[ERROR][torch/iidp/test/utils/api/cost_estimator.py] measure_epoch_cost() " \
+        f"Argument ```epoch_duration``` must be > 0"
+    total_cost_per_epoch = 0
+    for server_info in global_server_info:
+        cost = estimate_cost(
+                server_info.resource_info.tfplos,
+                server_info.resource_info.num_gpus_in_server,
+                epoch_duration / 3600 # convert to sec to hour
+            )
+        assert cost > 0, \
+            f"[ERROR][torch/iidp/test/utils/api/cost_estimator.py] measure_epoch_cost() " \
+            f"Return value of ```estimate_cost()``` must be > 0 | " \
+            f"server_info: {server_info} | epoch_duration: {epoch_duration}"
+        total_cost_per_epoch += cost
+    print(f'[epoch {epoch}] Epoch cost: {total_cost_per_epoch:.2f} | '
+          f'duration (sec): {epoch_duration} | '
+          f'resource info: {resource_info}')
+    return total_cost_per_epoch
+
+
+def main():
+    args = parser.parse_args()
+
+    if not os.path.isfile(args.input_file):
+        raise FileExistsError(f'--input-file must be exist: {args.input_file}')
+    #args.output_dir = '/'.join(args.input_file.split('/')[:-1])
+    #print(f'[INFO] output dir: {args.output_dir}')
+    #os.makedirs(args.output_dir, exist_ok=True)
+
+    config_params = read_json(args.config_file)
+    gpu_cluster_info = read_json(config_params["gpu_cluster_info"])
+    available_servers = config_params["available_servers"]
+    server_str_list = []
+    for server_name in available_servers:
+        server_str_list.append(f'{server_name}:{gpu_cluster_info[server_name]["number"]}')
+    cluster_str = ','.join(server_str_list)
+    mock_available_server_info, _ = build_mock_server_info(cluster_str, gpu_cluster_info)
+
+    all_time_per_epoch = parse_time_per_epoch(args)
+    all_as_time_per_epoch = parse_as_time_per_epoch(args)
+    if len(all_as_time_per_epoch) == 0:
+        all_as_time_per_epoch = [0] * (len(all_time_per_epoch)-1)
+    if len(all_time_per_epoch) != len(all_as_time_per_epoch)+1:
+        raise ValueError(f'Number of 1) time per epoch and 2) ```auto-scaling overhead per epoch``` must be same, '
+                         f'but 1) {len(all_time_per_epoch)} and 2) {len(all_as_time_per_epoch)}')
+    else:
+        # NOTE: At last epoch, auto-scaling is not executed
+        all_as_time_per_epoch.append(0)
+
+    all_gpus_info_per_epoch = parse_resource_per_epoch(args)
+    if len(all_time_per_epoch) != len(all_gpus_info_per_epoch):
+        raise ValueError(f'Number of 1) time per epoch and 2) ```resource (GPU) info``` must be same, '
+                         f'but 1) {len(all_time_per_epoch)} and 2) {len(all_gpus_info_per_epoch)}')
+
+    total_cost = 0
+    for epoch, (epoch_duration, as_time, resource_info) in \
+            enumerate(zip(all_time_per_epoch, all_as_time_per_epoch, all_gpus_info_per_epoch)):
+        mock_global_server_info = build_server_from_resource_info(resource_info, mock_available_server_info, gpu_cluster_info)
+        total_cost += measure_epoch_cost(epoch, epoch_duration+as_time, mock_global_server_info, resource_info)
+
+    print(f'\n=====================================================')
+    print(f'[INFO] Total cost (dollar): {total_cost:.3f}')
+    print(f'=====================================================')
+
+
+
+if __name__ == '__main__':
+    main()
diff --git a/torch/iidp/test/utils/common_utils.py b/torch/iidp/test/utils/common_utils.py
new file mode 100644
index 00000000000..f82753f29c7
--- /dev/null
+++ b/torch/iidp/test/utils/common_utils.py
@@ -0,0 +1,48 @@
+import os
+import math
+
+from torch.iidp.cluster.server import GlobalServerInfo
+
+
+# Similar logic in __init__ of IIDPConfigurator in torch/iidp/config/configurator.py
+def get_possible_batch_size_across_cluster(comp_profile_dir, global_server_info):
+    all_server_names = []
+    if type(global_server_info) == list:
+        all_server_names = global_server_info
+    elif type(global_server_info) == GlobalServerInfo:
+        for server_info in global_server_info:
+            all_server_names.append(server_info.name)
+    else:
+        raise TypeError(f'Not support type of arugment global_server_info: {type(global_server_info)}')
+
+    min_possible_lbs = math.inf
+    for lbs in os.listdir(comp_profile_dir):
+        local_batch_size = int(lbs)
+        static_lbs_comp_profile_dir = os.path.join(comp_profile_dir, lbs)
+        # Check current local batch size can be supported by current global servers
+        """
+        print(
+            f'all_server_names: {all_server_names} | '
+            f'static_lbs_comp_profile_dir: {static_lbs_comp_profile_dir} | '
+            f'os.listdir(static_lbs_comp_profile_dir): {os.listdir(static_lbs_comp_profile_dir)}'
+        )
+        """
+        if not set(all_server_names).issubset(set(os.listdir(static_lbs_comp_profile_dir))):
+            """
+            print(
+                f'local_batch_size: {local_batch_size} '
+                f'is not supported by current cluster: {" ".join(all_server_names)} '
+                f'==> skip it for IIDP configuration'
+            )
+            """
+            continue
+        if local_batch_size < min_possible_lbs:
+            min_possible_lbs = local_batch_size
+    if type(min_possible_lbs) != int:
+        raise ValueError(
+            f'[ERROR][get_possible_batch_size_across_cluster()] '
+            f'Not exists any possible min local batch size across cluster: '
+            f'{",".join(all_server_names)}.\n'
+            f'It might cause since no profile data for local batch size exists on some server.\n'
+            f'Please check profile data directory: ```{comp_profile_dir}```')
+    return min_possible_lbs
diff --git a/torch/iidp/test/utils/config.py b/torch/iidp/test/utils/config.py
new file mode 100644
index 00000000000..c93dbfc00c5
--- /dev/null
+++ b/torch/iidp/test/utils/config.py
@@ -0,0 +1,15 @@
+
+
+def get_iidp_config_map(config_str):
+    iidp_config_map = {}
+    unique_ranks = []
+    rank_configs = config_str.split('|')
+    for rank_config in rank_configs:
+        rank, config_tuple = rank_config.split(':')
+        iidp_config_map[int(rank)] = eval(config_tuple)
+        # Check uniqueness of rank
+        if rank not in unique_ranks:
+            unique_ranks.append(rank)
+        else:
+            raise ValueError(f'Rank in argument string must be unique - {config_str}')
+    return iidp_config_map
\ No newline at end of file
diff --git a/torch/iidp/test/utils/dataloader.py b/torch/iidp/test/utils/dataloader.py
new file mode 100644
index 00000000000..6eb38c23e1f
--- /dev/null
+++ b/torch/iidp/test/utils/dataloader.py
@@ -0,0 +1,19 @@
+from torch.iidp.config.examples.config_utils import NUM_DATASET, REGISTERED_MODELS_FOR_DATASET
+
+
+class MockDataLoader(object):
+    def __init__(self, model_name):
+        if model_name not in REGISTERED_MODELS_FOR_DATASET:
+            raise ValueError(
+                f'argument ```model_name``` must be chosen among {REGISTERED_MODELS_FOR_DATASET}')
+        self.dataset = MockDataset(model_name)
+
+class MockDataset(object):
+    def __init__(self, model_name):
+        if model_name not in REGISTERED_MODELS_FOR_DATASET:
+            raise ValueError(
+                f'argument ```model_name``` must be chosen among {REGISTERED_MODELS_FOR_DATASET}')
+        self.model_name = model_name
+
+    def __len__(self):
+        return NUM_DATASET(self.model_name)
\ No newline at end of file
diff --git a/torch/iidp/test/utils/dp_solver.py b/torch/iidp/test/utils/dp_solver.py
new file mode 100644
index 00000000000..eedf51eb1b4
--- /dev/null
+++ b/torch/iidp/test/utils/dp_solver.py
@@ -0,0 +1,49 @@
+import os
+from torch.iidp.config.model.throughput.throughput_model import ThroughputModel
+from torch.iidp.config.configurator import DynamicProgrammingSolver
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.profiler.profiler import MAX_MEM_PROFILE_FILE_NAME
+
+
+def create_memory_profile_info(memory_profile_dir):
+    # Create memory profile info (type: dict)
+    memory_profile_info = {}
+    for lbs in os.listdir(memory_profile_dir):
+        if not lbs in memory_profile_info.keys():
+            memory_profile_info[lbs] = {}
+        static_lbs_comp_profile_dir = os.path.join(memory_profile_dir, lbs)
+        for server_name in os.listdir(static_lbs_comp_profile_dir):
+            max_memory_profile_file = os.path.join(
+                static_lbs_comp_profile_dir, server_name, MAX_MEM_PROFILE_FILE_NAME)
+            memory_profile_json_data = read_json(max_memory_profile_file)
+            memory_profile_info[lbs][memory_profile_json_data['gpu_type']] = memory_profile_json_data['max_num_models']
+    return memory_profile_info
+
+
+def instanciate_dp_solver(config_params, global_server_info, local_batch_size, weight_sync_method):
+    comp_profile_dir = config_params["comp_profile_dir"]
+    comm_profile_dir = config_params["comm_profile_dir"]
+    static_lbs_comp_profile_dir = os.path.join(comp_profile_dir, str(local_batch_size))
+    bucket_profile_dir = config_params["bucket_profile_dir"]
+    memory_profile_dir = config_params["memory_profile_dir"]
+    throughput_models = {}
+    for server_info in global_server_info:
+        local_comp_profile_dir = os.path.join(static_lbs_comp_profile_dir, server_info.name)
+        throughput_models[server_info.name] = \
+            ThroughputModel(local_comp_profile_dir, comm_profile_dir, bucket_profile_dir)
+
+    if len(os.listdir(comp_profile_dir)) != len(os.listdir(memory_profile_dir)):
+        raise ValueError(
+            f'[ERROR] Computation and memory profile data for range of local batch size '
+            f'must be equal, but comp: {os.listdir(comp_profile_dir)} | mem: {os.listdir(memory_profile_dir)}')
+    memory_profile_info = create_memory_profile_info(memory_profile_dir)
+    static_lbs_mem_profile_info = memory_profile_info[str(local_batch_size)]
+
+    if "batch_size_upper_bound" not in config_params.keys():
+        max_num_workers = -1
+    else:
+        max_num_workers = config_params["batch_size_upper_bound"]//local_batch_size+1
+    dp_solver = DynamicProgrammingSolver(
+                    local_batch_size, weight_sync_method, throughput_models, static_lbs_mem_profile_info,
+                    global_server_info, max_num_workers)
+    return dp_solver
\ No newline at end of file
diff --git a/torch/iidp/test/utils/resource.py b/torch/iidp/test/utils/resource.py
new file mode 100644
index 00000000000..c06c89b0b63
--- /dev/null
+++ b/torch/iidp/test/utils/resource.py
@@ -0,0 +1,16 @@
+from torch.iidp.cluster.server import GlobalServerInfo
+
+
+def resource_info_parser(global_server_info):
+    if not isinstance(global_server_info, GlobalServerInfo):
+        raise TypeError(f'Argument global_server_info must be type of GlobalServerInfo, but {type(global_server_info)}')
+    #print(f'[DEBUG][current_resource_info_parser] global_server_info: {global_server_info}')
+
+    resource_info_dict = {}
+    resource_info_dict['total_num_gpus'] = global_server_info.total_num_gpus
+    for server_info in global_server_info:
+        if server_info.resource_info.device_name in resource_info_dict.keys():
+            resource_info_dict[server_info.resource_info.device_name] += server_info.resource_info.num_gpus_in_server
+        else:
+            resource_info_dict[server_info.resource_info.device_name] = server_info.resource_info.num_gpus_in_server
+    return resource_info_dict
diff --git a/torch/iidp/test/utils/server.py b/torch/iidp/test/utils/server.py
new file mode 100644
index 00000000000..b1e7932fb8f
--- /dev/null
+++ b/torch/iidp/test/utils/server.py
@@ -0,0 +1,85 @@
+from torch.iidp.cluster.server import GlobalServerInfo, ServerInfo
+
+
+def build_global_cluster_by_config_file(available_servers: list, gpu_cluster_info: dict):
+    cluster_str_list = []
+    for available_server in available_servers:
+        num_gpus_in_server = gpu_cluster_info[available_server]['number']
+        cluster_str_list.append(f'{available_server}:{num_gpus_in_server}')
+    cluster = ','.join(cluster_str_list)
+    print('====================================================================')
+    print(f'[TEST] Global cluster: {cluster}')
+    print('====================================================================')
+    return cluster
+
+
+def build_mock_server_info(cluster: str, gpu_cluster_info: dict):
+    if type(cluster) != str:
+        raise TypeError(f'[ERROR] Argument cluster must be string type, '
+                        f'but type: {type(cluster)} | cluster: {cluster}')
+    if type(gpu_cluster_info) != dict:
+        raise TypeError(f'[ERROR] Argument gpu_cluster_info must be dictionary type, '
+                        f'but type: {type(gpu_cluster_info)} | gpu_cluster_info: {gpu_cluster_info}')
+    mock_global_server_group = {}
+    server_groups = cluster.split(',')
+    last_rank = 0
+    total_num_gpus = 0
+    for server_group in server_groups:
+        hostname, num_gpus_in_server = server_group.split(':')
+        ranks = [last_rank + rank for rank in range(int(num_gpus_in_server))]
+        last_rank = ranks[-1] + 1
+        mock_global_server_group[hostname] = ranks
+        total_num_gpus+=int(num_gpus_in_server)
+    print(f'[TEST] Mock server group: {mock_global_server_group}')
+    mock_global_server_info = GlobalServerInfo()
+    for name, ranks in mock_global_server_group.items():
+        mock_global_server_info.add(ServerInfo(name, ranks, gpu_cluster_info[name]))
+    print(f'[TEST] Mock Global Server Info: {mock_global_server_info}')
+    return mock_global_server_info, mock_global_server_group
+
+
+def build_server_from_resource_info(resource_info: dict, available_servers: GlobalServerInfo, gpu_cluster_info: dict):
+    ret_server_info = GlobalServerInfo() # return value
+    device_names_in_resource_info = list(resource_info.keys())
+    device_names_in_available_servers = []
+    for server in available_servers:
+        device_names_in_available_servers.append(server.resource_info.device_name)
+    if not set(device_names_in_resource_info).issubset(set(device_names_in_available_servers)):
+        raise AssertionError(
+            f'[ERROR][torch/iidp/test/utils/server.py] '
+            f'build_server_from_resource_info() => '
+            f'No common device between resource info and available servers!\n'
+            f'===================================================\n'
+            f'Device names in ```available_servers```: {device_names_in_available_servers} \n'
+            f'===================================================\n'
+            f'Device names in ```resource_info```: {device_names_in_resource_info}')
+
+    remaining_num_gpus = 0
+    for gpu_type, num_gpus in resource_info.items():
+        remaining_num_gpus = num_gpus
+        for server in available_servers:
+            if server.resource_info.device_name == gpu_type and remaining_num_gpus > 0:
+                allocated_num_gpus = min(server.resource_info.max_num_gpus_in_server, remaining_num_gpus)
+                ranks = list(range(server.ranks[0], server.ranks[0]+allocated_num_gpus))
+                ret_server_info.add(
+                    ServerInfo(server.name, ranks, gpu_cluster_info[server.name])
+                )
+                remaining_num_gpus -= allocated_num_gpus
+        assert remaining_num_gpus == 0, \
+            f"[ERROR][torch/iidp/test/utils/server.py] " \
+            f"build_server_from_resource_info() => " \
+            f"total number of GPUs from resource info cannot be used by available servers! " \
+            f"gpu_type: {gpu_type} | num_gpus: {num_gpus}\n" \
+            f'===================================================\n' \
+            f'available_servers: {available_servers} \n' \
+            f'===================================================\n'
+
+    if ret_server_info.total_num_servers == 0:
+        raise AssertionError(
+            f'[ERROR][torch/iidp/test/utils/server.py] '
+            f'build_server_from_resource_info() => server info is empty!\n'
+            f'===================================================\n'
+            f'available_servers: {available_servers} \n'
+            f'===================================================\n'
+            f'resource_info: {resource_info}')
+    return ret_server_info
\ No newline at end of file
diff --git a/torch/iidp/test/utils/timer.py b/torch/iidp/test/utils/timer.py
new file mode 100644
index 00000000000..b84c7dee78f
--- /dev/null
+++ b/torch/iidp/test/utils/timer.py
@@ -0,0 +1,24 @@
+import time
+from contextlib import ContextDecorator
+
+import torch.distributed as dist
+
+
+class Timer(ContextDecorator):
+    def __init__(self, msg, verbose=True):
+        self.msg = msg
+        self.verbose = verbose
+        self.elapsed = 0
+
+    def __enter__(self):
+        self.time = time.time()
+        return self
+
+    def __exit__(self, type, value, traceback):
+        self.elapsed = time.time() - self.time
+        if self.verbose:
+            if dist.is_initialized():
+                if dist.get_rank() == 0:
+                    print(f'{self.msg} takes {self.elapsed:.3f} sec')
+            else:
+                print(f'{self.msg} takes {self.elapsed:.3f} sec')
\ No newline at end of file
diff --git a/torch/iidp/trainer.py b/torch/iidp/trainer.py
new file mode 100644
index 00000000000..64b6a28f4c6
--- /dev/null
+++ b/torch/iidp/trainer.py
@@ -0,0 +1,2471 @@
+from contextlib import contextmanager
+from collections import defaultdict
+import math
+import os
+import sys
+sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1)
+import time
+import datetime
+
+import torch
+import torch.distributed as dist
+
+import threading
+import inspect
+import copy
+
+from torch.iidp.utils.distributed import get_allgather_value, print_one_rank
+from torch.iidp.utils.json_utils import read_json
+from torch.iidp.utils.cost_utils import estimate_cost
+
+from .elastic.runtime.rpc import trainer_client as trainer_client
+from .config.configurator import IIDPConfig, IIDPConfigurator, IIDPFutureConfigurator
+from .cluster.cluster_manager import IIDPClusterManager
+
+from torch.iidp.config.model.global_batch_size.gaussian_process import GaussianProcessRegressionModel
+from torch.iidp.config.model.global_batch_size.exponential_smoothing import ExponentialSmoothing
+from torch.iidp.config.model.global_batch_size.ensemble_method import EnsembleMethod
+
+from torch.iidp.config.config_utils import check_user_config_is_valid
+from torch.iidp.profiler.profiler_utils import get_mem_profile_data_summary
+
+from torch.iidp.test.utils.timer import Timer
+
+import gc
+
+
+REGISTERED_WEIGHT_SYNC_METHODS = [
+    'recommend',
+    'overlap',
+    'sequential'
+]
+
+
+class EpochIterator(object):
+    def __init__(self):
+        self.epoch_idx = -1
+        self.final_epochs = -1
+
+    @property
+    def epoch(self):
+        #print(f'[DEBUG][EpochIterator] property self.epoch_idx: {self.epoch_idx}')
+        if self.final_epochs == -1:
+            return 0
+        else:
+            return self.epoch_idx
+
+    @epoch.setter
+    def epoch(self, epoch):
+        self.epoch_idx = epoch
+        #print(f'[DEBUG][EpochIterator] self.epoch_idx: {self.epoch_idx}')
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        if self.final_epochs == -1:
+            raise ValueError(
+                f'[ERROR][torch/iidp/trainer.py] final_epochs must be > 0 in EpochIterator().__next__()')
+        if self.epoch_idx < self.final_epochs-1:
+            self.epoch_idx += 1
+            #print(f'[DEBUG][EpochIterator] self.epoch_idx: {self.epoch_idx}')
+            return self.epoch_idx
+        else:
+            self.epoch_idx = -1
+            raise StopIteration
+
+    def __len__(self):
+        return self.final_epochs
+
+
+class GlobalTrainerState(object):
+    def __init__(self):
+        self.partition_size = []
+        self.is_accum_mode = False
+        self.global_batch_size = 0
+
+
+class LocalTrainerState(object):
+    def __init__(self):
+        self.local_batch_size = 0
+        self.num_models = 0
+        self.accum_step = 0
+
+
+GLOBAL_TRAINER_STATE = GlobalTrainerState()
+LOCAL_TRAINER_STATE = LocalTrainerState()
+
+
+class TrainerHelper(object):
+    def __init__(self, gpu, local_batch_size, num_models, accum_step=1, weight_sync_method='recommend'):
+        self.gpu = gpu
+        self.local_batch_size = local_batch_size
+        self.num_models = num_models
+        self.accum_step = accum_step
+        self.max_accum_step = -1
+        self.batch_size_per_gpu = self.local_batch_size * self.num_models
+        if self.batch_size_per_gpu % local_batch_size != 0:
+            raise ValueError('Local batch size must be dividied by batch size per GPU')
+        if weight_sync_method not in REGISTERED_WEIGHT_SYNC_METHODS:
+            raise ValueError(f'Not support unregisted weight_sync_method: {weight_sync_method}')
+        self.weight_sync_method = weight_sync_method
+        if os.getenv("EASYSCALE") == "1" or os.getenv("SIMIGRAD") == "1":
+            if self.weight_sync_method != 'sequential':
+                raise ValueError(
+                    f'With EASYSCALE or SIMIGRAD weight sync method must be sequential, '
+                    f'but {self.weight_sync_method}')
+
+        self.model_streams = []
+
+        self.original_local_models = []
+        self.local_models = []
+        self.local_optimizers = []
+        self.local_schedulers = []
+        self.criterion = None
+        self.output_as_loss = False
+        self.losses = {}
+        self.sampler = None
+
+        # For overlapping optimizer
+        self.prepared_for_ddp = False
+        self.hooks = []
+        self.optimizer_stream = None
+        # One DDP model's bucket_indices (All of local model's bucket indices is same)
+        self.ddp_bucket_indices = []
+        self.is_rebuilt_ddp_bucket_indices = False
+
+        self._get_total_num_models()
+        self.global_batch_size = self.local_batch_size * self.total_num_models
+
+        self.all_partition_size_in_process_group = []
+        self._get_all_partition_size_in_process_group()
+
+        self.all_accum_step_in_process_group = []
+        self._get_all_accum_step_in_process_group()
+        self.max_accum_step = max(self.all_accum_step_in_process_group)
+
+        self.is_accum_mode = True if self.max_accum_step > 1 else False
+        # Used in seq_parallel_compute() for being block different number of VSWs on inter-node
+        self.sync_accum_barrier = threading.Barrier(self.num_models)
+
+        # It is used for _sync_params() in torch/nn/parallel/distributed.py
+        self._sync_buffer_barrier = [None, None]
+        if self.num_models > 1:
+            self._sync_buffer_barrier = [threading.Barrier(self.num_models) for i in range(2)]
+
+        self._set_trainer_state()
+
+        self.local_accum_step = 0
+        self.sync_step = 0
+        self.epoch_iterator = EpochIterator()
+        self.elapsed_time = 0
+        self.total_epoch_time = 0
+
+    def _set_trainer_state(self):
+        GLOBAL_TRAINER_STATE.partition_size = self.all_partition_size_in_process_group
+        GLOBAL_TRAINER_STATE.is_accum_mode = self.is_accum_mode
+        GLOBAL_TRAINER_STATE.global_batch_size = self.global_batch_size
+
+        LOCAL_TRAINER_STATE.local_batch_size = self.local_batch_size
+        LOCAL_TRAINER_STATE.num_models = self.num_models
+        LOCAL_TRAINER_STATE.accum_step = self.accum_step
+
+    def _get_total_num_models(self):
+        if dist.is_initialized():
+            tensor = torch.tensor([self.num_models * self.accum_step], dtype=torch.int64).to(self.gpu)
+            dist.all_reduce(tensor) # Default op is SUM
+            self.total_num_models = tensor.item()
+            tensor.cpu()
+            del tensor
+        else:
+            self.total_num_models = self.num_models * self.accum_step
+
+    def _get_all_partition_size_in_process_group(self):
+        local_partition_size = (self.batch_size_per_gpu * self.accum_step) / self.global_batch_size
+        self.all_partition_size_in_process_group = get_allgather_value(local_partition_size, self.gpu)
+
+    def _get_all_accum_step_in_process_group(self):
+        self.all_accum_step_in_process_group = get_allgather_value(self.accum_step, self.gpu)
+
+    def set_original_local_models(self, models):
+        """Set the compelete local models by users"""
+        if models is None:
+            raise ValueError(f"Argument is None: {models}")
+        else:
+            if not isinstance(models, (list, tuple)):
+                raise ValueError(
+                    f"Argument models must be list or tuple type: {type(models)}")
+        self.original_local_models = models
+        assert len(self.original_local_models) == self.num_models
+
+    def set_local_optimizers(self, optimizers):
+        """Set the compelete local optimizers by users"""
+        if optimizers is None:
+            raise ValueError(f"Argument is None: {optimizers}")
+        else:
+            if not isinstance(optimizers, (list, tuple)):
+                raise ValueError(
+                    f"Argument optimizers must be list or tuple type: {type(optimizers)}")
+        self.local_optimizers = optimizers
+        assert len(self.local_optimizers) == self.num_models
+
+    def set_local_schedulers(self, schedulers=None):
+        """Set the compelete local schedulers by users"""
+        # LR scheduler is optional
+        if schedulers is not None:
+            if not isinstance(schedulers, (list, tuple)):
+                raise ValueError(
+                    f"Argument optimizers must be list or tuple type: {type(schedulers)}")
+            self.local_schedulers = schedulers
+            assert len(self.local_schedulers) == self.num_models
+
+    def _set_original_local_models(self, model):
+        if model is None:
+            raise ValueError(f"Argument is None: {model}")
+        is_set_by_user = (len(self.original_local_models) == self.num_models)
+        if not is_set_by_user:
+            self.original_local_models = [model]
+            for _ in range(1, self.num_models):
+                copied_model = copy.deepcopy(model)
+                self.original_local_models.append(copied_model)
+
+    def _set_criterion(self, criterion):
+        if criterion is None:
+            raise ValueError(f"Argument is None: {criterion}")
+        self.criterion = criterion
+        if hasattr(self.criterion, 'forward'):
+            args_of_criterion = inspect.getfullargspec(getattr(self.criterion, 'forward')).args
+        else:
+            args_of_criterion = inspect.getfullargspec(self.criterion).args
+        if 'self' in args_of_criterion:
+            args_of_criterion.remove('self')
+        num_args = len(args_of_criterion)
+        if num_args == 1:
+            self.output_as_loss = True
+        elif num_args == 2: # We expect arguments as output (y) and target (y^)
+            self.output_as_loss = False
+        else:
+            raise ValueError(
+                f"Not support number of arguments in criterion function > 2: {num_args}")
+
+        self.trainer_print(f"Criterion has {num_args} argument(s): {','.join(args_of_criterion)} "
+              f"=> self.output_as_loss: {self.output_as_loss}")
+
+    def _get_required_args_value(self, instance):
+        """Helper function for _set_local_optimizers() and _set_local_schedulers()"""
+        removable_args = ['self', 'optimizer', 'params', 'lr']
+        args_inspect = inspect.getfullargspec(instance.__init__)
+        args_of_instace = args_inspect.args
+        filtered_args_of_instance = [x for x in args_of_instace if x not in removable_args]
+        is_defaults_exists = (args_inspect.defaults is not None and len(args_inspect.defaults) > 1)
+        if is_defaults_exists:
+            required_args = filtered_args_of_instance[:-len(args_inspect.defaults)]
+        else:
+            required_args = filtered_args_of_instance
+        args = []
+        for arg_name in required_args:
+            try:
+                # NOTE: In torch/optim/lr_scheduler.py, ```LambdaLR``` class has self.lr_lambdas,
+                # but argument is lr_lambda
+                if arg_name == 'lr_lambda':
+                    args.append(instance.__dict__['lr_lambdas'][0])
+                else:
+                    args.append(instance.__dict__[arg_name])
+            except KeyError:
+                raise KeyError(f'[ERROR] instance.__dict__: {instance.__dict__} \n'
+                               f'This might happen if argument is not registered by '
+                               f'member variable of instance.')
+        return args
+
+    def _set_local_optimizers(self, optimizer, param_groups_func=None):
+        """
+        NOTE: Even main optimizer only updates globally aggregated gradients,
+        optimizer.zero_grad() is efficient for parallel_compute().
+        That's why we keep the individual optimizer for each local model.
+        """
+        if not issubclass(type(optimizer), torch.optim.Optimizer):
+            raise TypeError(
+                f'To set local optimizers for copy (use _set_local_optimizers()), original optimizer type: '
+                f'{type(optimizer)} '
+                f'must be sub-class of torch.optim.Optimizer')
+        if optimizer is None:
+            raise TypeError(f"Argument optimizer must be configured, but {optimizer}")
+
+        self.param_groups_func = param_groups_func
+        is_set_by_user = (len(self.local_optimizers) == self.num_models)
+        if not is_set_by_user:
+            self.local_optimizers = [optimizer]
+            for idx in range(1, self.num_models):
+                if self.param_groups_func:
+                    params = self.param_groups_func(self.original_local_models[idx])
+                else:
+                    params = self.original_local_models[idx].parameters()
+                args = self._get_required_args_value(optimizer)
+                # https://stackoverflow.com/questions/21060073/dynamic-inheritance-in-python
+                cls = type(optimizer.__class__.__name__, (optimizer.__class__,), dict(optimizer.__dict__))
+                copied_optimizer = cls(params, lr=optimizer.defaults['lr'], *args)
+                copied_optimizer.load_state_dict(optimizer.state_dict())
+                self.local_optimizers.append(copied_optimizer)
+        for optimizer in self.local_optimizers:
+            optimizer.zero_grad()
+
+    def _set_local_schedulers(self, scheduler=None):
+        # LR scheduler is optional
+        if scheduler is not None:
+            is_set_by_user = (len(self.local_schedulers) == self.num_models)
+            if not is_set_by_user:
+                self.local_schedulers = [scheduler]
+
+    @property
+    def main_stream(self):
+        return self.model_streams[0]
+
+    @property
+    def eval_model(self):
+        if type(self.local_models[0]) == torch.nn.parallel.DistributedDataParallel:
+            return self.local_models[0].module
+        else:
+            return self.local_models[0]
+
+    @property
+    def main_model(self):
+        return self.local_models[0]
+
+    @property
+    def main_optimizer(self):
+        return self.local_optimizers[0]
+
+    @property
+    def main_scheduler(self):
+        return self.local_schedulers[0] if self.local_schedulers is not None else None
+
+    @property
+    def num_local_models(self):
+        return self.num_models
+
+    @property
+    def epoch(self):
+        #self.trainer_print(f'trainer.epoch property: {self.epoch_iterator.epoch}')
+        return self.epoch_iterator.epoch
+
+    @epoch.setter
+    def epoch(self, epoch):
+        self.epoch_iterator.epoch = epoch
+
+    def remaining_epochs(self, final_epochs):
+        #self.trainer_print(f'Number of epochs to train: {final_epochs}')
+        self.epoch_iterator.final_epochs = final_epochs
+        try:
+            for epoch in self.epoch_iterator.__iter__():
+                yield epoch
+        finally:
+            self.print_final_results()
+
+    def print_final_results(self):
+        self.trainer_print(f'Total epoch time (sec): {self.total_epoch_time}')
+        self.trainer_print(f'Total epoch time: {datetime.timedelta(seconds=self.total_epoch_time)}')
+
+    def set_model_train(self):
+        for local_model in self.local_models:
+            local_model.train()
+
+    def _create_model_streams(self):
+        for _ in range(self.num_models):
+            self.model_streams.append(torch.cuda.Stream())
+
+    def _create_stream_for_optimizer(self):
+        self.optimizer_stream = torch.cuda.Stream()
+
+    def _prepare_hooks_for_local_models(self, hook):
+        def dummy_hook(state, bucket):
+            fut = torch.futures.Future()
+            fut.set_result(bucket.get_tensors())
+            return fut
+
+        for model_idx in range(self.num_models):
+            if model_idx == 0:
+                self.hooks.append(hook)
+            else:
+                self.hooks.append(dummy_hook)
+
+    def _check_overlap_with_ddp(self):
+        if not self.prepared_for_ddp:
+            raise ValueError(
+                "DDP instance must be prepared with self.weight_sync_method = overlap")
+
+        for param_indices in self.ddp_bucket_indices:
+            if param_indices != sorted(param_indices):
+                raise RuntimeError(
+                    "Parameter indices in each bucket must be sorted with self.weight_sync_method = overlap")
+
+        if self.hooks is None:
+            raise ValueError(
+                "hooks must be prepared with self.weight_sync_method = overlap")
+        elif not isinstance(self.hooks, (list, tuple)):
+            raise ValueError(
+                f"Argument hooks must be list or tuple type: {type(self.hooks)}")
+        elif len(self.hooks) != self.num_models:
+            raise ValueError(f"Number of hooks: {len(self.hooks)} "
+                             f"must be equal to "
+                             f"number of local models : {self.num_models}")
+        for hook in self.hooks:
+            if not callable(hook):
+                raise TypeError("hook must be callable.")
+
+        if self.optimizer_stream is None:
+            raise ValueError(
+                "optimizer_stream must be assigned with self.weight_sync_method = overlap")
+
+    def trainer_print(self, message, status='info'):
+        print_msg = f'[{status.upper()}][{self.__class__.__name__}] {message}'
+        if dist.is_initialized():
+            if dist.get_rank() == 0:
+                print(print_msg)
+        else:
+            print(print_msg)
+
+    @contextmanager
+    def measure_epoch_time(self):
+        try:
+            start_time = time.time()
+            yield
+        finally:
+            self.elapsed_time = int(time.time() - start_time)
+            self.trainer_print(f'Epoch time: {self.elapsed_time}')
+            self.total_epoch_time += self.elapsed_time
+
+    @contextmanager
+    def record_epoch_data(self):
+        try:
+            yield
+        finally:
+            self.trainer_print(f'record at epoch: {self.epoch} | iterations: {self.sync_step} | loss: {self.losses[0]:.3f}')
+
+
+class IIDPTrainer(TrainerHelper):
+    def __init__(self, gpu, local_batch_size, num_models, accum_step, weight_sync_method='recommend'):
+        super().__init__(gpu, local_batch_size, num_models, accum_step, weight_sync_method)
+
+    def prepare_stream_parallel(self, model, criterion, **kwargs):
+        gradient_as_bucket_view = kwargs.get('gradient_as_bucket_view') or False
+        find_unused_parameters = kwargs.get('find_unused_parameters') or False
+        self._create_model_streams()
+        self._set_original_local_models(model)
+        self._set_criterion(criterion)
+        for idx, original_model in enumerate(self.original_local_models):
+            # Assign buckets for DDP to model's stream to synchronize copy in all-reduce
+            with torch.cuda.stream(self.model_streams[idx]):
+                local_ddp_module = torch.nn.parallel.DistributedDataParallel(
+                                    original_model, device_ids=[self.gpu], output_device=[self.gpu],
+                                    find_unused_parameters=find_unused_parameters,
+                                    gradient_as_bucket_view=gradient_as_bucket_view,
+                                    model_index=idx, num_local_models=self.num_models,
+                                    total_num_models=self.total_num_models,
+                                    sync_buffer_barrier=self._sync_buffer_barrier)
+                self.local_models.append(local_ddp_module)
+            # It is used for overlapping optimizer with torch.nn.parallel.DistributedDataParallel
+            self.ddp_bucket_indices = self.main_model.bucket_indices
+            self.prepared_for_ddp = True
+
+            if self.weight_sync_method == 'recommend':
+                self._recommend_weight_update()
+            else: # Show bucket distribution even weight sync method is not 'recommend' option
+                self._calculate_bucket_distribution()
+
+    def prepare_weight_sync_method(self, optimizer, scheduler=None, param_groups_func=None):
+        self._set_local_optimizers(optimizer, param_groups_func)
+        self._set_local_schedulers(scheduler)
+        if self.weight_sync_method == 'overlap':
+            if self.prepared_for_ddp:
+                self._prepare_overlap_optimizer_with_ddp()
+            else:
+                raise RuntimeError("[ERROR] Without DDP, overlap optimizer cannot work")
+
+    def _recommend_weight_update(self):
+        """
+        If the distribution of bucket size is not uniform,
+        overlapping optimizer is not recommended
+        due to interference with all-reduce NCCL kernel
+        """
+        def get_avg(val):
+            if isinstance(val, (list, tuple)):
+                return int(sum(val) / len(val))
+        bukcet_capacity = self.main_model.bucket_bytes_cap / (1024 * 1024) # MB
+        bucket_size_distribution = self._calculate_bucket_distribution()
+        # Last bucket doesn't overlap with all-reduce kernel
+        allreduce_overlap_bucket_distribution = bucket_size_distribution[:-1]
+        #self.trainer_print(f'bucket : {allreduce_overlap_bucket_distribution}', 'debug')
+        """ Buckets to the size specified by the user is larger than a threshold
+        potential_interference_bucket = [
+            bucket for bucket in allreduce_overlap_bucket_distribution \
+                 if bucket > bukcet_capacity
+        ]
+        """
+        potential_interference_bucket = allreduce_overlap_bucket_distribution
+        #self.trainer_print(f'potential_interference_bucket: {potential_interference_bucket}', 'debug')
+        avg_outlier_bucket_size = get_avg(potential_interference_bucket)
+        #self.trainer_print(f'avg_outlier_bucket_size: {avg_outlier_bucket_size}', 'debug')
+        norm_avg_outlier_bucket_size = avg_outlier_bucket_size / bukcet_capacity
+        if norm_avg_outlier_bucket_size > 1.5: # Threshold is heuristic
+            self.weight_sync_method = 'sequential'
+        else:
+            self.weight_sync_method = 'overlap'
+        self.trainer_print(f'Recommend [{self.weight_sync_method}] as weight sync method '
+              f'as uniformity of bucket size is {norm_avg_outlier_bucket_size}')
+
+    def _calculate_bucket_distribution(self):
+        bucket_size_distribution = []
+        parameter_size_distribution = []
+        for _, param in enumerate(self.main_model.ddp_register_params):
+            if hasattr(param, 'index'):
+                param_mem_value = round(param.nelement() * param.element_size() / (1024 ** 2), 2)
+                parameter_size_distribution.append(param_mem_value)
+
+        for bucket in self.ddp_bucket_indices:
+            bucket_size = 0
+            for param_index in bucket:
+                param_size = parameter_size_distribution[param_index]
+                bucket_size += param_size
+            bucket_size_distribution.append(round(bucket_size, 2))
+        self.trainer_print(f'bucket_size_distribution (backward order): {bucket_size_distribution}', 'debug')
+        return bucket_size_distribution
+
+    @contextmanager
+    def accum_processing(self):
+        if dist.is_initialized() and self.local_accum_step == 0:
+            self.prev_require_forward_param_sync = self.main_model.require_forward_param_sync
+            def _forward_model(model, stream):
+                with torch.cuda.stream(stream):
+                    if model.require_forward_param_sync:
+                        model._sync_params()
+                        model.require_forward_param_sync = False
+
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_forward_model,
+                                        args=(self.local_models[idx], self.model_streams[idx],))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        try:
+            yield
+        finally:
+            self.local_accum_step += 1
+            if dist.is_initialized() and self.local_accum_step == self.accum_step:
+                for model in self.local_models:
+                    model.require_forward_param_sync = self.prev_require_forward_param_sync
+
+    def _compute_forward_and_loss(self, model, criterion, input, target):
+        #print(f'[DEBUG] _compute_forward_and_loss() - input.size(): {input.size()} | target.size(): {target.size()}')
+        if self.output_as_loss:
+            model_to_inspect = model.module if type(model) == torch.nn.parallel.DistributedDataParallel else model
+            args_of_model = inspect.getfullargspec(getattr(model_to_inspect, 'forward')).args
+            if 'self' in args_of_model:
+                args_of_model.remove('self')
+            num_args = len(args_of_model)
+            if num_args > 2:
+                loss = criterion(model(*input, target))
+            else:
+                loss = criterion(model(input, target))
+        else:
+            if isinstance(input, (tuple, list)):
+                output = model(*input)
+            else:
+                output = model(input)
+            loss = criterion(output, target)
+        return loss
+
+    # Use to computation profiler
+    def parallel_forward(self, scatter_input, scatter_target):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion):
+            with torch.cuda.stream(stream):
+                loss = self._compute_forward_and_loss(model, criterion, input, target)
+                with lock:
+                    self.losses[index] = loss
+                    #self.trainer_print(f'loss at index: {index}: {self.losses[index]}', 'debug')
+
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx], self.criterion))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx], self.criterion)
+
+    # Use to computation profiler
+    def parallel_backward(self, scatter_input, scatter_target):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion, optimizer):
+            with torch.cuda.stream(stream):
+                loss = self.losses[index]
+                if not self.is_accum_mode:
+                    optimizer.zero_grad()
+                loss.backward(model_index=index)
+
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx],
+                                            self.criterion, self.local_optimizers[idx]))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx],
+                          self.criterion, self.local_optimizers[idx])
+
+    def parallel_compute(self, scatter_input, scatter_target, accum_step=-1):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+        if self.is_accum_mode and self.prepared_for_ddp:
+            self.seq_parallel_compute(scatter_input, scatter_target, accum_step)
+            return
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion, optimizer):
+            with torch.cuda.stream(stream):
+                #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | _local_worker() - idx: {index}')
+                loss = self._compute_forward_and_loss(model, criterion, input, target)
+                #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | after _compute_forward_and_loss - idx: {index}')
+                with lock:
+                    self.losses[index] = loss
+                    #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | loss at index: {index}: {self.losses[index]}')
+                if not self.is_accum_mode:
+                    optimizer.zero_grad()
+                #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | before loss.backward() - idx: {index}')
+                loss.backward(model_index=index)
+                #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | @@@@@@@@@@@@@@@@@@@@@@@@@ after loss.backward() - idx: {index}')
+
+        #print(f'[DEBUG] rank: {dist.get_rank()} | parallel_compute() - self.num_models: {self.num_models} | '
+        #      f'len(self.local_models): {len(self.local_models)} | '
+        #      f'len(self.model_streams): {len(self.model_streams)}')
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx],
+                                            self.criterion, self.local_optimizers[idx]))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx],
+                          self.criterion, self.local_optimizers[idx])
+
+    def seq_parallel_compute(self, scatter_input, scatter_target, accum_step=-1):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+        if self.max_accum_step <= 1 or accum_step < 0:
+            raise RuntimeError('If self.max_accum_step <= 1 or accum_step < 0, '
+                               'seq_parallel_compute() must not be called')
+
+        lock = threading.Lock()
+        def _local_accum_worker(index, model, stream, input, target, criterion):
+            #print(f'[_local_accum_worker] rank: {dist.get_rank()} | step: {self.sync_step} | model index: {index}')
+            with torch.cuda.stream(stream):
+                with model.no_sync():
+                    loss = self._compute_forward_and_loss(model, criterion, input, target)
+                    #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | after _compute_forward_and_loss - idx: {index}')
+                    with lock:
+                        self.losses[index] = loss
+                        #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | loss at index: {index}: {self.losses[index]}')
+                    loss.backward(model_index=index)
+                    #print(f'[_local_accum_worker] rank: {dist.get_rank()} | step: {self.sync_step} | model index: {index} | after backward()')
+
+        def _local_sync_worker(index, model, stream, input, target, criterion):
+            #print(f'[_local_sync_worker] rank: {dist.get_rank()} | step: {self.sync_step} | model index: {index}')
+            with torch.cuda.stream(stream):
+                loss = self._compute_forward_and_loss(model, criterion, input, target)
+                #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | after _compute_forward_and_loss - idx: {index}')
+                with lock:
+                    self.losses[index] = loss
+                    #print(f'[DEBUG] rank: {dist.get_rank()} | step: {self.sync_step} | loss at index: {index}: {self.losses[index]}')
+                self.sync_accum_barrier.wait()
+                loss.backward(model_index=index)
+                #print(f'[_local_sync_worker] rank: {dist.get_rank()} | step: {self.sync_step} | model index: {index} | after backward()')
+
+        if self.num_models > 1:
+            if accum_step < self.accum_step - 1:
+                #print(f'[TEST] _assert_equal_params() | accum_step: {accum_step} | self.accum_step: {self.accum_step}')
+                #self._assert_equal_params()
+                threads = []
+                for idx in range(self.num_models):
+                    threads.append(threading.Thread(target=_local_accum_worker,
+                                            args=(idx, self.local_models[idx], self.model_streams[idx],
+                                                scatter_input[idx], scatter_target[idx],
+                                                self.criterion))
+                                )
+                for thread in threads:
+                    thread.start()
+                for thread in threads:
+                    thread.join()
+            else:
+                #print(f'[TEST] _assert_equal_params() | accum_step: {accum_step} | self.accum_step: {self.accum_step}')
+                #self._assert_equal_params()
+                threads = []
+                for idx in range(self.num_models):
+                    threads.append(threading.Thread(target=_local_sync_worker,
+                                            args=(idx, self.local_models[idx], self.model_streams[idx],
+                                                scatter_input[idx], scatter_target[idx],
+                                                self.criterion))
+                                )
+                for thread in threads:
+                    thread.start()
+                for thread in threads:
+                    thread.join()
+        else:
+            idx = 0
+            if accum_step < self.accum_step - 1:
+                _local_accum_worker(idx, self.local_models[idx], self.model_streams[idx],
+                                    scatter_input[idx], scatter_target[idx],
+                                    self.criterion)
+            else:
+                _local_sync_worker(idx, self.local_models[idx], self.model_streams[idx],
+                                    scatter_input[idx], scatter_target[idx],
+                                    self.criterion)
+
+    def compute(self, data):
+        if self.is_accum_mode and self.prepared_for_ddp:
+            """
+            parallel_accum_inputs = [[data[i+self.num_models*j][0] for i in range(self.num_models)] for j in range(self.accum_step)]
+            parallel_accum_targets = [[data[i+self.num_models*j][1] for i in range(self.num_models)] for j in range(self.accum_step)]
+            with self.accum_processing():
+                for seq_step, (parallel_input, parallel_target) in \
+                        enumerate(zip(parallel_accum_inputs, parallel_accum_targets)):
+                    #print(f'[DEBUG] *****> seq step: {seq_step}')
+                    self.parallel_compute(parallel_input, parallel_target, seq_step)
+            """
+            parallel_input = [data[i][0] for i in range(self.num_models)]
+            parallel_target = [data[i][1] for i in range(self.num_models)]
+            with self.accum_processing():
+                self.parallel_compute(parallel_input, parallel_target, self.local_accum_step)
+        else:
+            parallel_input = [data[i][0] for i in range(self.num_models)]
+            parallel_target = [data[i][1] for i in range(self.num_models)]
+            self.parallel_compute(parallel_input, parallel_target)
+
+    def profile_parallel_forward(self, scatter_input, scatter_target):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+
+        fwd_start = torch.cuda.Event(enable_timing=True)
+        fwd_end = torch.cuda.Event(enable_timing=True)
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion):
+            with torch.cuda.stream(stream):
+                if index == 0:
+                    fwd_start.record()
+                loss = self._compute_forward_and_loss(model, criterion, input, target)
+                with lock:
+                    self.losses[index] = loss
+
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx], self.criterion))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx], self.criterion)
+        if idx == self.num_models-1:
+            fwd_end.record()
+        torch.cuda.synchronize()
+        return fwd_start.elapsed_time(fwd_end)
+
+    def profile_parallel_backward(self, scatter_input, scatter_target):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+
+        bwd_start = torch.cuda.Event(enable_timing=True)
+        bwd_end = torch.cuda.Event(enable_timing=True)
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion, optimizer):
+            with torch.cuda.stream(stream):
+                if index == 0:
+                    bwd_start.record()
+                loss = self.losses[index]
+                if not self.is_accum_mode:
+                    optimizer.zero_grad()
+                loss.backward(model_index=index)
+
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(threading.Thread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx],
+                                            self.criterion, self.local_optimizers[idx]))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx],
+                          self.criterion, self.local_optimizers[idx])
+        if idx == self.num_models-1:
+            bwd_end.record()
+        torch.cuda.synchronize()
+        return bwd_start.elapsed_time(bwd_end)
+
+    def profile_parallel_compute(self, scatter_input, scatter_target):
+        if scatter_input is None or scatter_target is None:
+            raise RuntimeError("scatter_input and scatter_target must be configured "
+                               "to arguments of parallel_compute()")
+        elif len(scatter_input) != self.num_models or len(scatter_target) != self.num_models:
+            raise RuntimeError(f"Length of scatter_input: {len(scatter_input)} "
+                               f"and scatter_target: {len(scatter_target)} "
+                               f"must be equal to "
+                               f"number of local models : {self.num_models}")
+
+        fwd_start = torch.cuda.Event(enable_timing=True)
+        fwd_end = torch.cuda.Event(enable_timing=True)
+        bwd_start = torch.cuda.Event(enable_timing=True)
+        bwd_end = torch.cuda.Event(enable_timing=True)
+
+        # reference: https://stackoverflow.com/questions/2829329/catch-a-threads-exception-in-the-caller-thread
+        class ParallelComputeThread(threading.Thread):
+            def run(self):
+                self._exc = None
+                try:
+                    super().run()
+                except Exception as e:
+                    self._exc = e
+
+            def join(self, timeout=None):
+                super().join(timeout=timeout)
+                if self._exc:
+                    raise self._exc
+
+        lock = threading.Lock()
+        def _local_worker(index, model, stream, input, target, criterion, optimizer):
+            if index == 0:
+                fwd_start.record()
+            with torch.cuda.stream(stream):
+                loss = self._compute_forward_and_loss(model, criterion, input, target)
+                with lock:
+                    self.losses[index] = loss
+                    #print(f'[DEBUG] loss at index: {index}: {self.losses[index]}')
+                if not self.is_accum_mode:
+                    optimizer.zero_grad()
+                if index == self.num_models-1:
+                    fwd_end.record()
+                if index == 0:
+                    bwd_start.record()
+                loss.backward(model_index=index)
+
+        if self.num_models > 1:
+            threads = []
+            for idx in range(self.num_models):
+                threads.append(ParallelComputeThread(target=_local_worker,
+                                        args=(idx, self.local_models[idx], self.model_streams[idx],
+                                            scatter_input[idx], scatter_target[idx],
+                                            self.criterion, self.local_optimizers[idx]))
+                            )
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                try:
+                    thread.join()
+                except RuntimeError as e:
+                    raise RuntimeError(e)
+        else:
+            idx = 0
+            _local_worker(idx, self.local_models[idx], self.model_streams[idx],
+                          scatter_input[idx], scatter_target[idx],
+                          self.criterion, self.local_optimizers[idx])
+        if idx == self.num_models-1:
+            bwd_end.record()
+        torch.cuda.synchronize()
+        return fwd_start.elapsed_time(fwd_end), bwd_start.elapsed_time(bwd_end)
+
+    def register_comm_hook(self, hook):
+        dummy_hook = torch.iidp.ddp_comm_hooks.dummy_hook
+
+        for model_idx in range(self.num_models):
+            if model_idx == 0:
+                self.hooks.append(hook)
+            else:
+                self.hooks.append(dummy_hook)
+        for local_model, hook in zip(self.local_models, self.hooks):
+            local_model.register_comm_hook(state=None, hook=hook)
+
+    def _create_optimizer_hook(self, hook):
+        def hook_rebuild_bucket_setup():
+            if self.main_model._has_rebuilt_buckets and not self.is_rebuilt_ddp_bucket_indices:
+                self.ddp_bucket_indices = self.main_model.bucket_indices
+                self._calculate_bucket_distribution()
+                #self.trainer_print(f'hook_rebuild_bucket_setup() - rebult ddp_bucket_indices: {self.ddp_bucket_indices}', 'debug')
+                self.is_rebuilt_ddp_bucket_indices = True
+
+        def hook_with_optimizer_step(state, bucket):
+            future_work = hook(state, bucket)
+            hook_rebuild_bucket_setup()
+            def optimizer_step(fut: torch.futures.Future):
+                bucket_index = bucket.get_index()
+                param_indices = self.ddp_bucket_indices[bucket_index]
+                nccl_stream = torch.cuda.current_stream()
+                self.optimizer_stream.wait_stream(nccl_stream)
+                with torch.cuda.stream(self.optimizer_stream):
+                    gradients = bucket.get_gradients()
+                    #self.trainer_print(f'gradients: {gradients}', 'debug')
+                    for index, grad in zip(param_indices, gradients):
+                        grad.index = index
+                    self._optimizer_step(gradients, param_indices)
+                return bucket.get_tensors()
+
+            return future_work.then(optimizer_step)
+        return hook_with_optimizer_step
+
+    def _prepare_overlap_optimizer_with_ddp(self):
+        hook = torch.iidp.ddp_comm_hooks.iidp_allreduce_hook
+        self._create_stream_for_optimizer()
+        self._prepare_hooks_for_local_models(hook)
+        self._check_overlap_with_ddp()
+        for i, (local_model, hook) in enumerate(zip(self.local_models, self.hooks)):
+            if i == 0:
+                state = torch.iidp.ddp_comm_hooks.IIDPState(None, self.total_num_models)
+                hook = self._create_optimizer_hook(hook)
+            else:
+                state = None
+            local_model.register_comm_hook(state=state, hook=hook)
+
+    def _optimizer_step(self, gradients, param_indices):
+        if not self.weight_sync_method == 'overlap':
+            raise RuntimeError("This function must be called if weight_sync_method is overlap")
+
+        self.main_optimizer.step(gradients)
+        # Partial weight copy
+        partial_src_params_to_copy = [self.main_model.ddp_register_params[i] for i in param_indices]
+        for idx in range(1, self.num_models):
+            partial_dst_params_to_copy = [self.local_models[idx].ddp_register_params[i] for i in param_indices]
+            for src_param, dst_param in \
+                    zip(partial_src_params_to_copy, partial_dst_params_to_copy):
+                dst_param.data.copy_(src_param.data)
+
+    def is_sync_step(self):
+        if self.is_accum_mode and self.local_accum_step < self.accum_step:
+            return False
+        else:
+            return True
+
+    def step(self):
+        if self.is_accum_mode and self.local_accum_step < self.accum_step:
+            # NOTE: Synchronize multi-stream before next computation
+            # to avoid RuntimeError: CUDA error: device-side assert triggered
+            torch.cuda.synchronize()
+            return False
+        if self.weight_sync_method == 'overlap':
+            if self.is_accum_mode:
+                with torch.cuda.stream(self.optimizer_stream):
+                    self.main_optimizer.zero_grad()
+                for idx in range(1, self.num_models):
+                    stream = self.model_streams[idx]
+                    optimizer = self.local_optimizers[idx]
+                    with torch.cuda.stream(stream):
+                        optimizer.zero_grad()
+            torch.cuda.synchronize()
+
+        elif self.weight_sync_method == 'sequential':
+            with torch.cuda.stream(self.main_stream):
+                self.main_optimizer.step()
+                if self.is_accum_mode:
+                    self.main_optimizer.zero_grad()
+            torch.cuda.synchronize()
+            for idx in range(1, self.num_models):
+                stream = self.model_streams[idx]
+                optimizer = self.local_optimizers[idx]
+                with torch.cuda.stream(stream):
+                    for src_param, dst_param in \
+                            zip(self.main_model.parameters(), self.local_models[idx].parameters()):
+                        dst_param.data.copy_(src_param.data)
+                    if self.is_accum_mode:
+                        optimizer.zero_grad()
+            torch.cuda.synchronize()
+        else:
+            raise RuntimeError(f'Not support weight_sync_method: {self.weight_sync_method}')
+        self.sync_step += 1
+        self.local_accum_step = 0
+        return True
+
+    def profile_step(self):
+        update_start = torch.cuda.Event(enable_timing=True)
+        update_end = torch.cuda.Event(enable_timing=True)
+        copy_start = torch.cuda.Event(enable_timing=True)
+        copy_end = torch.cuda.Event(enable_timing=True)
+        torch.cuda.synchronize()
+        with torch.cuda.stream(self.main_stream):
+            update_start.record()
+            self.main_optimizer.step()
+            update_end.record()
+        torch.cuda.synchronize()
+        if self.num_models == 1:
+            copy_start.record()
+        for idx in range(1, self.num_models):
+            stream = self.model_streams[idx]
+            with torch.cuda.stream(stream):
+                if idx == 1:
+                    copy_start.record()
+                for src_param, dst_param in \
+                        zip(self.main_model.parameters(), self.local_models[idx].parameters()):
+                    dst_param.data.copy_(src_param.data)
+        if self.num_models == 1 or idx == self.num_models-1:
+            copy_end.record()
+        torch.cuda.synchronize()
+        return update_start.elapsed_time(update_end), copy_start.elapsed_time(copy_end)
+
+    def scheduler_step(self):
+        if self.local_schedulers:
+            if self.weight_sync_method == 'overlap':
+                with torch.cuda.stream(self.optimizer_stream):
+                    self.main_scheduler.step()
+            elif self.weight_sync_method == 'sequential':
+                with torch.cuda.stream(self.main_stream):
+                    self.main_scheduler.step()
+            else:
+                raise RuntimeError(f'Not support weight_sync_method: {self.weight_sync_method}')
+
+
+class ElasticTrainTimer(object):
+    def __init__(self, start_time):
+        self.start_time = start_time
+        self.elapsed_time = 0
+
+    def update(self, measured_time):
+        self.elapsed_time = measured_time - self.start_time
+
+
+class AdaptiveIIDPTrainer(IIDPTrainer):
+    def __init__(self, gpu, local_batch_size, num_models, accum_step,
+                 weight_sync_method='recommend', adaptive_batch_params=None,
+                 checkpoint_dir=None, elastic_train_timer=None):
+        super().__init__(gpu, local_batch_size, num_models, accum_step, weight_sync_method)
+        self.data_loader = None
+        self.is_elastic_training = False
+        self.is_resource_reallocated = False
+        self._checkpoint_dir = checkpoint_dir
+        self._checkpoint_path = None
+        if self._checkpoint_dir is not None:
+            self.is_elastic_training = True
+            self._trainer_id = int(os.environ['IIDP_JOB_ID'])
+            self._local_rank = int(os.environ['IIDP_LOCAL_RANK'])
+            self._worker_id = int(os.environ['IIDP_WORKER_ID'])
+            self._sched_addr = os.environ['IIDP_SCHED_ADDR']
+            self._sched_port = int(os.environ['IIDP_SCHED_PORT'])
+            self._rpc_client = trainer_client.TrainerRpcClient(
+                    self._trainer_id, self._worker_id, self._sched_addr, self._sched_port)
+            if not os.path.isdir(self._checkpoint_dir):
+                raise ValueError(f'self._checkpoint_dir must be directory: {self._checkpoint_dir}')
+            self._checkpoint_path = os.path.join(self._checkpoint_dir, 'checkpoint.pth')
+
+            self.is_resource_reallocated = (os.path.exists(self._checkpoint_path) is True)
+            self.trainer_print(f'self.is_resource_reallocated: {self.is_resource_reallocated}', 'debug')
+            # NOTE: Include the below elapsed time
+            # 1) re-initialize and train components setup (model, optimizer, data loader, etc)
+            # 2) save & load checkpoint overhead
+            self.reallocation_overhead = -1
+            self.elastic_train_timer = elastic_train_timer
+
+        self.adaptive_batch_params = adaptive_batch_params
+        self._check_user_config_is_valid()
+        self._init_adaptive_batch_params()
+        # For IIDP dynamic configuration
+        self.prev_max_num_models = self.num_models
+        # NOTE: Even if elastic training is not configured, this is used for the system analysis
+        self.global_batch_size_trajectory = []
+        if self.is_elastic_training is True:
+            if os.getenv('NO_BATCH_SIZE_PREDICTION_EXP') == "1":
+                self.trainer_print('**********************************************', 'experimental')
+                self.trainer_print('Elastic training without batch size prediction', 'experimental')
+                self.trainer_print('**********************************************', 'experimental')
+            else:
+                self._prepare_global_batch_size_prediction()
+
+        self.total_overhead_dict = {
+            'dynamic config': 0,
+            'forecasting': 0,
+            'dp solver': {
+                'adaptive batching': 0,
+                'auto-scaling': 0
+            }
+        }
+        self.total_epoch_cost = 0
+
+    def prepare_stream_parallel(self, model, criterion, **kwargs):
+        if not kwargs.get('gradient_as_bucket_view'):
+            kwargs['gradient_as_bucket_view'] = True
+            self.trainer_print('gradient_as_bucket_view must be True')
+        super().prepare_stream_parallel(model, criterion, **kwargs)
+
+    def prepare_weight_sync_method(self, optimizer, scheduler=None, param_groups_func=None):
+        self._set_local_optimizers(optimizer, param_groups_func)
+        self._set_local_schedulers(scheduler)
+        self._prepare_gradient_based_metric()
+        if self.prepared_for_ddp:
+            if self.weight_sync_method == 'overlap':
+                self._prepare_overlap_optimizer_with_ddp()
+            if self.weight_sync_method == 'sequential':
+                self.register_comm_hook()
+        else:
+            raise RuntimeError("[ERROR] Without DDP, AdaptiveIIDPTrainer cannot work")
+
+    def prepare_adaptive_data_loader(self, data_loader):
+        if not isinstance(data_loader, torch.iidp.data.AdaptiveDataLoader):
+            raise ValueError(f'Only support torch.iidp.data.AdaptiveDataLoader, but {type(data_loader)}')
+        self.data_loader = data_loader
+
+    def _prepare_overlap_optimizer_with_ddp(self):
+        self._create_stream_for_optimizer()
+        self._check_overlap_with_ddp()
+        for i, (local_model, state, hook) in enumerate(zip(self.local_models, self.states, self.hooks)):
+            if i == 0:
+                hook = self._create_optimizer_hook(hook)
+            local_model.register_comm_hook(state=state, hook=hook)
+
+    def register_comm_hook(self):
+        #self.trainer_print(f'register_comm_hook() - self.states: {self.states} | self.hooks: {self.hooks}', 'debug')
+        for _, (local_model, state, hook) in enumerate(zip(self.local_models, self.states, self.hooks)):
+            local_model.register_comm_hook(state=state, hook=hook)
+
+    def prepare_adaptive_training(self):
+        # NOTE: Resource configuration is set up by the assumption of checkpoint-based restart
+        # TODO: self.available_server_name_list is given by elastic agent by registered GPU servers (workers)
+        if self.adaptive_batch_params["enable_adjust"] is True:
+            self.available_server_name_list = self.adaptive_batch_params["available_servers"]
+            self.cluster_manager = IIDPClusterManager(
+                    self.adaptive_batch_params["gpu_cluster_info"],
+                    self.available_server_name_list, self.gpu,
+                    homo_servers=self.adaptive_batch_params["homo_servers"],
+                    resource_alloc_unit=self.adaptive_batch_params["resource_alloc_unit"])
+        self._prepare_iidp_configurator()
+
+        if self.local_models == [] or self.local_optimizers == [] or self.data_loader is None:
+            raise ValueError(
+                f'Before calling prepare_adaptive_training(), model, optimizer and data loader must be configured')
+        if self.is_elastic_training is True:
+            self._prepare_checkpoint_based_restart()
+            # NOTE: Pre-build configuratos for all candidate global resource
+            self.future_configurator.prepare()
+
+    def _prepare_checkpoint_based_restart(self):
+        if self.is_elastic_training is False:
+            raise ValueError(
+                f'_prepare_checkpoint_based_restart() must be called if elf.is_elastic_training is True'
+            )
+        if self.is_resource_reallocated:
+            self.trainer_print(f'Load checkpoint: {self._checkpoint_path}')
+            self.load_checkpoint()
+
+        # If self.reallocation_overhead > 0 after loading checkpoint,
+        # it indicates the overhead has already been measured.
+        if self.reallocation_overhead < 0:
+            if dist.get_rank() == 0:
+                ckpt_dir = 'profile_ckpt_overhead_dir'
+                os.makedirs(ckpt_dir, exist_ok=True)
+                ckpt_file_path = os.path.join(ckpt_dir, 'checkpoint.pth')
+                self.save_checkpoint(ckpt_file_path)
+                self.load_checkpoint(ckpt_file_path)
+                os.system(f'rm -rf {ckpt_dir}')
+            dist.barrier()
+            self.elastic_train_timer.update(time.time())
+            self.reallocation_overhead = self.elastic_train_timer.elapsed_time
+            self.trainer_print(f'Reallocation overhead = {self.reallocation_overhead:.2f} sec')
+
+        self._rpc_client.init()
+
+    def _prepare_gradient_based_metric(self):
+        self.trainer_print('Prepare for adaptive training!')
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            self._prepare_cosine_similarity()
+        else: # TODO: suuport various metrics - e.g, GNS, Norm
+            raise ValueError(f'Not support other gradient-based metric except similarity: {self.adaptive_batch_params["metric"]}')
+
+    def _prepare_iidp_configurator(self):
+        # NOTE
+        # 1) Profile data dir must be placed on each local server even GPU type is same among another servers
+        # 2) Porfile data on all of local servers must be placed on every servers (e.g, NFS)
+        self.local_config = IIDPConfig(self.local_batch_size, self.num_models, self.accum_step, self.weight_sync_method)
+        if self.adaptive_batch_params["enable_adjust"] is True:
+            self.configurator = IIDPConfigurator(
+                self.adaptive_batch_params["comp_profile_dir"],
+                self.adaptive_batch_params["comm_profile_dir"],
+                self.adaptive_batch_params["bucket_profile_dir"],
+                self.adaptive_batch_params["memory_profile_dir"],
+                self.local_config,
+                self.cluster_manager.global_server_info,
+                self.adaptive_batch_params["batch_size_upper_bound"],
+                self.adaptive_batch_params["enable_adjust_lbs"],
+                self.gpu
+            )
+            self.print_initial_config()
+            if self.is_elastic_training is True:
+                self.future_configurator = IIDPFutureConfigurator(
+                    self.adaptive_batch_params["comp_profile_dir"],
+                    self.adaptive_batch_params["comm_profile_dir"],
+                    self.adaptive_batch_params["bucket_profile_dir"],
+                    self.adaptive_batch_params["memory_profile_dir"],
+                    self.local_config,
+                    self.cluster_manager.candidate_server_infos,
+                    self.adaptive_batch_params["batch_size_upper_bound"],
+                    self.adaptive_batch_params["enable_adjust_lbs"],
+                    self.gpu
+                )
+
+    def print_initial_config(self):
+        if self.epoch == 0 and self.is_resource_reallocated is False:
+            cluster_config_str = ''
+            for server_info in self.cluster_manager.global_server_info:
+                cluster_config_str += (server_info.__repr__() + '\n')
+            self.trainer_print(
+                f'\n====================== Initial configuration ======================\n'
+                f'-------------------------- Cluster --------------------------------\n'
+                f'{cluster_config_str}'
+                f'-------------------------------------------------------------------\n'
+                f'GBS: {self.global_batch_size} | LBS: {self.local_batch_size} | '
+                f'IIDP config: {self.configurator.iidp_config_map_in_cluster}\n'
+                f'===================================================================='
+            )
+            self.trainer_print(
+                f'\n========== Memory Profile Data Summary ==========\n'
+                f'   LBS\t|\tGPU\t|\tMax number of VSWs\n'
+                f'---------------------------------------------------\n'
+                f'{get_mem_profile_data_summary(self.adaptive_batch_params["memory_profile_dir"])}'
+                f'==================================================='
+            )
+
+    def _prepare_global_batch_size_prediction(self):
+        self.batch_size_model = None
+        if self.adaptive_batch_params["batch_size_predict_model"] == 'gaussian':
+            self.trainer_print('Global batch size prediction model = Gaussian Process Regression (GPR)')
+            self.batch_size_model = GaussianProcessRegressionModel()
+        elif self.adaptive_batch_params["batch_size_predict_model"] == 'exp_smoothing':
+            self.trainer_print('Global batch size prediction model = ExponentialSmoothing')
+            self.batch_size_model = ExponentialSmoothing()
+        elif self.adaptive_batch_params["batch_size_predict_model"] == 'ensemble':
+            self.trainer_print('Global batch size prediction model = Ensemble learning with ExponentialSmoothing + Gaussian Process Regression (GPR)')
+            models = [GaussianProcessRegressionModel(), ExponentialSmoothing()]
+            rates = [0.5, 0.5]
+            self.batch_size_model = EnsembleMethod(models, rates)
+        else:
+            raise ValueError(
+                f'Not support such batch size prediction model: '
+                f'{self.adaptive_batch_params["batch_size_predict_model"]}')
+
+    def _check_user_config_is_valid(self):
+        if self.epoch == 0 and self.is_resource_reallocated is False:
+            check_user_config_is_valid(self.adaptive_batch_params)
+
+    def _init_adaptive_batch_params(self):
+        if self.adaptive_batch_params is not None:
+            if os.path.isfile(self.adaptive_batch_params):
+                self.adaptive_batch_params = read_json(self.adaptive_batch_params)
+            else:
+                raise ValueError(f'Adaptive config param file: {self.adaptive_batch_params} must exist')
+            self.adaptive_batch_params=defaultdict(lambda: None, self.adaptive_batch_params)
+            if self.is_elastic_training is True:
+                if self.adaptive_batch_params["batch_size_lower_bound"] is None:
+                    raise ValueError(
+                        f'If is_elastic_training = True, adaptive_batch_params["batch_size_lower_bound"] must be configured')
+                else:
+                    if self.is_resource_reallocated is False:
+                        if self.global_batch_size < self.adaptive_batch_params["batch_size_lower_bound"]:
+                            raise ValueError(
+                                f'Within elastic training, initial global batch size: {self.global_batch_size} '
+                                f'must be < batch_size_lower_bound: {self.adaptive_batch_params["batch_size_lower_bound"]}'
+                            )
+                        self.adaptive_batch_params["original_batch_size"] = self.global_batch_size
+                if self.adaptive_batch_params["available_servers"] is None:
+                    raise ValueError(
+                        f'If is_elastic_training = True, adaptive_batch_params["available_servers"] must be configured')
+            else:
+                self.adaptive_batch_params["original_batch_size"] = self.global_batch_size
+                if self.adaptive_batch_params["batch_size_lower_bound"] is None:
+                    self.adaptive_batch_params["batch_size_lower_bound"] = self.adaptive_batch_params["original_batch_size"]
+                    self.trainer_print(f'batch_size_lower_bound is configured by initial global batch size: {self.global_batch_size}')
+            if self.adaptive_batch_params["batch_size_lower_bound"] is not None and self.adaptive_batch_params["batch_size_upper_bound"] is not None:
+                assert self.adaptive_batch_params["batch_size_upper_bound"]>=self.adaptive_batch_params["batch_size_lower_bound"]
+            self.adaptive_batch_params["global_lr_modifier"]=1.0
+            if self.adaptive_batch_params["enable_decrease_batch_size"] is None:
+                self.adaptive_batch_params["enable_decrease_batch_size"] = True
+            if self.adaptive_batch_params["enable_adjust"] is None:
+                self.adaptive_batch_params["enable_adjust"] = True
+            else:
+                if isinstance(self.adaptive_batch_params["enable_adjust"], str): # handle to parse from json file
+                    self.adaptive_batch_params["enable_adjust"] = bool(self.adaptive_batch_params["enable_adjust"] == "True")
+            if self.adaptive_batch_params["enable_adjust_lbs"] is None:
+                self.adaptive_batch_params["enable_adjust_lbs"] = True
+            else:
+                if isinstance(self.adaptive_batch_params["enable_adjust_lbs"], str): # handle to parse from json file
+                    self.adaptive_batch_params["enable_adjust_lbs"] = bool(self.adaptive_batch_params["enable_adjust_lbs"] == "True")
+            if self.adaptive_batch_params["verbose"] is None:
+                self.adaptive_batch_params["verbose"] = True
+            else:
+                if isinstance(self.adaptive_batch_params["verbose"], str): # handle to parse from json file
+                    self.adaptive_batch_params["verbose"] = bool(self.adaptive_batch_params["verbose"] == "True")
+            if self.adaptive_batch_params["metric"] is None:
+                self.adaptive_batch_params["metric"] = 'similarity'
+            # TODO: sub-group partitioning - default: horizontal
+            if self.adaptive_batch_params["subgroup_partitioning"] is None:
+                self.adaptive_batch_params["subgroup_partitioning"] = 'horizontal'
+            if self.adaptive_batch_params["batch_size_adjust_perc"] is None:
+                self.adaptive_batch_params["batch_size_adjust_perc"] = 0.1 # 10%
+            if self.adaptive_batch_params["batch_size_adjust_interval"] is None:
+                self.adaptive_batch_params["batch_size_adjust_interval"] = 100
+            else:
+                # handle to parse from json file
+                if isinstance(self.adaptive_batch_params["batch_size_adjust_interval"], str):
+                    self.adaptive_batch_params["batch_size_adjust_interval"] = int(self.adaptive_batch_params["batch_size_adjust_interval"])
+            if self.adaptive_batch_params["utility_threshold"] is None:
+                self.adaptive_batch_params["utility_threshold"] = 0
+            if self.adaptive_batch_params["available_servers"] is None:
+                self.adaptive_batch_params["available_servers"] = []
+            if self.adaptive_batch_params["batch_size_predict_model"] is None:
+                self.adaptive_batch_params["batch_size_predict_model"] = 'ensemble'
+        else:
+            raise ValueError(f'Adaptive config param must be configured')
+
+        self.trainer_print(f'Adaptive training parameters: {self.adaptive_batch_params}')
+
+    def save_checkpoint(self, checkpoint_file_path=None):
+        # NOTE: If checkpoint_file_path is configured, this is for testing checkpoint
+        if checkpoint_file_path:
+            save_ckpt_path = checkpoint_file_path
+            epoch = -1 # NOTE: epoch idx starts from -1, refer to [EpochIterator]
+        else:
+            if not os.path.exists(self._checkpoint_dir):
+                os.makedirs(self._checkpoint_dir)
+                self.trainer_print(f'Make a checkpoint dir: {self._checkpoint_dir}')
+            save_ckpt_path = self._checkpoint_path
+            epoch = self.epoch
+        self.trainer_print(f'Save checkpoint path: {save_ckpt_path}')
+        self.trainer_print(f'Saved epoch: {self.epoch}')
+        if self.local_schedulers:
+            scheduler_state = self.main_scheduler.state_dict()
+            # Scheduler for adaptive training may need data loader's get_progress()
+            if scheduler_state.get('data_loader'):
+                scheduler_state.pop('data_loader')
+        else:
+            scheduler_state = None
+        trainer_state_dict = {
+            'epoch': epoch,
+            'total_epoch_time': self.total_epoch_time,
+            'total_epoch_cost': self.total_epoch_cost,
+            'step': self.sync_step,
+            'gbs_trajectory': self.global_batch_size_trajectory,
+            'model' : self.main_model.module.state_dict(),
+            'optimizer'  : self.main_optimizer.state_dict(),
+            'scheduler'  : scheduler_state,
+            'data': self.data_loader.state_dict()
+        }
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            trainer_state_dict['simigrad_step'] = self.simigrad_state.step
+            trainer_state_dict['global_lr_modifier'] = self.adaptive_batch_params["global_lr_modifier"]
+        trainer_state_dict['total_overhead_dict'] = self.total_overhead_dict
+        if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+            trainer_state_dict['simigrad_interval'] = self.simigrad_state.interval
+        if self.is_elastic_training is True:
+            trainer_state_dict['initial_global_batch_size'] = self.adaptive_batch_params["original_batch_size"]
+            trainer_state_dict['reallocation_overhead'] = self.reallocation_overhead
+            trainer_state_dict['future_configurator'] = self.future_configurator.state_dict()
+            if os.getenv('NO_BATCH_SIZE_PREDICTION_EXP') == "1":
+                self.trainer_print('No need to save checkpoint of batch size prediction model', 'experimental')
+            else:
+                self.batch_size_model.save(self._checkpoint_dir)
+
+        torch.save(trainer_state_dict, save_ckpt_path)
+
+    def load_checkpoint(self, checkpoint_file_path=None):
+        # NOTE: If checkpoint_file_path is configured, this is for testing checkpoint
+        if checkpoint_file_path:
+            load_ckpt_path = checkpoint_file_path
+        else:
+            load_ckpt_path = self._checkpoint_path
+        self.trainer_print(f'Load checkpoint path: {load_ckpt_path}')
+        loc = 'cuda:{}'.format(self.gpu) if type(self.gpu) == int else self.gpu
+        checkpoint = torch.load(load_ckpt_path, map_location=loc)
+        for local_model in self.local_models:
+            local_model.module.load_state_dict(checkpoint['model'])
+        for local_optimizer in self.local_optimizers:
+            local_optimizer.load_state_dict(checkpoint['optimizer'])
+        for local_scheduler in self.local_schedulers:
+            local_scheduler.load_state_dict(checkpoint['scheduler'])
+        self.data_loader.load_state_dict(checkpoint['data'])
+        self.epoch = checkpoint['epoch']
+        self.total_epoch_time = checkpoint['total_epoch_time']
+        self.total_epoch_cost = checkpoint['total_epoch_cost']
+        self.sync_step = checkpoint['step']
+        self.global_batch_size_trajectory = checkpoint['gbs_trajectory']
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            self.simigrad_state.step = checkpoint['simigrad_step']
+            self.adaptive_batch_params["global_lr_modifier"] = checkpoint['global_lr_modifier']
+        self.total_overhead_dict = checkpoint['total_overhead_dict']
+        if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+            self.simigrad_state.interval = checkpoint['simigrad_interval']
+        if self.is_elastic_training is True:
+            if 'initial_global_batch_size' in checkpoint.keys():
+                self.adaptive_batch_params["original_batch_size"] = checkpoint['initial_global_batch_size']
+            self.reallocation_overhead = checkpoint['reallocation_overhead']
+            self.future_configurator.all_candidate_server_configurators = checkpoint['future_configurator']
+            if os.getenv('NO_BATCH_SIZE_PREDICTION_EXP') == "1":
+                self.trainer_print('No need to load checkpoint of batch size prediction model', 'experimental')
+            else:
+                self.batch_size_model.load(self._checkpoint_dir)
+
+        self.trainer_print(f'Loaded epoch: {checkpoint["epoch"]+1} | iterations: {self.sync_step}')
+
+    def _build_vertical_subgroup(self): # TODO
+        num_gpus_in_server = torch.cuda.device_count()
+        total_num_gpus = dist.get_world_size()
+        tensor_list = [
+            torch.tensor([0], dtype=torch.float32).to(self.gpu) for _ in range(total_num_gpus)
+        ]
+        tensor = torch.tensor([self.num_models * self.accum_step], dtype=torch.float32).to(self.gpu)
+        dist.all_gather(tensor_list, tensor)
+        num_models_per_gpu =  [int(tensor.item()) for tensor in tensor_list]
+        num_models_per_server = num_models_per_gpu[::num_gpus_in_server]
+        self.trainer_print(f'_prepare_cosine_similarity() - num_gpus_in_server: {num_gpus_in_server}', 'debug')
+        self.trainer_print(f'_prepare_cosine_similarity() - num_models_per_server: {num_models_per_server}', 'debug')
+        assert self.total_num_models % 2 == 0, \
+            '[ERROR] To support cosine similarity, total number of models is even'
+        total_num_models_in_subgroup = self.total_num_models / 2
+
+    def _build_horizontal_subgroup(self):
+        # Original simigrad group-making -> horizontal slice
+        # Advantage: simple and robust for various (vsw, ga) configurations
+        assert torch.distributed.get_world_size() % 2 == 0
+        for i in range(torch.distributed.get_world_size()):
+            self.sub_groups_idx[i%2].append(i)
+
+    def _prepare_cosine_similarity(self):
+        #self.trainer_print('_prepare_cosine_similarity()', 'debug')
+        num_sub_groups = 2 # SimiGrad builds two all-reduce sub-groups
+        self.sub_groups_idx = [[] for _ in range(num_sub_groups)]
+        self.sub_groups = []
+
+        if self.adaptive_batch_params["subgroup_partitioning"] == 'vertical':
+            """
+            try:
+                self._build_vertical_subgroup()
+            except:
+                self.trainer_print('Partitioning sub-group vertically is impossible! => Horizontal way', 'warning')
+                self._build_horizontal_subgroup()
+            """
+            raise NotImplementedError('[TODO] Not support subgroup_partitioning == vetical')
+        else:
+            self._build_horizontal_subgroup()
+        self.trainer_print(f'sub group id: {self.sub_groups_idx}')
+        assert len(self.sub_groups_idx) == 2
+        for idx in self.sub_groups_idx:
+            self.sub_groups.append(torch.distributed.new_group(idx))
+        self.first_subgroup_src_rank, self.second_subgroup_src_rank = self.sub_groups_idx[0][0], self.sub_groups_idx[1][0]
+        # To compare gradients of the representative rank in each sub-group => used in compute_cosine_similarity()
+        self.sub_groups.append(torch.distributed.new_group([self.first_subgroup_src_rank, self.second_subgroup_src_rank]))
+
+        self.grad_placeholders = [[] for _ in range(num_sub_groups)]
+        self.cos_placeholder = torch.rand(1).to(self.gpu)
+
+        self._prepare_simigrad_allreduce_hooks()
+
+    def _prepare_simigrad_allreduce_hooks(self):
+        self.states, self.hooks = [], []
+        #self.trainer_print('_prepare_simigrad_allreduce_hooks()', 'debug')
+        self.simigrad_state = torch.iidp.ddp_comm_hooks.SimiGradState(
+            dist.group.WORLD, self.total_num_models,
+            self.sub_groups[dist.get_rank()%2], self.grad_placeholders[dist.get_rank()%2],
+            self.adaptive_batch_params["batch_size_adjust_interval"]
+        )
+        subgroup_allreduce_hook = torch.iidp.ddp_comm_hooks.subgroup_allreduce_hook
+        main_hook = torch.iidp.ddp_comm_hooks.simigrad_allreduce_hook(subgroup_allreduce_hook)
+        dummy_hook = torch.iidp.ddp_comm_hooks.dummy_hook
+        for i in range(self.num_models):
+            if i == 0:
+                self.states.append(self.simigrad_state)
+                self.hooks.append(main_hook)
+            else:
+                self.states.append(None)
+                self.hooks.append(dummy_hook)
+
+    def compute_cosine_similarity(self):
+        if dist.get_rank() == self.first_subgroup_src_rank or dist.get_rank() == self.second_subgroup_src_rank:
+            #print(f'rank: {dist.get_rank()} [DEBUG] compute_cosine_similarity() - self.simigrad_state.grad_placeholder: {self.simigrad_state.grad_placeholder}')
+            self.allgather_grad_placeholders = [
+                torch.cat([torch.zeros_like(grad) for grad in self.simigrad_state.grad_placeholder]) for _ in range(2)
+            ]
+            grad_placeholder = torch.cat([grad for grad in self.simigrad_state.grad_placeholder])
+            dist.all_gather(self.allgather_grad_placeholders, grad_placeholder, group=self.sub_groups[-1])
+            if dist.get_rank() == self.first_subgroup_src_rank:
+                self.cos_placeholder = torch.nn.functional.cosine_similarity(self.allgather_grad_placeholders[0], self.allgather_grad_placeholders[1], dim=0)
+        dist.broadcast(self.cos_placeholder, self.first_subgroup_src_rank)
+        self.trainer_print(f"cosine similarity: {self.cos_placeholder}")
+        self.simigrad_state.grad_placeholder = []
+        # NOTE: Memory deallocation when number of VSWs changes by change_local_models_state()
+        if dist.get_rank() == self.first_subgroup_src_rank or dist.get_rank() == self.second_subgroup_src_rank:
+            del self.allgather_grad_placeholders
+        """
+        gc.collect()
+        with torch.no_grad():
+            torch.cuda.empty_cache()
+        """
+
+    def change_local_models_state(self, adjust_num_models_diff):
+        """
+        torch.cuda.synchronize()
+        gc.collect()
+        with torch.no_grad():
+            torch.cuda.empty_cache()
+        torch.cuda.synchronize()
+        """
+        if self.adaptive_batch_params["enable_adjust_lbs"] is False:
+            self.change_local_models_state_by_reuse(adjust_num_models_diff)
+        else: # dynamic LBS
+            """
+            print(f'================= start of change model state =====================')
+            print(f'[DEBUG] memory (MB): {round(torch.cuda.memory_allocated()/(1024*1024))}')
+            print(f'[DEBUG] cached memory (MB): {round(torch.cuda.memory_cached()/(1024*1024))}')
+            print(f'[DEBUG] max memory (MB): {round(torch.cuda.max_memory_allocated()/(1024*1024))}')
+            """
+            if adjust_num_models_diff > 0:
+                #start_time = time.time()
+                gc.collect()
+                with torch.no_grad():
+                    torch.cuda.empty_cache()
+                torch.cuda.synchronize()
+                #print(f'[DEBUG][trainer.py]  **************************** adjust_num_models_diff > 0')
+                # Create new components for training
+                # 1) stream
+                #if self.num_models > len(self.model_streams):
+                #    for _ in range(len(self.model_streams), self.num_models):
+                #        self.model_streams.append(torch.cuda.Stream())
+                #for _ in range(self.num_models):
+                #    self.model_streams.append(torch.cuda.Stream())
+                for _ in range(adjust_num_models_diff):
+                    # 1) stream
+                    self.model_streams.append(torch.cuda.Stream())
+                    # 2) model
+                    copied_model = copy.deepcopy(self.main_model.module)
+                    self.original_local_models.append(copied_model)
+                    # 3) optimizer
+                    # For .zero_grad(), optimizer should be added
+                    # TODO: remove optimizer except main model
+                    cls = type(self.main_optimizer.__class__.__name__, (self.main_optimizer.__class__,), dict(self.main_optimizer.__dict__))
+                    if self.param_groups_func:
+                        params = self.param_groups_func(copied_model)
+                    else:
+                        params = copied_model.parameters()
+                    args = self._get_required_args_value(self.main_optimizer)
+                    copied_optimizer = cls(params, lr=self.main_optimizer.defaults['lr'], *args)
+                    copied_optimizer.load_state_dict(self.main_optimizer.state_dict())
+                    copied_optimizer.zero_grad()
+                    self.local_optimizers.append(copied_optimizer)
+                find_unused_parameters = self.main_model.find_unused_parameters
+                gradient_as_bucket_view = self.main_model.gradient_as_bucket_view
+                for idx in range(self.prev_num_models, self.num_models):
+                    with torch.cuda.stream(self.model_streams[idx]):
+                        local_ddp_module = torch.nn.parallel.DistributedDataParallel(
+                            self.original_local_models[idx], device_ids=[self.gpu], output_device=[self.gpu],
+                            find_unused_parameters=find_unused_parameters,
+                            gradient_as_bucket_view=gradient_as_bucket_view,
+                            model_index=idx, num_local_models=self.num_models,
+                            total_num_models=self.total_num_models,
+                            sync_buffer_barrier=self._sync_buffer_barrier)
+                        if self.main_model._has_rebuilt_buckets:
+                            local_ddp_module.reducer.initialize_buckets(self.main_model.bucket_indices)
+                            local_ddp_module._has_rebuilt_buckets = True
+                        self.local_models.append(local_ddp_module)
+                assert (len(self.local_models) == self.num_models) and (len(self.local_optimizers) == self.num_models)
+                for i in range(self.num_models):
+                    self.local_models[i].reconfigure(self.num_models, self.total_num_models, self._sync_buffer_barrier)
+                # Synchornize previous models
+                for i in range(self.prev_num_models, self.num_models):
+                    with torch.cuda.stream(self.model_streams[i]):
+                        for src_param, dst_param in \
+                                zip(self.main_model.parameters(), self.local_models[i].parameters()):
+                            dst_param.data.copy_(src_param.data)
+
+                # hook - total num models
+                assert self.total_num_models % 2 == 0
+                dummy_hook = torch.iidp.ddp_comm_hooks.dummy_hook
+                for i in range(self.prev_num_models, self.num_models):
+                    self.states.append(None)
+                    self.hooks.append(dummy_hook)
+                    self.local_models[i].register_comm_hook(state=None, hook=dummy_hook)
+                self.states[0].total_num_models = self.total_num_models
+                self.states[0].subgroup_total_num_models = self.total_num_models / 2
+                #print(f'[DEBUG] change_local_models_state() - rank: {dist.get_rank()} | Increase VSW overhead: {time.time()-start_time:.3f} sec')
+            else:
+                # Remove unused streams, models and optimizers
+                """
+                print(f'[DEBUG] self.num_models: {self.num_models} | '
+                    f'self.prev_num_models: {self.prev_num_models} | '
+                    f'adjust_num_models_diff: {adjust_num_models_diff} | '
+                    f'len(self.local_models): {len(self.local_models)} | '
+                    f'len(self.original_local_models): {len(self.original_local_models)} | '
+                    f'len(self.local_optimizers): {len(self.local_optimizers)} | '
+                    f'len(self.model_streams): {len(self.model_streams)}')
+                """
+                #start_time = time.time()
+                for _ in range(self.num_models, self.prev_num_models):
+                    # NOTE: Moving models to CPU tensor and removing it enables GPU memory to be decreased
+                    # reference: https://discuss.pytorch.org/t/deleting-tensors-in-context-save-for-backward/122917/11
+                    self.local_models[-1].zero_grad(set_to_none=True)
+                    self.local_models[-1].cpu()
+                    self.original_local_models[-1].zero_grad(set_to_none=True)
+                    self.original_local_models[-1].cpu()
+                    self.local_optimizers[-1].zero_grad(set_to_none=True)
+                    del self.local_models[-1]
+                    del self.original_local_models[-1]
+                    del self.local_optimizers[-1]
+                    del self.model_streams[-1]
+                    del self.states[-1]
+                    del self.hooks[-1]
+                assert (len(self.local_models) == self.num_models) and (len(self.local_optimizers) == self.num_models)
+                for i in range(self.num_models):
+                    self.local_models[i].reconfigure(self.num_models, self.total_num_models, self._sync_buffer_barrier)
+                if adjust_num_models_diff < 0:
+                    gc.collect()
+                    with torch.no_grad():
+                        torch.cuda.empty_cache()
+                # hook - total num models
+                assert self.total_num_models % 2 == 0
+                self.states[0].total_num_models = self.total_num_models
+                self.states[0].subgroup_total_num_models = self.total_num_models / 2
+                #print(f'[DEBUG] change_local_models_state() - rank: {dist.get_rank()} | Decrease or Remain VSW overhead: {time.time()-start_time:.3f} sec')
+            if self.is_accum_mode:
+                for i in range(self.num_models):
+                    with torch.cuda.stream(self.model_streams[i]):
+                        self.local_optimizers[i].zero_grad()
+            """
+            torch.cuda.synchronize()
+            gc.collect()
+            with torch.no_grad():
+                torch.cuda.empty_cache()
+            torch.cuda.synchronize()
+            """
+            """
+            print(f'================= end of change model state =====================')
+            print(f'[DEBUG] memory (MB): {round(torch.cuda.memory_allocated()/(1024*1024))}')
+            print(f'[DEBUG] cached memory (MB): {round(torch.cuda.memory_cached()/(1024*1024))}')
+            print(f'[DEBUG] max memory (MB): {round(torch.cuda.max_memory_allocated()/(1024*1024))}')
+            print(torch.cuda.memory_summary())
+            #print(torch.cuda.memory_snapshot())
+            print('sleep 5 sec ..')
+            time.sleep(5)
+            """
+            """ TEST code
+            for idx in range(1, self.num_models):
+                for i, j in zip(self.main_model.parameters(), self.local_models[idx].parameters()):
+                    self.assert_equal(i, j)
+                    if self.is_accum_mode:
+                        i_grad = i.grad
+                        j_grad = j.grad
+                        if i_grad is not None and j_grad is not None:
+                            self.assert_equal(i_grad, j_grad)
+                    assert i.data.ne(j.data).sum() == 0, \
+                        f"rank {dist.get_rank()}: local model {idx} does not share same parameter with main model"
+            if adjust_num_models_diff > 0:
+               assert self.num_models == len(self.hooks), \
+                   f"when adjust_num_models_diff > 0, self.num_models == len(self.hooks), but {self.num_models} != {len(self.hooks)}"
+            print(f'end of change_local_models_state() '
+                  f'rank: {dist.get_rank()} | '
+                  f'self.num_models: {self.num_models} | '
+                  f'len(self.local_models): {len(self.local_models)} | '
+                  f'len(self.model_streams): {len(self.model_streams)}', 'debug')
+            """
+
+    def change_local_models_state_by_reuse(self, adjust_num_models_diff):
+        if adjust_num_models_diff > 0:
+            if self.num_models > self.prev_max_num_models: # To reuse previous models
+                actual_adjust_num_models_diff = self.num_models - self.prev_max_num_models
+                """
+                self.trainer_print(f'change_local_models_state() if adjust_num_models_diff > 0 '
+                      f'rank: {dist.get_rank()} | self.num_models: {self.num_models} | '
+                      f'actual_adjust_num_models_diff: {actual_adjust_num_models_diff}', 'debug')
+                """
+                # Create new streams for model
+                for _ in range(actual_adjust_num_models_diff):
+                    self.model_streams.append(torch.cuda.Stream())
+                # Create new models
+                for _ in range(actual_adjust_num_models_diff):
+                    copied_model = copy.deepcopy(self.main_model.module)
+                    self.original_local_models.append(copied_model)
+                    # For .zero_grad(), optimizer should be added
+                    # TODO: remove optimizer except main model
+                    cls = type(self.main_optimizer.__class__.__name__, (self.main_optimizer.__class__,), dict(self.main_optimizer.__dict__))
+                    if self.param_groups_func:
+                        params = self.param_groups_func(copied_model)
+                    else:
+                        params = copied_model.parameters()
+                    args = self._get_required_args_value(self.main_optimizer)
+                    copied_optimizer = cls(params, lr=self.main_optimizer.defaults['lr'], *args)
+                    copied_optimizer.load_state_dict(self.main_optimizer.state_dict())
+                    copied_optimizer.zero_grad()
+                    self.local_optimizers.append(copied_optimizer)
+                find_unused_parameters = self.main_model.find_unused_parameters
+                gradient_as_bucket_view = self.main_model.gradient_as_bucket_view
+                for idx in range(self.prev_max_num_models, self.num_models):
+                    with torch.cuda.stream(self.model_streams[idx]):
+                        local_ddp_module = torch.nn.parallel.DistributedDataParallel(
+                            self.original_local_models[idx], device_ids=[self.gpu], output_device=[self.gpu],
+                            find_unused_parameters=find_unused_parameters,
+                            gradient_as_bucket_view=gradient_as_bucket_view,
+                            model_index=idx, num_local_models=self.num_models,
+                            total_num_models=self.total_num_models,
+                            sync_buffer_barrier=self._sync_buffer_barrier)
+                        if self.main_model._has_rebuilt_buckets:
+                            local_ddp_module.reducer.initialize_buckets(self.main_model.bucket_indices)
+                            local_ddp_module._has_rebuilt_buckets = True
+                        self.local_models.append(local_ddp_module)
+                for i in range(self.prev_max_num_models):
+                    self.local_models[i].reconfigure(self.num_models, self.total_num_models, self._sync_buffer_barrier)
+                # Synchornize previous models
+                for i in range(self.prev_num_models, self.num_models):
+                    with torch.cuda.stream(self.model_streams[i]):
+                        for src_param, dst_param in \
+                                zip(self.main_model.parameters(), self.local_models[i].parameters()):
+                            dst_param.data.copy_(src_param.data)
+                self.prev_max_num_models = self.num_models
+            else:
+                for i in range(self.num_models):
+                    self.local_models[i].reconfigure(self.num_models, self.total_num_models, self._sync_buffer_barrier)
+                # Synchornize previous models
+                for i in range(self.prev_num_models, self.num_models):
+                    with torch.cuda.stream(self.model_streams[i]):
+                        for src_param, dst_param in \
+                                zip(self.main_model.parameters(), self.local_models[i].parameters()):
+                            dst_param.data.copy_(src_param.data)
+
+            # hook - total num models
+            assert self.total_num_models % 2 == 0
+            dummy_hook = torch.iidp.ddp_comm_hooks.dummy_hook
+            if self.num_models > len(self.hooks):
+                for i in range(len(self.hooks), self.num_models):
+                    self.states.append(None)
+                    self.hooks.append(dummy_hook)
+                    self.local_models[i].register_comm_hook(state=None, hook=dummy_hook)
+            self.states[0].total_num_models = self.total_num_models
+            self.states[0].subgroup_total_num_models = self.total_num_models / 2
+        else:
+            for i in range(self.num_models):
+                self.local_models[i].reconfigure(self.num_models, self.total_num_models, self._sync_buffer_barrier)
+            # hook - total num models
+            assert self.total_num_models % 2 == 0
+            self.states[0].total_num_models = self.total_num_models
+            self.states[0].subgroup_total_num_models = self.total_num_models / 2
+        if self.is_accum_mode:
+            for i in range(self.num_models):
+                with torch.cuda.stream(self.model_streams[i]):
+                    self.local_optimizers[i].zero_grad()
+        torch.cuda.synchronize()
+        """ TEST code
+        for idx in range(1, self.num_models):
+            for i, j in zip(self.main_model.parameters(), self.local_models[idx].parameters()):
+                self.assert_equal(i, j)
+                if self.is_accum_mode:
+                    i_grad = i.grad
+                    j_grad = j.grad
+                    if i_grad is not None and j_grad is not None:
+                        self.assert_equal(i_grad, j_grad)
+                assert i.data.ne(j.data).sum() == 0, \
+                    f"rank {dist.get_rank()}: local model {idx} does not share same parameter with main model"
+        # if adjust_num_models_diff > 0:
+        #   assert self.num_models == len(self.hooks), \
+        #       f"when adjust_num_models_diff > 0, self.num_models == len(self.hooks), but {self.num_models} != {len(self.hooks)}"
+        """
+        """
+        self.trainer_print(f'end of change_local_models_state() '
+              f'rank: {dist.get_rank()} | '
+              f'self.num_models: {self.num_models} | '
+              f'len(self.local_models): {len(self.local_models)} | '
+              f'len(self.model_streams): {len(self.model_streams)}', 'debug')
+        """
+
+    def assert_equal(self, tensor1, tensor2):
+        if not isinstance(tensor1, torch.Tensor) or not isinstance(tensor2, torch.Tensor):
+            raise TypeError(
+                f'Both tensor1: {tensor1} and tensor2: {tensor2} must be torch.tensor')
+        if not torch.equal(tensor1, tensor2):
+            if dist.get_rank() == 0:
+                print(f'****************** {self.__class__} | {self.weight_sync_method} test fail! ******************')
+            raise ValueError(
+                f'[ERROR] Equal test failed - tensor1: {tensor1[0][0]} | tensor2: {tensor2[0][0]}')
+
+    def change_local_trainer_state(self, new_num_models, new_accum_step):
+        """This method is called by only selective rank (GPU)"""
+        self.prev_num_models = self.num_models
+        if new_num_models != 0:
+            self.num_models = self.prev_num_models + new_num_models
+            #print(f'[DEBUG][{self.__class__.__name__}] change_local_trainer_state() | rank: {dist.get_rank()} | self.num_models: {self.num_models}')
+            assert self.num_models > 0, f"self.num_models must be > 0"
+            # Used in seq_parallel_compute() for being block different number of VSWs on inter-node
+            self.sync_accum_barrier = threading.Barrier(self.num_models)
+            # It is used for _sync_params() in torch/nn/parallel/distributed.py
+            self._sync_buffer_barrier = [None, None]
+            if self.num_models > 1:
+                self._sync_buffer_barrier = [threading.Barrier(self.num_models) for _ in range(2)]
+
+        if new_accum_step != 0:
+            self.accum_step = self.accum_step + new_accum_step
+
+        # Data loading
+        self.batch_size_per_gpu = self.local_batch_size * self.num_models
+        print(f'[INFO][{self.__class__.__name__}] rank: {dist.get_rank()} | self.num_models: {self.num_models} | self.accum_step: {self.accum_step}')
+        print(f'[INFO][{self.__class__.__name__}] rank: {dist.get_rank()} | batch size per GPU: {self.batch_size_per_gpu}')
+        self.data_loader.update_local_state(self.batch_size_per_gpu, self.num_models, self.accum_step)
+
+    def change_global_trainer_state(self, solved_iidp_config_map):
+        self._get_total_num_models()
+        assert self.global_batch_size == self.local_batch_size * self.total_num_models, \
+            f"GBS: {self.global_batch_size} | LBS: {self.local_batch_size} | " \
+            f"total num models: {self.total_num_models} | " \
+            f"rank: {dist.get_rank()} - num_models: {self.num_models} | accum_step: {self.accum_step} | " \
+            f"self.configurator.iidp_config_map_in_cluster  {self.configurator.iidp_config_map_in_cluster} | " \
+            f"solved_iidp_config_map: {solved_iidp_config_map} " \
+            f"=> If solved_iidp_config_map is different among rank, please check config JSON file"
+
+        self._get_all_accum_step_in_process_group()
+        self.max_accum_step = max(self.all_accum_step_in_process_group)
+        self.is_accum_mode = True if self.max_accum_step > 1 else False
+
+        self._get_all_partition_size_in_process_group()
+        self.data_loader.update_global_state(
+                self.global_batch_size, self.all_partition_size_in_process_group)
+
+    def change_configuration_for_iidp(self, solved_iidp_config_map):
+        if len(solved_iidp_config_map) == 0:
+            # NOTE: Even though solved_iidp_config_map is empty, local batch size can be changed.
+            self.batch_size_per_gpu = self.local_batch_size * self.num_models
+            print(f'[INFO][{self.__class__.__name__}] rank: {dist.get_rank()} | self.num_models: {self.num_models} | self.accum_step: {self.accum_step}')
+            print(f'[INFO][{self.__class__.__name__}] rank: {dist.get_rank()} | batch size per GPU: {self.batch_size_per_gpu}')
+            self.data_loader.update_local_state(self.batch_size_per_gpu, self.num_models, self.accum_step)
+            self._set_trainer_state()
+            dist.barrier()
+            return
+        # == step 1) Change local trainer state ==
+        # 1-1) numer of local models, 1-2) accum step, 1-3) batch size per GPU
+        rank = dist.get_rank()
+        if rank in solved_iidp_config_map:
+            new_num_models, new_accum_step = solved_iidp_config_map[rank]
+            self.change_local_trainer_state(new_num_models, new_accum_step)
+        else:
+            new_num_models, new_accum_step = 0, 0
+            self.change_local_trainer_state(new_num_models, new_accum_step)
+        dist.barrier()
+        # == step 2) Change global trainer state ==
+        # 2-1) total number of models in process group
+        # 2-2) all accum step in process group -> determine is_accum_mode
+        # 2-3) data loader state
+        self.change_global_trainer_state(solved_iidp_config_map)
+        self._set_trainer_state()
+        # == step 3) Change local VSW state ==
+        # 3-1) Change (create / remove) streams, models and optimizers
+        # 3-2) Change communication hook state of local models
+        # NOTE [IMPORTANT] Even new number of models is zero,
+        # 3-2) must be done in change_local_models_state()
+        self.change_local_models_state(new_num_models)
+        if len(solved_iidp_config_map) > 0:
+            self.configurator.update()
+        dist.barrier()
+
+    def change_batch_size_for_iidp(self, new_global_batch_size_by_simigrad):
+        solved_iidp_config_map = {} # return
+        if self.adaptive_batch_params["batch_size_upper_bound"] is not None:
+            new_global_batch_size_by_simigrad = min(new_global_batch_size_by_simigrad, self.adaptive_batch_params["batch_size_upper_bound"])
+        if self.adaptive_batch_params["batch_size_lower_bound"] is not None:
+            new_global_batch_size_by_simigrad = max(new_global_batch_size_by_simigrad, self.adaptive_batch_params["batch_size_lower_bound"])
+
+        if self.global_batch_size == new_global_batch_size_by_simigrad:
+            return solved_iidp_config_map
+
+        self.trainer_print(f'new_global_batch_size_by_simigrad: {new_global_batch_size_by_simigrad}', 'debug')
+
+        with Timer(f'[INFO][{self.__class__.__name__}] DP solver overhead') as dp_solver_timer:
+            solved_iidp_config_map, new_local_batch_size, new_global_batch_size = \
+                    self.configurator.solve_placement(
+                        new_global_batch_size_by_simigrad,
+                        self.global_batch_size
+                    )
+        self.total_overhead_dict['dp solver']['adaptive batching'] += dp_solver_timer.elapsed
+        self.trainer_print(f'solved_iidp_config_map: {solved_iidp_config_map}', 'debug')
+        self.trainer_print(f'new_local_batch_size: {new_local_batch_size}', 'debug')
+        self.trainer_print(f'new_global_batch_size: {new_global_batch_size}', 'debug')
+        if new_local_batch_size == 0 and new_global_batch_size == 0:
+            self.trainer_print(f'Candidate global batch size by SimiGrad = {new_global_batch_size_by_simigrad}, '
+                               f'but no virtual worker placement solution by DP', 'warning')
+            return solved_iidp_config_map
+        if new_global_batch_size//new_local_batch_size < dist.get_world_size():
+            self.trainer_print(f'Candidate global batch size by SimiGrad = {new_global_batch_size_by_simigrad}, '
+                               f'but cannot support on current numer of GPUs: {dist.get_world_size()}', 'warning')
+            return solved_iidp_config_map
+
+        # == Change local, global batch size == #
+        self.local_batch_size = new_local_batch_size
+        self.global_batch_size = new_global_batch_size
+        assert self.global_batch_size % self.local_batch_size == 0, \
+            f"New global batch size {self.global_batch_size} must be preserved local batch size: {self.local_batch_size}"
+        return solved_iidp_config_map
+
+    def change_global_batch_size_for_iidp(self, new_global_batch_size_by_simigrad):
+        solved_iidp_config_map = {} # return
+        if self.adaptive_batch_params["batch_size_upper_bound"] is not None:
+            new_global_batch_size_by_simigrad = min(new_global_batch_size_by_simigrad, self.adaptive_batch_params["batch_size_upper_bound"])
+        if self.adaptive_batch_params["batch_size_lower_bound"] is not None:
+            new_global_batch_size_by_simigrad = max(new_global_batch_size_by_simigrad, self.adaptive_batch_params["batch_size_lower_bound"])
+
+        new_adjust_gbs_amount = new_global_batch_size_by_simigrad - self.global_batch_size
+        #self.trainer_print(f'change_global_batch_size_for_iidp() - new_global_batch_size: {new_global_batch_size_by_simigrad}', 'debug')
+        #self.trainer_print(f'change_global_batch_size_for_iidp() - new_adjust_gbs_amount: {new_adjust_gbs_amount}', 'debug')
+        if new_adjust_gbs_amount == 0:
+            return solved_iidp_config_map
+        if new_adjust_gbs_amount < 0:
+            iidp_adjust_new_num_models = min(-1, round(new_adjust_gbs_amount / self.local_batch_size))
+            iidp_adjust_new_num_models -= (iidp_adjust_new_num_models % 2)
+        else:
+            iidp_adjust_new_num_models = max(1, round(new_adjust_gbs_amount / self.local_batch_size))
+            iidp_adjust_new_num_models += (iidp_adjust_new_num_models % 2)
+
+        assert iidp_adjust_new_num_models % 2 == 0, \
+                f"With SimiGrad constraint, new adjust num models: {iidp_adjust_new_num_models} are must be even"
+
+        iidp_adjust_gbs_amount = iidp_adjust_new_num_models * self.local_batch_size
+        #self.trainer_print(f'iidp_adjust_gbs_amount: {iidp_adjust_gbs_amount}', 'debug')
+        candidate_global_batch_size = self.global_batch_size + iidp_adjust_gbs_amount
+        total_num_workers = candidate_global_batch_size // self.local_batch_size
+        if self.total_num_models == total_num_workers: # No global batch size change
+            return solved_iidp_config_map
+        if total_num_workers < dist.get_world_size():
+            self.trainer_print(f'Candidate global batch size by SimiGrad = {candidate_global_batch_size}, '
+                               f'but cannot support on current numer of GPUs: {dist.get_world_size()}', 'warning')
+            return solved_iidp_config_map
+        solved_iidp_config_map = self.configurator.solve_placement(total_num_workers, self.global_batch_size)
+        #self.trainer_print(f'solved_iidp_config_map: {solved_iidp_config_map}', 'debug')
+        if solved_iidp_config_map == {}:
+            self.trainer_print(f'Candidate global batch size by SimiGrad = {candidate_global_batch_size}, '
+                               f'but no virtual worker placement solution by DP', 'warning')
+            return solved_iidp_config_map
+
+        # == Change Global batch size == #
+        self.global_batch_size += iidp_adjust_gbs_amount
+        #self.trainer_print(f'Change Global batch size ==> self.global_batch_size: {self.global_batch_size}', 'debug')
+        assert self.global_batch_size % self.local_batch_size == 0, \
+            f"New global batch size {self.global_batch_size} must be preserved local batch size: {self.local_batch_size}"
+        return solved_iidp_config_map
+
+    def scale_lr(self, new_global_batch_size):
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            if self.is_elastic_training is True:
+                initial_batch_size = self.adaptive_batch_params["batch_size_lower_bound"]
+            else:
+                # NOTE: For some reasons, initial batch size is smaller than batch_size_lower_bound
+                # Then, learning rate for batch_size_lower_bound cannot work well with smaller initial batch size
+                initial_batch_size = max(
+                    self.adaptive_batch_params["original_batch_size"], self.adaptive_batch_params["batch_size_lower_bound"])
+            # Square-root scaling
+            new_ratio = math.sqrt(new_global_batch_size / initial_batch_size)
+            if self.adaptive_batch_params["lr_adjust_factor"] is not None:
+                new_ratio = (new_ratio-1) * self.adaptive_batch_params["lr_adjust_factor"] + 1
+            self.adaptive_batch_params["global_lr_modifier"] = new_ratio
+            self.trainer_print(f"scale_lr() - The learning rate modifier was updated to {self.adaptive_batch_params['global_lr_modifier']}")
+
+    def adjust_adaptive_lr(self, intialize=False):
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            if not intialize:
+                """
+                self.trainer_print(f'adjust_adaptive_lr() - global lr modifier: {self.adaptive_batch_params["global_lr_modifier"]} '
+                                   f'initial lr: {self.main_optimizer.param_groups[0]["lr"]} | '
+                                   f'at local accum step: {self.local_accum_step} | sync step: {self.simigrad_state.step}')
+                """
+                for param_group in self.main_optimizer.param_groups:
+                    param_group['lr']*=self.adaptive_batch_params["global_lr_modifier"]
+                    lr = param_group['lr']
+                if self.simigrad_state.step % self.simigrad_state.interval == 0:
+                    self.trainer_print(f"adjust_adaptive_lr() - scaled LR: {lr} at step: {self.simigrad_state.step}")
+                """
+                self.trainer_print(f'adjust_adaptive_lr() - global lr modifier: {self.adaptive_batch_params["global_lr_modifier"]} '
+                                   f'param groups lr: {self.main_optimizer.param_groups[0]["lr"]} | '
+                                   f'at local accum step: {self.local_accum_step} | sync step: {self.simigrad_state.step}')
+                self.trainer_print(f"adjust_adaptive_lr() - scaled LR: {lr} at local accum step: {self.local_accum_step} | "
+                                   f"sync step: {self.simigrad_state.step}")
+                """
+            else:
+                #for param_group in self.main_optimizer.param_groups:
+                #    lr = param_group['lr']
+                #self.trainer_print(f"step() - prev scaled LR: {lr} | lr modifier: {self.adaptive_batch_params['global_lr_modifier']} at step: {self.simigrad_state.step}")
+                for param_group in self.main_optimizer.param_groups:
+                    param_group['lr']/=self.adaptive_batch_params["global_lr_modifier"]
+                    lr = param_group['lr']
+                if self.simigrad_state.step % self.simigrad_state.interval == 0:
+                    self.trainer_print(f"adjust_adaptive_lr() - initial LR: {lr} at step: {self.simigrad_state.step}")
+
+    def adjust_adaptive_global_batch_size(self):
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            self.trainer_print(f"step {self.simigrad_state.step} - Current cos similiarity {self.cos_placeholder} for batch size {self.global_batch_size} ")
+            # [SimiGrad] Algorithm 1 - 2) If Φ < γ, target batch size B = 1.1B, else, B = 0.9B
+            # [IIDP] - target batch size with constraint to preserve local batch size
+            if self.cos_placeholder < self.adaptive_batch_params["similarity_target"]:
+                new_global_batch_size_by_simigrad = self.global_batch_size * (1 + self.adaptive_batch_params["batch_size_adjust_perc"])
+                if self.adaptive_batch_params["enable_adjust_lbs"] is True:
+                    solved_iidp_config_map = self.change_batch_size_for_iidp(new_global_batch_size_by_simigrad)
+                else:
+                    solved_iidp_config_map = self.change_global_batch_size_for_iidp(new_global_batch_size_by_simigrad)
+                self.trainer_print(f'step {self.simigrad_state.step} - current similarity < target ==> increase batch size !!! - new batch size: {self.global_batch_size}')
+            elif self.cos_placeholder > self.adaptive_batch_params["similarity_target"] and self.global_batch_size > 1:
+                new_global_batch_size_by_simigrad = self.global_batch_size * (1 - self.adaptive_batch_params["batch_size_adjust_perc"])
+                if self.adaptive_batch_params["enable_adjust_lbs"] is True:
+                    solved_iidp_config_map = self.change_batch_size_for_iidp(new_global_batch_size_by_simigrad)
+                else:
+                    solved_iidp_config_map = self.change_global_batch_size_for_iidp(new_global_batch_size_by_simigrad)
+                self.trainer_print(f'step {self.simigrad_state.step} - current similarity > target  ==> decrease batch size !!! - new batch size: {self.global_batch_size}')
+            with Timer(f'[INFO][{self.__class__.__name__}] Dynamic VSW config overhead') as dynamic_config_timer:
+                self.change_configuration_for_iidp(solved_iidp_config_map)
+            self.total_overhead_dict['dynamic config'] += dynamic_config_timer.elapsed
+            self.scale_lr(self.global_batch_size)
+        #self.trainer_print(f'self.epoch: {self.epoch} | self.global_batch_size_trajectory: {self.global_batch_size_trajectory}', 'debug')
+        self.global_batch_size_trajectory[self.epoch].append([self.simigrad_state.step, self.global_batch_size])
+
+    def compute(self, data):
+        if self.adaptive_batch_params["enable_adjust"] and self.local_accum_step == 0:
+        #if self.adaptive_batch_params["enable_adjust"]:
+            self.adjust_adaptive_lr()
+        super().compute(data)
+
+    def step(self):
+        # NOTE: As IIDP has overlapping backward pass and optimizer.step(),
+        # scaled LR must be adopted before forward pass in compute() method
+        if super().step() is False:
+            return False
+
+        if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+            if self.adaptive_batch_params["enable_adjust"]:
+                self.adjust_adaptive_lr(intialize=True)
+            #self.trainer_print(f'self.simigrad_state.step: {self.simigrad_state.step} | self.simigrad_state.interval: {self.simigrad_state.interval}', 'debug')
+            # == Measure aggregated gradients in sub-group only once in an epoch ==
+            if self.simigrad_state.step == self.simigrad_state.interval:
+                self.simigrad_state.done_epoch = True # allreduce hook for SimiGrad will work at next step
+                if self.adaptive_batch_params["metric"] == 'similarity':
+                    self.simigrad_state.step += 1
+                return True
+            if self.simigrad_state.done_epoch is True:
+                self.simigrad_state.done_epoch = False
+            # =====================================================================
+            if self.adaptive_batch_params["metric"] == 'similarity':
+                self.simigrad_state.step += 1
+            return True
+
+        if self.adaptive_batch_params["enable_adjust"]:
+            self.adjust_adaptive_lr(intialize=True)
+        if self.adaptive_batch_params["metric"] == 'similarity' and \
+                self.simigrad_state.step % self.simigrad_state.interval == 0:
+            self.compute_cosine_similarity()
+        if self.adaptive_batch_params["enable_adjust"] and \
+                self.simigrad_state.step % self.simigrad_state.interval == 0:
+            self.adjust_adaptive_global_batch_size()
+        if self.adaptive_batch_params["enable_adjust"] and \
+                self.simigrad_state.step % self.adaptive_batch_params["batch_size_adjust_interval"] == 0:
+            self.trainer_print(f"step() New local batch size {self.local_batch_size}")
+            self.trainer_print(f"step() New train batch size {self.global_batch_size}\n==============================")
+
+        if self.adaptive_batch_params["metric"] == 'similarity':
+            self.simigrad_state.step += 1
+
+        return True
+
+    @contextmanager
+    def measure_epoch_time(self):
+        try:
+            start_time = time.time()
+            yield
+        finally:
+            self.elapsed_time = int(time.time() - start_time)
+            if self.is_resource_reallocated:
+                self.elapsed_time += int(self.reallocation_overhead)
+                self.trainer_print(f'Epoch time: {self.elapsed_time} ' \
+                                   f'(include reallocation time: {self.reallocation_overhead:.2f} sec)')
+            else:
+                self.trainer_print(f'Epoch time: {self.elapsed_time}')
+            self.total_epoch_time += self.elapsed_time
+
+    def current_resource_info_parser(self, global_server_info):
+        resource_info_dict = {}
+        resource_info_dict['total_num_gpus'] = global_server_info.total_num_gpus
+        for server_info in global_server_info:
+            if server_info.resource_info.device_name in resource_info_dict.keys():
+                resource_info_dict[server_info.resource_info.device_name] += server_info.resource_info.num_gpus_in_server
+            else:
+                resource_info_dict[server_info.resource_info.device_name] = server_info.resource_info.num_gpus_in_server
+        self.trainer_print(f'current resource info: {resource_info_dict}')
+
+    def measure_epoch_cost(self):
+        total_cost_per_epoch = 0
+        for server_info in self.cluster_manager.global_server_info:
+            total_cost_per_epoch += estimate_cost(
+                    server_info.resource_info.tfplos,
+                    server_info.resource_info.num_gpus_in_server,
+                    self.elapsed_time / 3600 # convert to sec to hour
+                )
+        self.trainer_print(f'Epoch cost: {total_cost_per_epoch:.2f}')
+        self.total_epoch_cost += total_cost_per_epoch
+
+    def remaining_epochs(self, final_epochs):
+        self.epoch_iterator.final_epochs = final_epochs
+        try:
+            for epoch in self.epoch_iterator.__iter__():
+                self.global_batch_size_trajectory.append([])
+                if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+                    self.trainer_print('*************** GBS_INTERVAL_AS_EPOCH = 1 ***************', 'experiment')
+                    # [GBS prediction] ES model requires at least two data points
+                    if epoch == 0:
+                        self.global_batch_size_trajectory[epoch].append([0, self.global_batch_size])
+                    # As GBS does not change during epoch, interval can be calculated by the below
+                    self.simigrad_state.interval += int(len(self.data_loader.dataset)/self.global_batch_size)-1
+                    #self.trainer_print(f'self.global_batch_size: {self.global_batch_size} | self.data_loader.dataset: {len(self.data_loader.dataset)}',  'experiment')
+                    self.trainer_print(f'GBS_INTERVAL_AS_EPOCH - interval: {self.simigrad_state.interval}',  'experiment')
+                #self.trainer_print(f'remaining_epochs() - epoch: {epoch}', 'debug')
+                # [IIDP] For result parsing
+                if self.is_elastic_training is True:
+                    self.current_resource_info_parser(self.cluster_manager.global_server_info)
+                yield epoch
+                if os.getenv('GBS_INTERVAL_AS_EPOCH') == "1":
+                    self.trainer_print('*************** GBS_INTERVAL_AS_EPOCH = 1 ***************', 'experiment')
+                    if self.adaptive_batch_params["metric"] == 'similarity':
+                        self.compute_cosine_similarity()
+                    if self.adaptive_batch_params["enable_adjust"]:
+                        self.adjust_adaptive_global_batch_size()
+                    if self.adaptive_batch_params["enable_adjust"]:
+                        self.trainer_print(f"step() New local batch size {self.local_batch_size}")
+                        self.trainer_print(f"step() New train batch size {self.global_batch_size}\n==============================")
+                #print(f'[DEBUG][rank: {dist.get_rank()}] =============> after yield epoch {epoch} ..')
+                is_last_epoch = (epoch == len(self.epoch_iterator)-1)
+                if self.is_elastic_training is True:
+                    if self._trainer_id == 0 and not is_last_epoch:
+                        # NOTE: If auto-scaling is requested,
+                        # the below code after the method at rank 0 do not reach
+                        self.prepare_joint_adaptive_training_by_forecasting()
+                    if is_last_epoch:
+                        # NOTE: If not last epoch, it is called at prepare_joint_adaptive_training_by_forecasting()
+                        self.measure_epoch_cost()
+                dist.barrier()
+                self.is_resource_reallocated = False
+        finally:
+            self.print_final_results()
+
+        dist.barrier()
+        #self.trainer_print(f'Initial global batch size: {self.adaptive_batch_params["original_batch_size"]}', 'debug')
+        if self.is_elastic_training is True:
+            # NOTE: _local_rank is defined only if is_elastic_training = True
+            if self._local_rank == 0:
+                self._rpc_client.shutdown()
+
+    def print_final_results(self):
+        if self.is_elastic_training is True:
+            self.trainer_print(f'Total epoch time (sec): {self.total_epoch_time+int(self.total_overhead_dict["forecasting"])}')
+            self.trainer_print(f'Total epoch cost (dollar): {self.total_epoch_cost:.2f}')
+            self.trainer_print(f'Total train + reallocation time (sec): {self.total_epoch_time}')
+            self.trainer_print(f'Total epoch time: {datetime.timedelta(seconds=self.total_epoch_time+int(self.total_overhead_dict["forecasting"]))}')
+            self.trainer_print(f'Total forecasting overhead (sec): {self.total_overhead_dict["forecasting"]:.3f}', 'experimental')
+        else:
+            self.trainer_print(f'Total epoch time (sec): {self.total_epoch_time}')
+            self.trainer_print(f'Total epoch time: {datetime.timedelta(seconds=self.total_epoch_time)}')
+        self.trainer_print(f'Total dynamic VSW config overhead (sec): {self.total_overhead_dict["dynamic config"]:.3f}', 'experimental')
+        total_dp_solver_overhead = 0
+        for component, time in self.total_overhead_dict['dp solver'].items():
+            total_dp_solver_overhead += time
+            self.trainer_print(f'Total DP solver overhead for {component} (sec): {time:.3f}', 'experimental')
+        self.trainer_print(f'Total DP solver overhead (sec): {total_dp_solver_overhead:.3f}', 'experimental')
+
+    def prepare_joint_adaptive_training_by_forecasting(self):
+        with Timer(f'[INFO][{self.__class__.__name__}] Forecasting overhead') as forecast_timer:
+            if os.getenv('NO_BATCH_SIZE_PREDICTION_EXP') == "1":
+                    self.trainer_print('Without batch size prediction, prepare joint-adaptve training', 'experimental')
+                    predicted_gbs_trajectory = []
+            else:
+                #print(f'[DEBUG][rank: {dist.get_rank()}] [epoch {self.epoch}] =============> before self.predict_global_batch_size()')
+                predicted_gbs_trajectory = self.predict_global_batch_size()
+                #print(f'[DEBUG][rank: {dist.get_rank()}] [epoch {self.epoch}] =============> after self.predict_global_batch_size()')
+            gbs_trajectory = [self.global_batch_size] + predicted_gbs_trajectory
+            # [IIDP] For result parsing
+            self.trainer_print(f'Naive predicted GBS trajectory before estimate_efficient_resource: {gbs_trajectory}', 'info')
+            #self.trainer_print(f'predicted gbs_trajectory: {gbs_trajectory}', 'info')
+            #print(f'[DEBUG][rank: {dist.get_rank()}] [epoch {self.epoch}] =============> before self.estimate_efficient_resource()')
+            best_server_info, best_iidp_config_map, expected_avg_utility, expected_gbs_trajectory \
+                = self.estimate_efficient_resource(gbs_trajectory)
+        self.total_overhead_dict['forecasting'] += forecast_timer.elapsed
+        # NOTE: self.elapsed_time is used to measure cost, so forecasting time is added to it
+        self.elapsed_time += forecast_timer.elapsed
+        self.measure_epoch_cost()
+        # [IIDP] For result parsing
+        assert len(expected_gbs_trajectory) > 0, f'len(expected_gbs_trajectory) == 0 - {expected_gbs_trajectory}'
+        self.trainer_print(f'predicted gbs_trajectory: {expected_gbs_trajectory}', 'info')
+        self.trainer_print(f'[epoch {self.epoch}] =============> after self.estimate_efficient_resource()')
+        self.trainer_print(f'best server info: {best_server_info} | '
+                           f'best IIDP config map: {best_iidp_config_map} | '
+                           f'expected avg utility: {expected_avg_utility}', 'debug')
+        self.trainer_print(f'best_server_info != self.cluster_manager.global_server_info => '
+                           f'{best_server_info != self.cluster_manager.global_server_info}', 'debug')
+        self.trainer_print(f'[epoch {self.epoch}] =======================================================')
+        if best_server_info is not None and len(best_iidp_config_map) > 0 and \
+                    best_server_info != self.cluster_manager.global_server_info:
+            self.trainer_print('********************** Resource auto-scaling !!! **********************')
+            self.trainer_print(f'[epoch: {self.epoch}] best server info: {best_server_info} | LBS: {self.local_batch_size}'
+                f'best IIDP config map: {best_iidp_config_map} | '
+                f'expected avg utility: {expected_avg_utility}', 'info')
+            self.trainer_print('***********************************************************************')
+            #print(f'[DEBUG][rank: {dist.get_rank()}] [epoch {self.epoch}] =============> before self.request_resource_scaling()')
+            self.request_resource_scaling(best_iidp_config_map)
+
+    def predict_global_batch_size(self):
+        def train_batch_size_model():
+            x_train_list, y_train_list = [], []
+            self.trainer_print(
+                f'train_batch_size_model() - self.epoch: {self.epoch} | ' \
+                f'self.global_batch_size_trajectory[self.epoch]: {self.global_batch_size_trajectory[self.epoch]}',
+                'debug')
+            for step, gbs in self.global_batch_size_trajectory[self.epoch]:
+                x_train_list.append(step)
+                y_train_list.append(gbs)
+            self.batch_size_model.train(x_train_list, y_train_list)
+        # Train
+        if len(self.global_batch_size_trajectory[self.epoch]) > 0:
+            train_batch_size_model()
+        # Prepare around next steps for prediction
+        x_pred_list, iidp_gbs_trajectory = [], []
+        default_step = self.adaptive_batch_params["batch_size_adjust_interval"]
+        num_dataset = len(self.data_loader.dataset)
+        total_steps_at_next_epoch = num_dataset // self.global_batch_size
+        number_of_adjust_batch_size_at_next_epoch = total_steps_at_next_epoch // default_step
+        around_steps_at_next_epoch = []
+        for i in range(1, number_of_adjust_batch_size_at_next_epoch+1):
+            around_steps_at_next_epoch.append(self.sync_step + default_step*i)
+        self.trainer_print(
+            f'predict_global_batch_size() - total_steps_at_next_epoch: {total_steps_at_next_epoch} | ' \
+            f'number_of_adjust_batch_size_at_next_epoch: {number_of_adjust_batch_size_at_next_epoch} | ' \
+            f'around_steps_at_next_epoch: {around_steps_at_next_epoch}',
+            'debug')
+        for step in around_steps_at_next_epoch:
+            x_pred_list.append(step)
+        # Predict
+        if len(x_pred_list) > 0:
+            try:
+                y_pred_mean = self.batch_size_model.evaluate(x_pred_list)
+            except Exception as e:
+                self.trainer_print(
+                    f'predict_global_batch_size() - x_pred_list: {x_pred_list} | '
+                    f'self.epoch: {self.epoch} | '
+                    f'global_batch_size_trajectory at epoch: {self.global_batch_size_trajectory[self.epoch]}\n'
+                    f'total global_batch_size_trajectory: {self.global_batch_size_trajectory}', 'error')
+                raise e
+            predicted_gbs_trajectory = y_pred_mean.ravel()
+            # For static local batch size
+            if self.adaptive_batch_params["enable_adjust_lbs"] is False:
+                for predicted_global_batch_size in predicted_gbs_trajectory:
+                    total_num_virtual_workers = (int(predicted_global_batch_size) // self.local_batch_size)
+                    total_num_virtual_workers += (total_num_virtual_workers % 2)
+                    iidp_global_batch_size = total_num_virtual_workers * self.local_batch_size
+                    iidp_gbs_trajectory.append(iidp_global_batch_size)
+            else:
+                iidp_gbs_trajectory = list(predicted_gbs_trajectory)
+        return iidp_gbs_trajectory
+
+    def estimate_efficient_resource(self, gbs_trajectory):
+        best_server_info = None
+        best_iidp_config_map = {}
+        expected_avg_utility = -1
+        best_expected_gbs_trajectory = gbs_trajectory
+        default_step = self.adaptive_batch_params["batch_size_adjust_interval"]
+        min_epoch_time = math.inf
+
+        # For logging
+        total_num_candidate_servers = len(self.cluster_manager.candidate_server_infos)
+        print_freq = 10**(len(str(total_num_candidate_servers))-1)
+        self.trainer_print(
+            f'[{self.__class__.__name__}] Total number of candidate severs to estimate: '
+            f'{total_num_candidate_servers}'
+        )
+        for server_id, candidate_server_info in enumerate(self.cluster_manager.candidate_server_infos):
+            verbose = (server_id % print_freq == 0)
+            if verbose:
+                log_str = f'[{server_id} / {total_num_candidate_servers}] ' \
+                    f'Start to estimate efficient server config with predicted GBS at next epoch'
+                length = len(log_str) + 1
+                self.trainer_print('=' * length)
+                self.trainer_print(log_str)
+                self.trainer_print('=' * length)
+            #self.trainer_print(f'candidate_server_info: {candidate_server_info}', 'debug')
+            # [EXPERIMENTAL] - Not decrease total number of GPUs
+            """
+            if candidate_server_info.total_num_gpus < dist.get_world_size():
+                self.trainer_print(
+                    f'Not decrease total number of GPUs: {candidate_server_info.total_num_gpus} | '
+                    f'{dist.get_world_size()}', 'experimental')
+                continue
+            """
+            self.future_configurator.update(server_id, self.local_batch_size, self.global_batch_size)
+            #self.trainer_print(f'self.future_configurator.global_server_info: {self.future_configurator.global_server_info}', 'debug')
+            epoch_duration = 0
+            # [EXPERIMENTAL] - Reallocation penalty
+            """
+            if candidate_server_info != self.cluster_manager.global_server_info:
+                epoch_duration += self.reallocation_overhead
+            else: # debug
+                self.trainer_print(
+                    f'Current global server: {self.cluster_manager.global_server_info} | '
+                    f'candidate: {candidate_server_info}', 'debug'
+                )
+            """
+            total_utility = 0
+            init_config_map_next_epoch = {}
+            remaining_num_dataset = len(self.data_loader.dataset)
+            expected_gbs_trajectory = []
+            for gbs_idx, gbs in enumerate(gbs_trajectory):
+                step = default_step
+                if gbs_idx == 0: # current global batch size
+                    step = default_step - (self.sync_step % default_step)
+                # With last GBS, (iteration * GBS) must process all remaining dataset
+                if gbs_idx == len(gbs_trajectory)-1:
+                    step = (remaining_num_dataset // gbs) + 1
+                """
+                self.trainer_print(
+                    f'estimate_time_and_utility() - argument => '
+                    f'gbs: {gbs} | step: {step} | number of remaining dataset: {remaining_num_dataset}', 'debug')
+                """
+                with Timer(f'[INFO][{self.__class__.__name__}] DP solver overhead for auto-scaling', verbose) as dp_solver_timer:
+                    time, utility, iidp_config_map, expected_gbs, expected_step \
+                        = self.future_configurator.estimate_time_and_utility(gbs, step, remaining_num_dataset)
+                self.total_overhead_dict['dp solver']['auto-scaling'] += dp_solver_timer.elapsed
+                if time == math.inf:
+                    self.trainer_print(f'configuration is impossble for expected gbs: {expected_gbs}')
+                    break
+                if gbs_idx == 0: # current global batch size
+                    init_config_map_next_epoch = iidp_config_map
+                if expected_step <= 0 and time != math.inf:
+                    continue
+                """
+                self.trainer_print(
+                    f'estimate_time_and_utility() - return => '
+                    f'gbs: {gbs} | step: {step} | time: {time} | utility: {utility} | iidp config map: {iidp_config_map} | '
+                    f'expected_gbs: {expected_gbs} | expected_step: {expected_step}', 'debug')
+                """
+                remaining_num_dataset -= expected_gbs*expected_step
+                expected_gbs_trajectory.append(expected_gbs)
+                epoch_duration += time
+                total_utility += utility
+                avg_utility = total_utility / len(gbs_trajectory)
+                if epoch_duration == math.inf:
+                    break
+                """
+                self.trainer_print(
+                    f'number of remaining dataset: {remaining_num_dataset} | '
+                    f'expected_gbs_trajectory: {expected_gbs_trajectory} | '
+                    f'time: {time} | utility: {utility}', 'debug')
+                """
+            #self.trainer_print(f'epoch duration: {epoch_duration} | avg utility: {avg_utility}', 'debug')
+            if (epoch_duration > 0 and epoch_duration <= min_epoch_time) \
+                    and avg_utility >= self.adaptive_batch_params["utility_threshold"]:
+                min_epoch_time = epoch_duration
+                #self.trainer_print(f'*************************************************************', 'debug')
+                #self.trainer_print(f'===> min epoch time (sec): {epoch_duration:.2f} | utility (<= 1): {avg_utility:.2f}', 'debug')
+                #self.trainer_print(f'*************************************************************', 'debug')
+                best_server_info = candidate_server_info
+                best_iidp_config_map = init_config_map_next_epoch
+                expected_avg_utility = avg_utility
+                best_expected_gbs_trajectory = expected_gbs_trajectory
+        self.trainer_print(f'*********************** estimate_efficient_resource() ***********************', 'debug')
+        self.trainer_print(f'===> min epoch time (sec): {epoch_duration:.2f} | utility (<= 1): {avg_utility:.2f}', 'debug')
+        self.trainer_print(f'*****************************************************************************', 'debug')
+        return best_server_info, best_iidp_config_map, \
+                expected_avg_utility, best_expected_gbs_trajectory
+
+    def request_resource_scaling(self, rank_to_config_map):
+        """NOTE: [Important] This function is called by only one rank"""
+        #print(f'[DEBUG][rank: {dist.get_rank()}] [epoch {self.epoch}] =============> start self.request_resource_scaling()')
+        def convert_config_map_to_proto(config_map):
+            # NOTE: Message type is defined by torch/iidp/elastic/runtime/protobuf/trainer_to_scheuder.proto
+            config_map_proto = {}
+            for rank, (num_models, accum_step) in config_map.items():
+                config_map_proto[rank] = f"{num_models},{accum_step}"
+            return config_map_proto
+
+        if self.data_loader.done is False:
+            raise AssertionError('Resource re-allocation must be at the end of epoch')
+
+        self.save_checkpoint()
+        # NOTE: Only one rank communicates with agent to update configuration
+        config_map_proto = convert_config_map_to_proto(rank_to_config_map)
+        self._rpc_client.update_config(config_map_proto, self.local_batch_size)
+        # NOTE: As update_config() requests asynchronously, one rank should stop here
+        while True:
+            pass
diff --git a/torch/iidp/utils/__init__.py b/torch/iidp/utils/__init__.py
new file mode 100644
index 00000000000..64d0e0e9cc6
--- /dev/null
+++ b/torch/iidp/utils/__init__.py
@@ -0,0 +1,3 @@
+from . import json_utils
+from . import distributed
+from . import cost_utils
\ No newline at end of file
diff --git a/torch/iidp/utils/clip_grad.py b/torch/iidp/utils/clip_grad.py
new file mode 100644
index 00000000000..eb23f13cfc1
--- /dev/null
+++ b/torch/iidp/utils/clip_grad.py
@@ -0,0 +1,45 @@
+import torch
+
+from torch._six import inf
+from typing import Union, Iterable
+
+_tensor_or_tensors = Union[torch.Tensor, Iterable[torch.Tensor]]
+
+
+def clip_grad_norm_for_overlap(gradients: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0) -> torch.Tensor:
+    r"""Clips gradient norm of an iterable of gradients.
+
+    The norm is computed over all gradients together, as if they were
+    concatenated into a single vector. Gradients are modified in-place.
+
+    Args:
+        gradients (Iterable[Tensor] or Tensor): an iterable of Tensors or a
+            single Tensor that will have gradients normalized
+        max_norm (float or int): max norm of the gradients
+        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
+            infinity norm.
+
+    Returns:
+        Total norm of the gradients (viewed as a single vector).
+    """
+    if isinstance(gradients, torch.Tensor):
+        gradients = [gradients]
+    max_norm = float(max_norm)
+    norm_type = float(norm_type)
+    if len(gradients) == 0:
+        return torch.tensor(0.)
+    device = gradients[0].device
+    if norm_type == inf:
+        norms = [grad.detach().abs().max().to(device) for grad in gradients]
+        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
+    else:
+        total_norm = torch.norm(torch.stack([torch.norm(grad.detach(), norm_type).to(device) for grad in gradients]), norm_type)
+    clip_coef = max_norm / (total_norm + 1e-6)
+
+    # https://github.com/pytorch/pytorch/issues/60691
+    # To avoid `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization
+    # when the gradients do not reside in CPU memory.
+    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)
+    for grad in gradients:
+        grad.detach().mul_(clip_coef_clamped.to(grad.device))
+    return total_norm
\ No newline at end of file
diff --git a/torch/iidp/utils/cost_utils.py b/torch/iidp/utils/cost_utils.py
new file mode 100644
index 00000000000..ab4cafe491e
--- /dev/null
+++ b/torch/iidp/utils/cost_utils.py
@@ -0,0 +1,13 @@
+BASE_TFPLOS = 14.13
+
+# AWS P3 instance (4 V100 GPU) price per hour (dollar) = 12.24
+# Price of single GPU per hour = 12.24 / 4 = 3.06
+BASE_COST = 3.06
+
+
+def cost_model(tfplos):
+    return BASE_COST * (tfplos/BASE_TFPLOS)
+
+
+def estimate_cost(tfplos, num_gpus, time):
+    return cost_model(tfplos) * num_gpus * time
\ No newline at end of file
diff --git a/torch/iidp/utils/distributed.py b/torch/iidp/utils/distributed.py
new file mode 100644
index 00000000000..d3977626ab4
--- /dev/null
+++ b/torch/iidp/utils/distributed.py
@@ -0,0 +1,41 @@
+import torch
+import torch.distributed as dist
+
+
+def print_rank(message, status='info'):
+    if not isinstance(message, str):
+        message = str(message)
+    print(f'[{status.upper()}] rank: {dist.get_rank()} | ' + message)
+
+
+def print_one_rank(message, status='info', rank=0):
+    if not isinstance(message, str):
+        message = str(message)
+    if dist.is_initialized():
+        if dist.get_rank() == rank:
+            print(f'[{status.upper()}] rank: {dist.get_rank()} | ' + message)
+    else:
+        print(f'[{status.upper()}] | ' + message)
+
+
+def get_allgather_value(value, gpu=None):
+    if dist.is_initialized():
+        total_num_gpus = dist.get_world_size()
+        device = gpu if gpu is not None else dist.get_rank()
+        tensor_list = [
+            torch.tensor([0], dtype=torch.float32).to(device) for _ in range(total_num_gpus)
+        ]
+        tensor = torch.tensor([value], dtype=torch.float32).to(device)
+        dist.all_gather(tensor_list, tensor)
+        if isinstance(value, int):
+            ret_val = [int(tensor.item()) for tensor in tensor_list]
+        else:
+            ret_val = [tensor.item() for tensor in tensor_list]
+        # Save GPU memory
+        num_tensors = len(tensor_list)
+        for _ in range(num_tensors):
+            tensor_list[-1].cpu()
+            del tensor_list[-1]
+        return ret_val
+    else:
+        return [value]
\ No newline at end of file
diff --git a/torch/iidp/utils/json_utils.py b/torch/iidp/utils/json_utils.py
new file mode 100644
index 00000000000..df79c4bd0d5
--- /dev/null
+++ b/torch/iidp/utils/json_utils.py
@@ -0,0 +1,21 @@
+import json
+
+
+def read_json(json_file_path):
+    try:
+        with open(json_file_path, 'r') as jf:
+            json_data = json.load(jf)
+    except IOError as e:
+        print("I/O error({0}): {1} - {2}".format(e.errno, e.strerror, json_file_path))
+        exit(1)
+    return json_data
+
+
+def write_json(json_file_path, data):
+    try:
+        with open(json_file_path, 'w') as jf:
+            json_str = json.dumps(data)
+            jf.write(json_str)
+    except IOError as e:
+        print("I/O error({0}): {1} - file path: {2} data: {3}".format(e.errno, e.strerror, json_file_path, data))
+        exit(1)
\ No newline at end of file
diff --git a/torch/lib/c10d/ProcessGroup.hpp b/torch/lib/c10d/ProcessGroup.hpp
index d8ec4b31d36..47b838ddea1 100644
--- a/torch/lib/c10d/ProcessGroup.hpp
+++ b/torch/lib/c10d/ProcessGroup.hpp
@@ -192,6 +192,9 @@ class ProcessGroup : public torch::CustomClassHolder {
       std::vector<at::Tensor>& tensors,
       const ReduceOptions& opts = ReduceOptions()) = 0;
 
+  virtual c10::intrusive_ptr<ProcessGroup::Work> get_dummy_work(
+      std::vector<at::Tensor>& tensors) = 0;
+
   virtual c10::intrusive_ptr<ProcessGroup::Work> allgather(
       std::vector<std::vector<at::Tensor>>& outputTensors,
       std::vector<at::Tensor>& inputTensors,
diff --git a/torch/lib/c10d/ProcessGroupGloo.cpp b/torch/lib/c10d/ProcessGroupGloo.cpp
index f8cc3804a97..51bf146bf61 100644
--- a/torch/lib/c10d/ProcessGroupGloo.cpp
+++ b/torch/lib/c10d/ProcessGroupGloo.cpp
@@ -1363,6 +1363,12 @@ c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupGloo::allreduce(
   return work;
 }
 
+c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupGloo::get_dummy_work(
+        std::vector<at::Tensor>& tensors) {
+  c10::intrusive_ptr<AsyncWork> dummy;
+  return dummy;
+}
+
 c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupGloo::allreduce_coalesced(
     std::vector<at::Tensor>& tensors,
     const AllreduceCoalescedOptions& opts) {
diff --git a/torch/lib/c10d/ProcessGroupGloo.hpp b/torch/lib/c10d/ProcessGroupGloo.hpp
index d0befc95e97..870a6b490f6 100644
--- a/torch/lib/c10d/ProcessGroupGloo.hpp
+++ b/torch/lib/c10d/ProcessGroupGloo.hpp
@@ -169,6 +169,9 @@ class ProcessGroupGloo : public ProcessGroup {
       std::vector<at::Tensor>& tensors,
       const BroadcastOptions& opts = BroadcastOptions()) override;
 
+  c10::intrusive_ptr<ProcessGroup::Work> get_dummy_work(
+      std::vector<at::Tensor>& tensors) override;
+
   c10::intrusive_ptr<ProcessGroup::Work> allreduce(
       std::vector<at::Tensor>& tensors,
       const AllreduceOptions& opts = AllreduceOptions()) override;
diff --git a/torch/lib/c10d/ProcessGroupMPI.cpp b/torch/lib/c10d/ProcessGroupMPI.cpp
index 250b635e8c6..f1c00f519eb 100644
--- a/torch/lib/c10d/ProcessGroupMPI.cpp
+++ b/torch/lib/c10d/ProcessGroupMPI.cpp
@@ -339,6 +339,20 @@ c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupMPI::broadcast(
   return enqueue(std::move(entry));
 }
 
+c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupMPI::get_dummy_work(
+    std::vector<at::Tensor>& tensors) {
+
+  std::function<void(std::unique_ptr<WorkEntry>&)> doNothing =
+    [this](std::unique_ptr<WorkEntry>& entry) {
+      auto data = (entry->src)[0];
+    };
+
+  auto entry = std::unique_ptr<WorkEntry>(
+      new WorkEntry(&tensors, nullptr, std::move(doNothing)));
+  return enqueue(std::move(entry));
+}
+
+
 c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupMPI::allreduce(
     std::vector<at::Tensor>& tensors,
     const AllreduceOptions& opts) {
diff --git a/torch/lib/c10d/ProcessGroupMPI.hpp b/torch/lib/c10d/ProcessGroupMPI.hpp
index 4e5b7f5e619..4c660c5c3fe 100644
--- a/torch/lib/c10d/ProcessGroupMPI.hpp
+++ b/torch/lib/c10d/ProcessGroupMPI.hpp
@@ -122,6 +122,9 @@ class ProcessGroupMPI : public ProcessGroup {
       std::vector<at::Tensor>& tensors,
       const AllreduceOptions& opts = AllreduceOptions()) override;
 
+  c10::intrusive_ptr<ProcessGroup::Work> get_dummy_work(
+      std::vector<at::Tensor>& tensors) override;
+
   c10::intrusive_ptr<ProcessGroup::Work> allreduce_coalesced(
       std::vector<at::Tensor>& tensors,
       const AllreduceCoalescedOptions& opts =
diff --git a/torch/lib/c10d/ProcessGroupNCCL.cpp b/torch/lib/c10d/ProcessGroupNCCL.cpp
index 19bcb67a59a..2e9a2de6781 100644
--- a/torch/lib/c10d/ProcessGroupNCCL.cpp
+++ b/torch/lib/c10d/ProcessGroupNCCL.cpp
@@ -1180,6 +1180,16 @@ c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::pointToPoint(
       [](std::vector<at::cuda::CUDAStream>&) {});
 }
 
+c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::get_dummy_work(
+    std::vector<at::Tensor>& tensors) {
+
+  const auto& devices = getDeviceList(tensors);
+  auto dummy_work = c10::make_intrusive<ProcessGroupNCCL::WorkNCCL>(devices, rank_, OpType::ALLREDUCE, "nccl:dummy_work");
+  dummy_work->outputs_ = std::make_shared<std::vector<at::Tensor>>(tensors);
+  return dummy_work;
+}
+
+
 c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce(
     std::vector<at::Tensor>& tensors,
     const AllreduceOptions& opts) {
diff --git a/torch/lib/c10d/ProcessGroupNCCL.hpp b/torch/lib/c10d/ProcessGroupNCCL.hpp
index bc47b06245b..08c84a72272 100644
--- a/torch/lib/c10d/ProcessGroupNCCL.hpp
+++ b/torch/lib/c10d/ProcessGroupNCCL.hpp
@@ -237,6 +237,9 @@ class ProcessGroupNCCL : public ProcessGroup {
       std::vector<at::Tensor>& tensors,
       const BroadcastOptions& opts = BroadcastOptions()) override;
 
+  c10::intrusive_ptr<ProcessGroup::Work> get_dummy_work(
+      std::vector<at::Tensor>& tensors) override;
+
   c10::intrusive_ptr<ProcessGroup::Work> allreduce(
       std::vector<at::Tensor>& tensors,
       const AllreduceOptions& opts = AllreduceOptions()) override;
diff --git a/torch/lib/c10d/ProcessGroupRoundRobin.cpp b/torch/lib/c10d/ProcessGroupRoundRobin.cpp
index 455c1654f58..e2eda0db7e7 100644
--- a/torch/lib/c10d/ProcessGroupRoundRobin.cpp
+++ b/torch/lib/c10d/ProcessGroupRoundRobin.cpp
@@ -17,6 +17,12 @@ ProcessGroupRoundRobin::ProcessGroupRoundRobin(
 
 ProcessGroupRoundRobin::~ProcessGroupRoundRobin() {}
 
+c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupRoundRobin::get_dummy_work(
+        std::vector<at::Tensor>& tensors) {
+  auto dummy = c10::make_intrusive<ProcessGroup::Work>();
+  return dummy;
+}
+
 c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupRoundRobin::broadcast(
     std::vector<at::Tensor>& tensors,
     const BroadcastOptions& opts) {
diff --git a/torch/lib/c10d/ProcessGroupRoundRobin.hpp b/torch/lib/c10d/ProcessGroupRoundRobin.hpp
index 6ce7a780415..716a0c40048 100644
--- a/torch/lib/c10d/ProcessGroupRoundRobin.hpp
+++ b/torch/lib/c10d/ProcessGroupRoundRobin.hpp
@@ -35,6 +35,9 @@ class ProcessGroupRoundRobin final : public ProcessGroup {
       std::vector<at::Tensor>& tensors,
       const BroadcastOptions& opts = BroadcastOptions()) override;
 
+  c10::intrusive_ptr<ProcessGroup::Work> get_dummy_work(
+      std::vector<at::Tensor>& tensors) override;
+
   c10::intrusive_ptr<ProcessGroup::Work> allreduce(
       std::vector<at::Tensor>& tensors,
       const AllreduceOptions& opts = AllreduceOptions()) override;
diff --git a/torch/lib/c10d/comm.cpp b/torch/lib/c10d/comm.cpp
index 1db8901b285..1acc582184a 100644
--- a/torch/lib/c10d/comm.cpp
+++ b/torch/lib/c10d/comm.cpp
@@ -3,6 +3,7 @@
 #include <deque>
 
 #include <ATen/core/functional.h>
+#include <c10/util/irange.h>
 #include <c10d/reducer.hpp>
 #include <torch/csrc/jit/python/pybind_utils.h>
 #include <torch/csrc/utils/tensor_flatten.h>
@@ -87,4 +88,16 @@ void broadcast_coalesced(
   }
 }
 
+std::vector<at::Tensor> GradBucket::getGradients() const {
+  std::vector<at::Tensor> per_parameter_tensors;
+  size_t num_parameters = offsets_.size();
+  per_parameter_tensors.reserve(num_parameters);
+  for (const auto i : c10::irange(num_parameters)) {
+    per_parameter_tensors.push_back(
+        tensors_[0].slice(0, offsets_[i], offsets_[i] + lengths_[i])
+            .view(sizes_vec_[i]));
+  }
+  return per_parameter_tensors;
+}
+
 } // namespace c10d
diff --git a/torch/lib/c10d/comm.hpp b/torch/lib/c10d/comm.hpp
index 3a39baccc95..a9a31590d59 100644
--- a/torch/lib/c10d/comm.hpp
+++ b/torch/lib/c10d/comm.hpp
@@ -65,6 +65,10 @@ class GradBucket {
     return sizes_vec_;
   }
 
+  // Each tensor in the list that getGradients corresponds to a
+  // parameter.
+  std::vector<at::Tensor> getGradients() const;
+
  private:
   size_t index_;
   std::vector<at::Tensor> tensors_;
diff --git a/torch/lib/c10d/reducer.cpp b/torch/lib/c10d/reducer.cpp
index 741e60b70d9..5b5d9990c21 100644
--- a/torch/lib/c10d/reducer.cpp
+++ b/torch/lib/c10d/reducer.cpp
@@ -15,6 +15,20 @@
 #include <torch/csrc/autograd/utils/lambda_post_hook.h>
 #include <torch/csrc/utils/memory.h>
 
+#include <torch/csrc/autograd/python_engine.h>
+
+#include <mutex>
+#include <condition_variable>
+#include <thread>
+#include <chrono>
+#include <unistd.h>
+
+#include <ATen/cuda/CUDAContext.h>
+
+#define LOCAL_AGGREGATION true
+// To measure local aggregation overhead
+//#define LOCAL_AGGREGATION false
+
 namespace c10d {
 namespace {
 
@@ -26,6 +40,14 @@ constexpr int kUnsetDivFactor = -1;
 
 } // namespace
 
+
+std::vector<Reducer*> Reducer::reducer_list;
+std::vector<std::vector<bool>> Reducer::bucket_checker;
+
+Barrier Reducer::barrier(0);
+std::mutex Reducer::barrier_init_mutex;
+std::condition_variable Reducer::barrier_init_condition;
+
 Reducer::Reducer(
     std::vector<std::vector<torch::autograd::Variable>> replicas,
     std::vector<std::vector<size_t>> bucket_indices,
@@ -33,7 +55,10 @@ Reducer::Reducer(
     std::vector<std::vector<bool>> expect_sparse_gradients,
     int64_t bucket_bytes_cap,
     bool find_unused_parameters,
-    bool gradient_as_bucket_view)
+    bool gradient_as_bucket_view,
+    int model_index,
+    int num_local_models,
+    int total_num_models)
     : replicas_(std::move(replicas)),
       process_group_(std::move(process_group)),
       expect_sparse_gradients_(std::move(expect_sparse_gradients)),
@@ -43,6 +68,9 @@ Reducer::Reducer(
       has_marked_unused_parameters_(false),
       find_unused_parameters_(find_unused_parameters),
       gradient_as_bucket_view_(gradient_as_bucket_view),
+      model_index_(model_index),
+      num_local_models_(num_local_models),
+      total_num_models_(total_num_models),
       local_used_maps_reduced_(false),
       backward_stats_base_(0),
       has_rebuilt_bucket_(false),
@@ -54,6 +82,20 @@ Reducer::Reducer(
   TORCH_CHECK(replicas_.size() >= 1, "Expected at least one model replica.");
   TORCH_CHECK(replicas_[0].size() >= 1, "Expected at least one parameter.");
 
+  if (model_index_ == 0 && process_group_->getRank() == 0) {
+    std::cout << "======================================================" << std::endl;
+    std::cout << "[INFO][torch/lib/c10d/reducer.cpp]" << std::endl;
+    std::cout << "LOCAL AGGREGATION: " << LOCAL_AGGREGATION << std::endl;
+    std::cout << "Number of local models (VSWs) on GPU: " << num_local_models << std::endl;
+    std::cout << "Number of total models (VSWs x GA step x world size): " << total_num_models << std::endl;
+    std::cout << "[DEBUG] gradient_as_bucket_view_: " << gradient_as_bucket_view_ << std::endl;
+    std::cout << "======================================================" << std::endl;
+  }
+
+  int num_buckets = bucket_indices.size();
+  //std::cout << "[INFO][torch/lib/c10d/reducer.cpp] Reducer of model idx: " << model_index_
+  //          << " is called (total number of local models: " << num_local_models << ")"  << std::endl;
+
   // If `expect_sparse_gradients` is not specified, initialize it such that
   // we do not expect sparse gradients for any parameter.
   if (expect_sparse_gradients_.empty()) {
@@ -66,7 +108,10 @@ Reducer::Reducer(
   // replicas within this process and across processes.
   // (see Note:  "Gradient Layout Contract" in initialize_buckets).
   verify_replicas_within_process();
-  verify_replica0_across_processes();
+
+  if (model_index_ == 0) {
+    verify_replica0_across_processes();
+  }
 
   // Initialize variable bucketing.
   // This can be reinitialized later after capturing runtime information.
@@ -82,10 +127,18 @@ Reducer::Reducer(
   // can be marked as ready for reduction.
   {
     const auto replica_count = replicas_.size();
+    if (model_index_ == 0 && process_group_->getRank() == 0) {
+      std::cout << "[INFO][torch/lib/c10d/reducer.cpp] replica count: "
+                << replica_count << std::endl;
+    }
     grad_accumulators_.resize(replica_count);
     for (size_t replica_index = 0; replica_index < replica_count;
          replica_index++) {
       const auto variable_count = replicas_[replica_index].size();
+      if (model_index_ == 0 && process_group_->getRank() == 0) {
+        std::cout << "[INFO][torch/lib/c10d/reducer.cpp] variable count: "
+                  << variable_count << std::endl;
+      }
       grad_accumulators_[replica_index].resize(variable_count);
       for (size_t variable_index = 0; variable_index < variable_count;
            variable_index++) {
@@ -186,6 +239,45 @@ Reducer::Reducer(
       }
     }
   }
+
+  // To work with multiple local models (= DDPs) on a single GPU
+  reducer_list.push_back(this);
+  std::vector<bool> checker(num_buckets, false);
+  bucket_checker.push_back(checker);
+  /*
+  std::cout << "[INFO][torch/lib/c10d/reducer.cpp] Model "
+            << this->model_index_ << " has " << num_buckets << " buckets"
+            << std::endl;
+  */
+  //std::cout <<"[DEBUG] length of reducer_list: " << reducer_list.size() << std::endl;
+  if (this->model_index_ == 0) {
+    std::lock_guard<std::mutex> lock(barrier_init_mutex);
+    barrier.size = num_local_models;
+    barrier.remaining = num_local_models;
+    barrier_init_condition.notify_all();
+  } else {
+    std::unique_lock<std::mutex> lock(barrier_init_mutex);
+    barrier_init_condition.wait(lock, []() { return barrier.size > 0; } );
+  }
+
+  if (this->model_index_ == num_local_models-1) {
+    // IIDP: [For test code]
+    // Since test code calls different DDP instances sequentially in one process,
+    // static variables (e.g, reducer_list, bucket_checker) are not initialized.
+    //std::cout << "[DEBUG][reducer.cpp] rank: " << process_group_->getRank() << " | "
+    //          << "reducer_list.size(): " << reducer_list.size() << " | "
+    //          << "num_local_models: " << num_local_models << std::endl;
+    while (reducer_list.size() != num_local_models) {
+      TORCH_WARN_ONCE(
+        "Erase previous Virtual Reducer for the test code. "
+        "Unless the test code is intentionally executed, "
+        "this should never be happened.");
+      reducer_list.erase(reducer_list.begin());
+      bucket_checker.erase(bucket_checker.begin());
+    }
+    TORCH_INTERNAL_ASSERT(reducer_list.size() == num_local_models);
+    TORCH_INTERNAL_ASSERT(bucket_checker.size() == num_local_models);
+  }
 }
 
 // Note [Skip allreducing local_used_maps_dev]
@@ -377,7 +469,10 @@ void Reducer::copy_grad_to_bucket(
     // Divides while copying into the bucket view.
     at::native::mul_out(bucket_view, grad, wrapped);
   } else {
+    // ------------------------
+    // Original code
     bucket_view.copy_(grad);
+    // ------------------------
   }
 }
 
@@ -407,20 +502,24 @@ void Reducer::mark_variable_ready_dense(VariableIndex index) {
       // to bucket_view. If grad has already been set as views of buckets in
       // previous iterations, no copy is needed.
       if (!grad.is_alias_of(bucket_view)) {
+        //std::cout << "[DEBUG] mark_variable_ready_dense() | !grad.is_alias_of(bucket_view)) " << std::endl;
         this->copy_grad_to_bucket(grad, bucket_view);
         if (gradient_as_bucket_view_) {
+          //std::cout << "[DEBUG] mark_variable_ready_dense() | gradient_as_bucket_view_ " << std::endl;
           // Let grad point to bucket_view buffer.
           grad = bucket_view;
           // The grad is modified and need to be written back.
           return true;
         }
       } else {
+        //std::cout << "[DEBUG] mark_variable_ready_dense() | bucket_view.div_(divFactor_); " << std::endl;
         // If grad and bucket view point to the same storage, no need to copy
         if (comm_hook_ == nullptr) {
           bucket_view.div_(divFactor_);
         }
       }
     } else {
+      //std::cout << "[DEBUG] mark_variable_ready_dense() | bucket_view.zero_(); " << std::endl;
       bucket_view.zero_();
     }
     // The grad is not modified and doesn't need to be written back.
@@ -480,6 +579,34 @@ void Reducer::set_forward_pass_work_handle(
   forwardPassWorkHandle_.useStaticWorldSize = useStaticWorldSize;
 }
 
+void Reducer::reconfigure(int num_local_models, int total_num_models) {
+  total_num_models_ = total_num_models;
+  divFactor_ = total_num_models_;
+  num_local_models_ = num_local_models;
+
+  if (this->model_index_ == 0) {
+    std::lock_guard<std::mutex> lock(barrier_init_mutex);
+    barrier.size = num_local_models;
+    barrier.remaining = num_local_models;
+    barrier_init_condition.notify_all();
+  } else {
+    std::unique_lock<std::mutex> lock(barrier_init_mutex);
+    barrier_init_condition.wait(lock, []() { return barrier.size > 0; } );
+  }
+
+  if (this->model_index_ == num_local_models-1) {
+    //std::cout << "[DEBUG][reducer.cpp] reconfigure() - rank: " << process_group_->getRank() << " | "
+    //          << "reducer_list.size(): " << reducer_list.size() << " | "
+    //          << "num_local_models: " << num_local_models << std::endl;
+    while (reducer_list.size() != num_local_models) {
+      reducer_list.erase(reducer_list.end()-1);
+      bucket_checker.erase(bucket_checker.end()-1);
+    }
+    TORCH_INTERNAL_ASSERT(reducer_list.size() == num_local_models);
+    TORCH_INTERNAL_ASSERT(bucket_checker.size() == num_local_models);
+  }
+}
+
 std::vector<at::Tensor> Reducer::get_local_used_maps_on_device() const {
   std::lock_guard<std::mutex> lock(mutex_);
   return local_used_maps_dev_;
@@ -578,6 +705,10 @@ void Reducer::mark_variable_ready(VariableIndex index) {
   const auto& bucket_index = variable_locators_[variable_index];
   auto& bucket = buckets_[bucket_index.bucket_index];
   auto& replica = bucket.replicas[replica_index];
+  // [GC] Check variable's grad which is generated by autograd engine if finite
+  //auto& variable = replica.variables[bucket_index.intra_bucket_index];
+  //bool is_grad_finite = variable.mutable_grad().isfinite().all().is_nonzero();
+  //TORCH_INTERNAL_ASSERT(is_grad_finite);
 
   // Something is wrong if all variables contained in this bucket replica have
   // already been marked as ready.
@@ -615,7 +746,9 @@ void Reducer::mark_variable_ready(VariableIndex index) {
   // If it was scheduled, wait on allreduce in forward pass that tells us
   // division factor based on no. of currently participating processes.
   if (divFactor_ == kUnsetDivFactor) {
-    divFactor_ = process_group_->getSize();
+    /* original code */
+    //divFactor_ = process_group_->getSize();
+    divFactor_ = total_num_models_;
     auto& workHandle = forwardPassWorkHandle_.workHandle;
     if (workHandle && !forwardPassWorkHandle_.useStaticWorldSize) {
       workHandle->wait();
@@ -635,7 +768,7 @@ void Reducer::mark_variable_ready(VariableIndex index) {
 
   // TODO(@pietern): Make this work for both CPU/CUDA tensors.
   // When using CPU tensors we don't need to do this.
-  // // Record event so that we can wait for all of them.
+  // Record event so that we can wait for all of them.
   // auto& event = replica.events[bucket_index.intra_bucket_index];
   // event.record();
 
@@ -643,6 +776,9 @@ void Reducer::mark_variable_ready(VariableIndex index) {
   if (--replica.pending == 0) {
     // Kick off reduction if all replicas for this bucket are ready.
     if (--bucket.pending == 0) {
+      if (this->model_index_ != 0) {
+        replica.event.record();
+      }
       mark_bucket_ready(bucket_index.bucket_index);
     }
   }
@@ -658,7 +794,10 @@ void Reducer::mark_variable_ready(VariableIndex index) {
         // allreduce respect the current stream, so will be sequenced correctly.
         local_used_maps_dev_[i].copy_(local_used_maps_[i], true);
       }
-      local_used_work_ = process_group_->allreduce(local_used_maps_dev_);
+
+      if (model_index_ == 0) {
+        local_used_work_ = process_group_->allreduce(local_used_maps_dev_);
+      }
     }
 
     // The autograd engine uses the default stream when running callbacks, so we
@@ -668,7 +807,7 @@ void Reducer::mark_variable_ready(VariableIndex index) {
         c10::impl::VirtualGuardImpl{deviceType};
     const c10::Stream currentStream =
         guard.getStream(replica.contents.device());
-    torch::autograd::Engine::get_default_engine().queue_callback([=] {
+    torch::autograd::python::PythonEngine::get_python_engine(this->model_index_).queue_callback([=] {
       std::lock_guard<std::mutex> lock(this->mutex_);
       // Run callback with the current stream
       c10::OptionalStreamGuard currentStreamGuard{currentStream};
@@ -681,21 +820,29 @@ void Reducer::mark_variable_ready(VariableIndex index) {
 void Reducer::mark_bucket_ready(size_t bucket_index) {
   TORCH_INTERNAL_ASSERT(bucket_index >= next_bucket_);
 
+  if (this->model_index_ != 0) {
+    this->bucket_checker[this->model_index_][bucket_index] = true;
+  }
   // Buckets are reduced in sequence. Ignore this bucket if
   // it's not its turn to be reduced.
   if (bucket_index > next_bucket_) {
     return;
   }
 
+  barrier.arrive_and_wait();
+
   // Keep going, until we either:
   // - have kicked off reduction for all buckets, or
   // - found a bucket that's not yet ready for reduction.
+
   for (; next_bucket_ < buckets_.size() && buckets_[next_bucket_].pending == 0;
        next_bucket_++) {
     auto& bucket = buckets_[next_bucket_];
     std::vector<at::Tensor> tensors;
     tensors.reserve(bucket.replicas.size());
-    for (const auto& replica : bucket.replicas) {
+    int replica_idx = 0;
+    for (auto& replica : bucket.replicas) {
+      //std::cout << "[DEBUG] replica_idx: " << replica_idx << std::endl;
       // TODO(@pietern): Ensure proper synchronization with the CUDA events
       // that recorded copies into this contents tensor. If these copies are
       // executed on non-default streams, the current stream for the device
@@ -704,14 +851,69 @@ void Reducer::mark_bucket_ready(size_t bucket_index) {
       // As long as autograd uses the default stream for every device,
       // these operations are implicitly sequenced, and we don't need to
       // do any extra synchronization here.
-      //
+
+      // Do local aggregation in model index 0
+      if (LOCAL_AGGREGATION == true && this->model_index_ == 0 && this->num_local_models_ > 1) {
+        //for (Reducer* virtual_reducer : this->reducer_list) {
+        for (int i = 0 ; i < num_local_models_ ; i++) {
+          Reducer* virtual_reducer = this->reducer_list[i];
+          if (virtual_reducer->model_index_ == 0)
+            continue;
+          TORCH_INTERNAL_ASSERT(this->bucket_checker[virtual_reducer->model_index_][next_bucket_] == true);
+          auto& aggregate_bucket = virtual_reducer->buckets_[next_bucket_];
+          auto& aggregate_replica = aggregate_bucket.replicas[replica_idx];
+          auto& aggregate_event = aggregate_replica.event;
+          if (replica.contents.sizes() != aggregate_replica.contents.sizes()) {
+            std::cout << "[ERROR] replica.contents.sizes() != aggregate_replica.contents.sizes() "
+                      << replica.contents.sizes() << " " << aggregate_replica.contents.sizes() << std::endl;
+            TORCH_INTERNAL_ASSERT(replica.contents.sizes() == aggregate_replica.contents.sizes());
+          }
+          at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream();
+          aggregate_event.block(stream);
+          /*
+          std::cout << "[DEBUG] -- Before summation -- rank: " << process_group_->getRank()
+                    << " bucket index: " << bucket_index
+                    << " replica.contents: " << replica.contents[0] << std::endl;
+          std::cout << "[DEBUG] -- Before summation -- rank: " << process_group_->getRank()
+                    << " model index: " << virtual_reducer->model_index_
+                    << " bucket index: " << bucket_index
+                    << " | aggregate_replica.contents: " << aggregate_replica.contents[0] << std::endl;
+          */
+         /* [To debug test code]
+          double rtol=1e-4;
+          double atol=1e-4;
+          if (not replica.contents.allclose(aggregate_replica.contents, rtol=rtol, atol=atol)) {
+          //if (not replica.contents.equal(aggregate_replica.contents)) {
+            std::cout << "[DEBUG] [ERROR] rank: " << process_group_->getRank()
+                      << " model index: " << this->model_index_
+                      << " bucket index: " << bucket_index
+                      << " replica.contents: " << replica.contents[0]
+                      << "aggregate_replica.contents: " << aggregate_replica.contents[0]
+                      << std::endl;
+            TORCH_INTERNAL_ASSERT(false);
+          }
+          */
+          replica.contents.add_(aggregate_replica.contents);
+          //std::cout << "[DEBUG] -- After summation -- rank: " << process_group_->getRank()
+          //          << " bucket index: " << bucket_index
+          //          << " replica.contents: " << replica.contents[0] << std::endl;
+        }
+      }
+      //std::cout << "[DEBUG] -- Prepare for All-reduce -- rank: " << process_group_->getRank()
+      //    << " replica.contents: " << replica.contents[0] << std::endl;
       tensors.push_back(replica.contents);
+      replica_idx++;
     }
     // See Note [DDP Communication Hook]
     // TODO(@sinannasir): merge `work` and `future_work`. Related to GH Issue
     // #41266.
     if (comm_hook_ == nullptr) {
-      bucket.work = process_group_->allreduce(tensors);
+      if (this->model_index_ == 0) {
+        bucket.work = process_group_->allreduce(tensors);
+      }
+      else {
+        bucket.work = process_group_->get_dummy_work(tensors);
+      }
     } else {
       GradBucket grad_bucket(
           next_bucket_,
@@ -724,6 +926,11 @@ void Reducer::mark_bucket_ready(size_t bucket_index) {
       bucket.future_work = comm_hook_->runHook(grad_bucket);
     }
   }
+
+  barrier.arrive_and_wait();
+  if (this->model_index_ != 0) {
+    this->bucket_checker[this->model_index_][bucket_index] = false;
+  }
 }
 
 void Reducer::initialize_buckets(
@@ -933,11 +1140,15 @@ void Reducer::initialize_bucket_views(
       auto& bucket_view = replica.bucket_views_in.back();
       runGradCallbackForVariable(v, [&](auto& grad) {
         if (grad.defined() && !grad.is_alias_of(bucket_view)) {
+          //std::cout << "[DEBUG] In initialize_bucket_views(), "
+          //          << "gradient_as_bucket_view_ and modified condition enters!" << std::endl;
           bucket_view.copy_(grad);
           grad = bucket_view;
           // The grad is modefied and needs to be written back.
           return true;
         }
+        //std::cout << "[DEBUG] In initialize_bucket_views(), "
+        //  << "gradient_as_bucket_view_ not modified!" << std::endl;
         // The grad is not modified and does not need to be written back.
         return false;
       });
@@ -1115,7 +1326,9 @@ void Reducer::finalize_bucket_dense(Bucket& bucket) {
             local_used_maps_[replica_index][variable_index].item<int>() == 0;
         if (global_unused && !local_used_maps_reduced_) {
           // Wait for local_used_maps reduction to complete.
-          local_used_work_->wait();
+          if (model_index_ == 0) {
+            local_used_work_->wait();
+          }
           // D2H from local_used_maps_dev_ to local_used_maps_
           for (size_t i = 0; i < local_used_maps_.size(); i++) {
             local_used_maps_[i].copy_(local_used_maps_dev_[i]);
@@ -1207,18 +1420,19 @@ void Reducer::finalize_backward() {
           "Expected bucket.future_work not to be null. "
           "This may indicate that communication hook was not properly installed.");
       bucket.future_work->wait();
-
-      auto future_result =
-          comm_hook_->parseHookResult(bucket.future_work->value());
-
-      for (size_t i = 0; i < future_result.size(); i++) {
-        auto& replica = bucket.replicas[i];
-        if (bucket.expect_sparse_gradient) {
-          replica.contents.copy_(future_result[i]);
-        } else {
-          // Reinitialize only `bucket_views_out` with the future_result by
-          // following the same logic in `initialize_buckets`.
-          populate_bucket_views_out(replica, future_result[i]);
+      if (this->model_index_ == 0) {
+        auto future_result =
+            comm_hook_->parseHookResult(bucket.future_work->value());
+
+        for (size_t i = 0; i < future_result.size(); i++) {
+          auto& replica = bucket.replicas[i];
+          if (bucket.expect_sparse_gradient) {
+            replica.contents.copy_(future_result[i]);
+          } else {
+            // Reinitialize only `bucket_views_out` with the future_result by
+            // following the same logic in `initialize_buckets`.
+            populate_bucket_views_out(replica, future_result[i]);
+          }
         }
       }
     }
@@ -1242,7 +1456,7 @@ void Reducer::finalize_backward() {
     // complete before kicking off next one. Otherwise the previous one may
     // interfere, write to the device-side memory and clobber the content of
     // local_unused_maps_dev_.
-    if (!local_used_maps_reduced_) {
+    if (!local_used_maps_reduced_ && model_index_ == 0) {
       local_used_work_->wait();
     }
     local_used_maps_reduced_ = false;
@@ -1256,7 +1470,7 @@ void Reducer::runGradCallbackForVariable(
   if (context_ptr == nullptr) {
     cb(variable.mutable_grad());
   } else {
-    // Under distributed autograd
+// Under distributed autograd
 #ifndef _WIN32
     context_ptr->runGradCallbackForVariable(variable, std::move(cb));
 #endif
@@ -1353,6 +1567,7 @@ bool Reducer::rebuild_buckets() {
   // has unused parameters for example, this will raise an error recommending to
   // run with find_unused_parameters=True, instead of the size mismatch
   // exception below.
+  //
   ensure_prior_reduction_finished();
   std::lock_guard<std::mutex> lock(mutex_);
   if (!should_rebuild_buckets() || rebuilt_params_.empty()) {
@@ -1387,11 +1602,24 @@ bool Reducer::rebuild_buckets() {
   // For rebuilt bucket indices, it needs to be synced across all ranks.
   // Broadcast the newly rebuilt bucket indices from rank 0 in default.
   // After syncing up rebuilt bucket indices, initialize buckets for reducer.
-  sync_bucket_indices(rebuilt_bucket_indices);
+  // [IIDP] Not need to sync rebuilt bucket indices. When it's done, stuck happens.
+  // sync_bucket_indices(rebuilt_bucket_indices);
+  if (process_group_->getRank() == 0) {
+    std::cout << "[DEBUG][torch/lib/c10d/reducer.cpp] Reducer::rebuild_buckets() -  rebuilt bucket size: "
+              << rebuilt_bucket_indices.size()
+              << std::endl;
+  }
+  /*
+  std::cout << "[DEBUG][torch/lib/c10d/reducer.cpp] rebuilt bucket indices: " << std::endl;
+  for (int i = 0; i < rebuilt_bucket_indices.size() ; i++) {
+    std::cout << "bucket idx: " << i << "- [" << rebuilt_bucket_indices[i] << "]" << std::endl;
+  }
+  */
 
   has_rebuilt_bucket_ = true;
   rebuilt_params_.clear();
   rebuilt_param_indices_.clear();
+  rebuilt_bucket_indices_ = rebuilt_bucket_indices; // To used in get_rebuilt_bucket_indices()
 
   initialize_buckets(std::move(rebuilt_bucket_indices));
   return true;
@@ -1407,7 +1635,6 @@ void Reducer::register_comm_hook(std::unique_ptr<CommHookInterface> iface) {
   TORCH_CHECK(
       replicas_.size() == 1,
       "Communication hook does not support single-process multiple-device mode.");
-
   comm_hook_ = std::move(iface);
 }
 
@@ -1517,6 +1744,17 @@ inline bool operator==(const BucketKey& lhs, const BucketKey& rhs) {
 
 } // namespace
 
+bool check_expect_bucket_size(size_t cur_bucket_number) {
+  if (Reducer::bucket_checker.empty()) {
+    return true;
+  }
+  size_t prev_bucket_number = Reducer::bucket_checker.size();
+  if (prev_bucket_number == cur_bucket_number) {
+    return false;
+  }
+  return true;
+}
+
 std::vector<std::vector<size_t>> compute_bucket_assignment_by_size(
     const std::vector<at::Tensor>& tensors,
     const std::vector<size_t>& bucket_size_limits,
diff --git a/torch/lib/c10d/reducer.hpp b/torch/lib/c10d/reducer.hpp
index ea06276a795..e2015c54d8e 100644
--- a/torch/lib/c10d/reducer.hpp
+++ b/torch/lib/c10d/reducer.hpp
@@ -15,11 +15,40 @@
 #include <torch/csrc/autograd/variable.h>
 #include <torch/csrc/distributed/autograd/context/context.h>
 
+#include <iostream>
+#include <condition_variable>
+
+#include <ATen/cuda/CUDAEvent.h> // For bucket's events
+
 namespace c10d {
 
 constexpr int kDefaultFirstBucketBytes = int(1024 * 1024);
 constexpr int kDefaultBucketBytesCap = int(25 * 1024 * 1024);
 
+struct Barrier {
+  mutable std::mutex m;
+  std::condition_variable cv;
+  std::size_t size;
+  std::ptrdiff_t remaining;
+  std::ptrdiff_t phase = 0;
+
+  Barrier(std::size_t s):
+    size(s), remaining(s) {}
+
+  void arrive_and_wait() {
+    auto l = std::unique_lock<std::mutex>(m);
+    --remaining;
+    if (remaining != 0) {
+      auto myphase = phase + 1;
+      cv.wait(l, [&]{ return myphase - phase <= 0; });
+    } else {
+      remaining = size;
+      ++phase;
+      cv.notify_all();
+    }
+  }
+};
+
 class Reducer {
  public:
   // The constructor takes a list of variables for every model replica.
@@ -33,7 +62,10 @@ class Reducer {
       std::vector<std::vector<bool>> expect_sparse_gradients,
       int64_t bucket_bytes_cap,
       bool find_unused_parameters,
-      bool gradient_as_bucket_view);
+      bool gradient_as_bucket_view,
+      int model_index,
+      int num_local_models,
+      int total_num_models);
 
   ~Reducer() noexcept(false);
 
@@ -94,12 +126,18 @@ class Reducer {
   // Pushes all parameters to be rebuilt.
   void push_rebuilt_params_for_all_indices();
 
+  std::vector<std::vector<size_t>> get_rebuilt_bucket_indices() const {
+    return rebuilt_bucket_indices_;
+  }
+
   // Creates and sets ForwardPassWorkHandle given a ProcessGroup::Work and the
   // corresponding tensor being reduced.
   void set_forward_pass_work_handle(
       c10::intrusive_ptr<c10d::ProcessGroup::Work> forwardPassWorkHandle,
       bool useStaticWorldSize);
 
+  void reconfigure(int num_local_models, int total_num_models);
+
   // Retrieve on-device tensors used to track locally unused parameters. For
   // each replica, it is a tensor where index i = 1 if the Variable with that
   // index has been used.
@@ -117,6 +155,18 @@ class Reducer {
   // in the applications.
   c10::DDPLoggingData get_ddp_logging_data();
 
+  int model_index_;
+  int num_local_models_;
+  int total_num_models_;
+
+  static std::vector<Reducer*> reducer_list;
+  static std::vector<std::vector<bool>> bucket_checker;
+
+  static std::condition_variable barrier_init_condition;
+  static std::mutex barrier_init_mutex;
+  static Barrier barrier;
+
+
  protected:
   // Forward declaration.
   struct Bucket;
@@ -248,12 +298,15 @@ class Reducer {
     // Number of tensors to be added before this bucket is complete.
     // This is reset to `variables.size()` every iteration.
     size_t pending;
+    bool modified = false;
 
     // TODO(@pietern)
     // Memory copies from gradient tensors into the bucket are potentially
     // done on different CUDA streams. We record an event for every copy
     // so that we can synchronize with them prior to kicking off the reduction.
     // std::vector<at::cuda::CUDAEvent> events;
+
+    at::cuda::CUDAEvent event;
   };
 
   // This function is called inside `initialize_buckets`, it initializes both
@@ -339,6 +392,7 @@ class Reducer {
   std::vector<at::Tensor> rebuilt_params_;
   std::vector<int64_t> rebuilt_param_indices_;
   const int64_t bucket_bytes_cap_;
+  std::vector<std::vector<size_t>> rebuilt_bucket_indices_;
 
   struct RpcContext {
     using ContextPtr = torch::distributed::autograd::ContextPtr;
diff --git a/torch/nn/modules/module.py b/torch/nn/modules/module.py
index f99a588a2e3..0bb317efcc7 100644
--- a/torch/nn/modules/module.py
+++ b/torch/nn/modules/module.py
@@ -258,6 +258,7 @@ class Module:
         self._buffers = OrderedDict()
         self._non_persistent_buffers_set = set()
         self._backward_hooks = OrderedDict()
+        self._backward_pre_hooks = OrderedDict()
         self._is_full_backward_hook = None
         self._forward_hooks = OrderedDict()
         self._forward_pre_hooks = OrderedDict()
@@ -671,6 +672,11 @@ class Module:
             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
 
         return self._apply(convert)
+    
+    def reset_hooks(self):
+        self._backward_hooks = OrderedDict()
+        self._backward_pre_hooks = OrderedDict()
+        self._forward_hooks = OrderedDict()
 
     def register_backward_hook(
         self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
@@ -695,6 +701,15 @@ class Module:
         handle = hooks.RemovableHandle(self._backward_hooks)
         self._backward_hooks[handle.id] = hook
         return handle
+    
+    def register_backward_pre_hook(self, hook):
+        r"""Registers a backward pre-hook on the module.
+
+        The hook will be called every time before :func:`backward` is invoked.
+        """
+        handle = hooks.RemovableHandle(self._backward_pre_hooks)
+        self._backward_pre_hooks[handle.id] = hook
+        return handle
 
     def register_full_backward_hook(
         self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]
@@ -912,6 +927,25 @@ class Module:
                     functools.update_wrapper(wrapper, hook)
                     grad_fn.register_hook(wrapper)
                 self._maybe_warn_non_full_backward_hook(input, result, grad_fn)
+                
+        if len(self._backward_pre_hooks) > 0:
+            var = result
+            while not isinstance(var, torch.Tensor):
+                if isinstance(var, dict):
+                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
+                else:
+                    var = var[0]
+            grad_fn = var.grad_fn
+            if grad_fn is not None:
+                for hook in self._backward_pre_hooks.values():
+                    wrapper = functools.partial(hook, self)
+                    functools.update_wrapper(wrapper, hook)
+                    try:
+                        grad_fn.register_pre_hook(wrapper)
+                    except Exception as e:
+                        print("Error in registering pre-hook")
+                        print("Error: %s" % e)
+                        continue
 
         return result
 
diff --git a/torch/nn/parallel/distributed.py b/torch/nn/parallel/distributed.py
index ff7af61216f..577f328fc14 100644
--- a/torch/nn/parallel/distributed.py
+++ b/torch/nn/parallel/distributed.py
@@ -26,6 +26,8 @@ from .parallel_apply import parallel_apply
 from torch._utils import _get_device_index, _get_all_device_indices
 from ._functions import _get_stream
 
+local_DDP_list = {}
+
 
 def _find_tensors(obj):
     r"""
@@ -359,7 +361,12 @@ class DistributedDataParallel(Module):
                  bucket_cap_mb=25,
                  find_unused_parameters=False,
                  check_reduction=False,
-                 gradient_as_bucket_view=False):
+                 gradient_as_bucket_view=False,
+                 model_index=0,
+                 broadcast_master=0,
+                 num_local_models=1,
+                 total_num_models=1,
+                 sync_buffer_barrier=[None, None]):
 
         super(DistributedDataParallel, self).__init__()
 
@@ -368,6 +375,24 @@ class DistributedDataParallel(Module):
             "doesn't have any parameter that requires a gradient."
         )
 
+        self.model_index = model_index
+        self.broadcast_master = broadcast_master
+        self.num_local_models = num_local_models
+        self.total_num_models = total_num_models
+        if model_index == 0:
+            global local_DDP_list
+            local_DDP_list['0'] = self
+        #local_DDP_list[str(model_index)] = self
+        # It is used for '_create_optimizer_hook()' in torch/cuda/trainer.py
+        self.bucket_indices = []
+        self.ddp_register_params = []
+        self._has_rebuilt_buckets = False
+
+        self.sync_buffer_barrier = sync_buffer_barrier
+        if self.num_local_models > 1 and not self.sync_buffer_barrier[0]:
+            raise ValueError(
+                "If number of local models > 1, then sync_buffer_barrier must be configured.")
+
         self.is_multi_device_module = len({p.device for p in module.parameters()}) > 1
         distinct_device_types = {p.device.type for p in module.parameters()}
         assert len(distinct_device_types) == 1, (
@@ -454,10 +479,14 @@ class DistributedDataParallel(Module):
                 module_states.append(param)
 
         if len(module_states) > 0:
-            self._distributed_broadcast_coalesced(
-                module_states,
-                self.broadcast_bucket_size,
-                authoritative_rank)
+            if self.model_index == 0:
+                self._distributed_broadcast_coalesced(
+                    module_states,
+                    self.broadcast_bucket_size,
+                    authoritative_rank)
+            else:
+                local_master = local_DDP_list['0']
+                self.module.load_state_dict(local_master.module.state_dict())
 
     def _ddp_init_helper(self):
         """
@@ -587,6 +616,9 @@ class DistributedDataParallel(Module):
             list(produces_sparse_gradient(module) for module, _ in replica)
             for replica in modules_and_parameters]
 
+        # To assign an index of actual parameter in a gradient bucket
+        for index, param in enumerate(parameters[0]):
+            param.index = index
         # The bucket size limit is specified in the constructor.
         # Additionally, we allow for a single small bucket for parameters
         # that are defined first, such that their gradients don't spill into
@@ -596,7 +628,12 @@ class DistributedDataParallel(Module):
             parameters[0],
             [dist._DEFAULT_FIRST_BUCKET_BYTES, self.bucket_bytes_cap],
             expect_sparse_gradient[0])
-
+        self.bucket_indices = list(reversed(bucket_indices))
+        if self.model_index == 0:
+            torch.iidp.utils.distributed.print_one_rank(
+                f'[distributed.py] model index: 0 | self.bucket_indices: {self.bucket_indices}',
+                'debug'
+            )
         # Note: reverse list of buckets because we want to approximate the
         # order in which their gradients are produced, and assume they
         # are used in the forward pass in the order they are defined.
@@ -607,7 +644,12 @@ class DistributedDataParallel(Module):
             expect_sparse_gradient,
             self.bucket_bytes_cap,
             self.find_unused_parameters,
-            self.gradient_as_bucket_view)
+            self.gradient_as_bucket_view,
+            self.model_index,
+            self.num_local_models,
+            self.total_num_models)
+
+        self.ddp_register_params = parameters[0]
 
         # Set logging data that can be got during construction time.
         dist._set_construction_logging_data(
@@ -673,7 +715,14 @@ class DistributedDataParallel(Module):
         finally:
             self.require_backward_grad_sync = old_require_backward_grad_sync
 
+    def reconfigure(self, num_local_models, total_num_models, sync_buffer_barrier):
+        self.num_local_models = num_local_models
+        self.total_num_models = total_num_models
+        self.sync_buffer_barrier = sync_buffer_barrier
+        self.reducer.reconfigure(self.num_local_models, self.total_num_models)
+
     def forward(self, *inputs, **kwargs):
+        #print(f'[DEBUG][torch/nn/parallel/distributed.py] rank: {dist.get_rank()} | forward!')
         if self.ddp_uneven_inputs_config.ddp_join_enabled:
             ones = torch.ones(
                 1, device=self.device
@@ -691,14 +740,20 @@ class DistributedDataParallel(Module):
         # This should be called only once during whole training period.
         if self.reducer._rebuild_buckets():
             logging.info("Reducer buckets have been rebuilt in this iteration.")
-
+            self._has_rebuilt_buckets = True
+            self.bucket_indices = self.reducer.get_rebuilt_bucket_indices()
+            if self.model_index == 0:
+                torch.iidp.utils.distributed.print_one_rank(
+                    f'[distributed.py] model index: 0 | rebuild => self.bucket_indices: {self.bucket_indices}',
+                    'debug')
+        #print(f'[DEBUG][torch/nn/parallel/distributed.py] rank: {dist.get_rank()} | before _sync_params!')
         if self.require_forward_param_sync:
             self._sync_params()
-
+        #print(f'[DEBUG][torch/nn/parallel/distributed.py] rank: {dist.get_rank()} | after _sync_params!')
         if self.ddp_uneven_inputs_config.ddp_join_enabled:
             # Notify joined ranks whether they should sync in backwards pass or not.
             self._check_global_requires_backward_grad_sync(is_joined_rank=False)
-
+        #print(f'[DEBUG][torch/nn/parallel/distributed.py] rank: {dist.get_rank()} | before module forward!')
         if self.device_ids:
             if len(self.device_ids) == 1:
                 inputs, kwargs = self.to_kwargs(inputs, kwargs, self.device_ids[0])
@@ -709,7 +764,7 @@ class DistributedDataParallel(Module):
                 output = self.gather(outputs, self.output_device)
         else:
             output = self.module(*inputs, **kwargs)
-
+        #print(f'[DEBUG][torch/nn/parallel/distributed.py] rank: {dist.get_rank()} | after module forward!')
         if torch.is_grad_enabled() and self.require_backward_grad_sync:
             self.require_forward_param_sync = True
             # We'll return the output object verbatim since it is a freeform
@@ -824,6 +879,8 @@ class DistributedDataParallel(Module):
         if self.will_sync_module_buffers():
             my_rank = dist.get_rank(self.process_group)
             authoritative_rank = self._find_common_rank(my_rank, False)
+            if self.model_index > 0:
+                assert(False)
             self._distributed_broadcast_coalesced(
                 self.modules_buffers[0], self.broadcast_bucket_size, authoritative_rank
             )
@@ -998,7 +1055,8 @@ class DistributedDataParallel(Module):
                         self.reducer._rebuild_buckets()
                         # Schedule a corresponding broadcast if we are syncing module
                         # buffers in the forward pass.
-                        self._check_and_sync_module_buffers()
+                        if self.model_index == 0:
+                            self._check_and_sync_module_buffers()
 
                         (
                             work,
@@ -1152,6 +1210,8 @@ class DistributedDataParallel(Module):
     def _distributed_broadcast_coalesced(
         self, tensors, buffer_size, authoritative_rank=0
     ):
+        if self.model_index > 0:
+            assert(False)
         dist._broadcast_coalesced(
             self.process_group, tensors, buffer_size, authoritative_rank
         )
@@ -1177,12 +1237,19 @@ class DistributedDataParallel(Module):
             )
         return rank_to_use.item()
 
+    def _sync_buffers(self):
+        local_master = local_DDP_list['0']
+        for _buffer, _master in zip(self.modules_buffers[0], local_master.modules_buffers[0]):
+            _buffer.copy_(_master)
+
     def _sync_params(self):
         with torch.no_grad():
             # only do intra-node parameters sync for replicated single-device
             # CUDA modules
             if self.device_ids and len(self.device_ids) > 1:
                 # intra-node parameter sync
+                if self.model_index > 0:
+                    assert(False)
                 result = comm.broadcast_coalesced(
                     self.modules_params[0],
                     self.device_ids,
@@ -1218,24 +1285,44 @@ class DistributedDataParallel(Module):
                     authoritative_rank = self._find_common_rank(dist.get_rank(), True)
                 else:
                     # The process with rank 0 is considered the authoritative copy.
-                    authoritative_rank = 0
-                self._distributed_broadcast_coalesced(
-                    self.modules_buffers[0],
-                    self.broadcast_bucket_size,
-                    authoritative_rank,
-                )
-                # only do intra-node buffer sync for replicated single-device
-                # CUDA modules
-                if self.device_ids and len(self.device_ids) > 1:
-                    # intra-node buffer sync
-                    result = comm.broadcast_coalesced(
-                        self.modules_buffers[0],
-                        self.device_ids,
-                        self.broadcast_bucket_size)
-                    for tensors, module_buffers in zip(result[1:],
-                                                       self.modules_buffers[1:]):
-                        for tensor, buffer in zip(tensors, module_buffers):
-                            buffer.set_(tensor)
+                    authoritative_rank = self.broadcast_master
+
+                if self.model_index == 0:
+                    self._distributed_broadcast_coalesced(
+                            self.modules_buffers[0],
+                            self.broadcast_bucket_size,
+                            authoritative_rank,
+                    )
+                    torch.cuda.synchronize()
+                    # only do intra-node buffer sync for replicated single-device
+                    # CUDA modules
+                    if self.device_ids and len(self.device_ids) > 1:
+                        # intra-node buffer sync
+                        result = comm.broadcast_coalesced(
+                            self.modules_buffers[0],
+                            self.device_ids,
+                            self.broadcast_bucket_size)
+                        for tensors, module_buffers in zip(result[1:],
+                                                           self.modules_buffers[1:]):
+                            for tensor, buffer in zip(tensors, module_buffers):
+                                buffer.set_(tensor)
+
+                    if self.sync_buffer_barrier[0]:
+                        self.sync_buffer_barrier[0].wait()
+                else:
+                    if self.sync_buffer_barrier[0]:
+                        self.sync_buffer_barrier[0].wait()
+                    self._sync_buffers()
+
+                if self.sync_buffer_barrier[1]:
+                    self.sync_buffer_barrier[1].wait()
+                """
+                local_master = local_DDP_list['0']
+                #for cur_tensor, main_tensor in zip(self.module.parameters(), local_master.module.parameters()):
+                for cur_tensor, main_tensor in zip(self.module.buffers(), local_master.module.buffers()):
+                    if not torch.equal(cur_tensor, main_tensor):
+                        raise RuntimeError("Tensor is not synchronized!")
+                """
 
     def _passing_sync_batchnorm_handle(self, module_copies):
         for dev_idx, module in enumerate(module_copies):
diff --git a/torch/tensor.py b/torch/tensor.py
index fd9b60d0773..0ae5ab7a530 100644
--- a/torch/tensor.py
+++ b/torch/tensor.py
@@ -192,7 +192,7 @@ class Tensor(torch._C._TensorBase):
         # All strings are unicode in Python 3.
         return torch._tensor_str._str(self)
 
-    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None):
+    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None, model_index=0):
         r"""Computes the gradient of current tensor w.r.t. graph leaves.
 
         The graph is differentiated using the chain rule. If the tensor is
@@ -242,7 +242,7 @@ class Tensor(torch._C._TensorBase):
                 retain_graph=retain_graph,
                 create_graph=create_graph,
                 inputs=inputs)
-        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
+        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs, model_index=model_index)
 
     def register_hook(self, hook):
         r"""Registers a backward hook.
@@ -286,6 +286,22 @@ class Tensor(torch._C._TensorBase):
         self._backward_hooks[handle.id] = hook
         return handle
 
+    def register_pre_hook(self, hook):
+        r"""Registers a backward pre-hook.
+
+        The hook will be called every time a backward pass is invoked.
+        """
+        if not self.requires_grad:
+            raise RuntimeError("cannot register a pre_hook on a tensor that "
+                               "doesn't require gradient")
+        if self._backward_pre_hooks is None:
+            self._backward_pre_hooks = OrderedDict()
+            if self.grad_fn is not None:
+                self.grad_fn._register_pre_hook_dict(self)
+        handle = hooks.RemovableHandle(self._backward_pre_hooks)
+        self._backward_pre_hooks[handle.id] = hook
+        return handle
+
     def reinforce(self, reward):
         def trim(str):
             return '\n'.join([line.strip() for line in str.split('\n')])
diff --git a/torch/utils/data/distributed.py b/torch/utils/data/distributed.py
index 8ce61d9948c..9a830cda03b 100644
--- a/torch/utils/data/distributed.py
+++ b/torch/utils/data/distributed.py
@@ -1,5 +1,5 @@
 import math
-from typing import TypeVar, Optional, Iterator
+from typing import TypeVar, Optional, Iterator, List
 
 import torch
 from . import Sampler, Dataset
@@ -76,6 +76,10 @@ class DistributedSampler(Sampler[T_co]):
         self.rank = rank
         self.epoch = 0
         self.drop_last = drop_last
+        #print('[DEBUG][torch/utils/data/distributed.py] == DistributedSampler() == ')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] rank: {rank}')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] num_replicas: {num_replicas}')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] dataset length: {len(self.dataset)}')
         # If the dataset length is evenly divisible by # of replicas, then there
         # is no need to drop any data, since the dataset will be split equally.
         if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore
@@ -89,7 +93,9 @@ class DistributedSampler(Sampler[T_co]):
             )
         else:
             self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore
+        #print(f'[DEBUG][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
         self.total_size = self.num_samples * self.num_replicas
+        #print(f'[DEBUG][torch/utils/data/distributed.py] self.total_size: {self.total_size}')
         self.shuffle = shuffle
         self.seed = seed
 
@@ -116,11 +122,16 @@ class DistributedSampler(Sampler[T_co]):
 
         # subsample
         indices = indices[self.rank:self.total_size:self.num_replicas]
+        #print('[DEBUG][torch/utils/data/distributed.py] == DistributedSampler().__iter__ == ')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] len(indices): {len(indices)}')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
         assert len(indices) == self.num_samples
 
         return iter(indices)
 
     def __len__(self) -> int:
+        #print('[DEBUG][torch/utils/data/distributed.py] == DistributedSampler() == ')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
         return self.num_samples
 
     def set_epoch(self, epoch: int) -> None:
@@ -133,3 +144,94 @@ class DistributedSampler(Sampler[T_co]):
             epoch (int): Epoch number.
         """
         self.epoch = epoch
+
+
+class ImbalancedSampler(Sampler[T_co]):
+    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,
+                 rank: Optional[int] = None, shuffle: bool = True,
+                 seed: int = 0, drop_last: bool = False, partition_size: List[float] = None) -> None:
+        if num_replicas is None:
+            if not dist.is_available():
+                raise RuntimeError("Requires distributed package to be available")
+            num_replicas = dist.get_world_size()
+        if rank is None:
+            if not dist.is_available():
+                raise RuntimeError("Requires distributed package to be available")
+            rank = dist.get_rank()
+        if rank >= num_replicas or rank < 0:
+            raise ValueError(
+                "Invalid rank {}, rank should be in the interval"
+                " [0, {}]".format(rank, num_replicas - 1))
+        if drop_last:
+            raise ValueError("Argument drop_last is not supported currently")
+        self.dataset = dataset
+        self.num_replicas = num_replicas
+        self.rank = rank
+        self.epoch = 0
+        self.drop_last = drop_last
+        self.partition_size = partition_size
+        self.all_num_samples_in_process_group = [
+            math.ceil(len(self.dataset) * partition_size) \
+            for partition_size in self.partition_size
+        ]
+        print('[INFO][torch/utils/data/distributed.py] == ImbalancedSampler() == ')
+        print(f'[INFO][torch/utils/data/distributed.py] rank: {rank}')
+        print(f'[INFO][torch/utils/data/distributed.py] num_replicas: {num_replicas}')
+        print(f'[INFO][torch/utils/data/distributed.py] dataset length: {len(self.dataset)}')
+        print(f'[INFO][torch/utils/data/distributed.py] partition_size: {self.partition_size}')
+        #self.num_samples = math.ceil(len(self.dataset) * self.partition_size)
+        self.num_samples = self.all_num_samples_in_process_group[rank]
+        self.total_size = len(self.dataset)
+        print(f'[INFO][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
+        print(f'[INFO][torch/utils/data/distributed.py] self.total_size: {self.total_size}')
+        self.shuffle = shuffle
+        self.seed = seed
+        
+
+    def __iter__(self) -> Iterator[T_co]:
+        if self.shuffle:
+            # deterministically shuffle based on epoch and seed
+            g = torch.Generator()
+            g.manual_seed(self.seed + self.epoch)
+            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore
+        else:
+            indices = list(range(len(self.dataset)))  # type: ignore
+        #print('[DEBUG][torch/utils/data/distributed.py] == ImbalancedSampler().__iter__ == ')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] len(indices): {len(indices)}')
+        # subsample
+        start_index = sum(self.all_num_samples_in_process_group[:self.rank])
+        end_index = start_index + self.num_samples
+        #print(f'[DEBUG][torch/utils/data/distributed.py] subsampling index at rank: {self.rank} ==> {start_index} : {end_index}')
+        #indices = indices[self.rank*self.num_samples:(self.rank + 1)*self.num_samples]
+        indices = indices[start_index:end_index]
+        #print('[DEBUG][torch/utils/data/distributed.py] == after sub-sampling ==')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] len(indices): {len(indices)}')
+        #print(f'[DEBUG][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
+        if len(indices) != self.num_samples:
+            # add extra samples to make it evenly divisible
+            padding_size = self.num_samples - len(indices)
+            #print(f'[DEBUG][torch/utils/data/distributed.py] padding_size: {padding_size}')
+            if padding_size <= len(indices):
+                indices += indices[:padding_size]
+            else:
+                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]
+
+        assert len(indices) == self.num_samples
+
+        return iter(indices)
+
+    def __len__(self) -> int:
+        # print('[DEBUG][torch/utils/data/distributed.py] == ImbalancedSampler() == ')
+        # print(f'[DEBUG][torch/utils/data/distributed.py] self.num_samples: {self.num_samples}')
+        return self.num_samples
+
+    def set_epoch(self, epoch: int) -> None:
+        r"""
+        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
+        use a different random ordering for each epoch. Otherwise, the next iteration of this
+        sampler will yield the same ordering.
+
+        Args:
+            epoch (int): Epoch number.
+        """
+        self.epoch = epoch
\ No newline at end of file
diff --git a/torch/utils/data/sampler.py b/torch/utils/data/sampler.py
index e48ad64fdc9..9e1594c2c22 100644
--- a/torch/utils/data/sampler.py
+++ b/torch/utils/data/sampler.py
@@ -227,6 +227,7 @@ class BatchSampler(Sampler[List[int]]):
         for idx in self.sampler:
             batch.append(idx)
             if len(batch) == self.batch_size:
+                #print(f'[DEBUG][torch/utils/data/sampler.py] rank: {torch.distributed.get_rank()} | BatchSampler - self.batch_size: {self.batch_size}')
                 yield batch
                 batch = []
         if len(batch) > 0 and not self.drop_last:
